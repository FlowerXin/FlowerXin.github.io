<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>YOLO v1-7系列总结、思考 | FlowerXin's Blog</title><meta name="keywords" content="-YOLO"><meta name="author" content="FlowerXin"><meta name="copyright" content="FlowerXin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="referrer" content="no-referrer"><meta name="description" content="本篇文章将详细介绍YOLO v1-7系列和自己在学习的时候一些思考与总结，由于文章内容过多，建议可以在目录栏导航进行阅读。后续若发布v8系列，本文将持续更新。感兴趣的话就读下去吧~ 1. YOLO目标检测基本概念目标检测（Object Detection）是在图像对一类或多类感兴趣的目标进行查找和分类，以便于确定他们的类别和位置。 图像识别任务：1.分类（classification）：判断照">
<meta property="og:type" content="article">
<meta property="og:title" content="YOLO v1-7系列总结、思考">
<meta property="og:url" content="http://flowerxin.github.io/post/undefined.html">
<meta property="og:site_name" content="FlowerXin&#39;s Blog">
<meta property="og:description" content="本篇文章将详细介绍YOLO v1-7系列和自己在学习的时候一些思考与总结，由于文章内容过多，建议可以在目录栏导航进行阅读。后续若发布v8系列，本文将持续更新。感兴趣的话就读下去吧~ 1. YOLO目标检测基本概念目标检测（Object Detection）是在图像对一类或多类感兴趣的目标进行查找和分类，以便于确定他们的类别和位置。 图像识别任务：1.分类（classification）：判断照">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/9f087efcba3946f787555283b4172d64.jpeg">
<meta property="article:published_time" content="2022-07-28T05:55:00.000Z">
<meta property="article:modified_time" content="2022-08-01T15:44:22.976Z">
<meta property="article:author" content="FlowerXin">
<meta property="article:tag" content="-YOLO">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/9f087efcba3946f787555283b4172d64.jpeg"><link rel="shortcut icon" href="/assets/imgs/favicon.png"><link rel="canonical" href="http://flowerxin.github.io/post/undefined"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: FlowerXin","link":"链接: ","source":"来源: FlowerXin's Blog","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'YOLO v1-7系列总结、思考',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2022-08-01 23:44:22'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><div class="aplayer" data-id="7506016963" data-server="netease" data-type="playlist" data-fixed="true" data-listFolded="false" data-order="random" data-preload="none"></div><link rel="stylesheet" href="https://unpkg.com/aplayer@1.10.1/dist/APlayer.min.css"><script src="https://unpkg.com/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="https://unpkg.com/meting@2.0.1/dist/Meting.min.js"></script><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/friend_404.gif" data-original="/assets/imgs/avatar.JPG" onerror="onerror=null;src='assets/imgs/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://img-blog.csdnimg.cn/9f087efcba3946f787555283b4172d64.jpeg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">FlowerXin's Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">YOLO v1-7系列总结、思考</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-07-28T05:55:00.000Z" title="发表于 2022-07-28 13:55:00">2022-07-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-08-01T15:44:22.976Z" title="更新于 2022-08-01 23:44:22">2022-08-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/YOLO/">-YOLO</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">19.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>63分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="YOLO v1-7系列总结、思考"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><hr>
<hr>
<p>本篇文章将详细介绍YOLO v1-7系列和自己在学习的时候一些思考与总结，由于文章内容过多，建议可以在目录栏导航进行阅读。后续若发布v8系列，本文将持续更新。感兴趣的话就读下去吧~<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/img_convert/f8b21cb07c63093b44affd3e4d995477.png" alt="avatar"></p>
<h1 id="1-YOLO目标检测基本概念"><a href="#1-YOLO目标检测基本概念" class="headerlink" title="1. YOLO目标检测基本概念"></a>1. YOLO目标检测基本概念</h1><p>目标检测（Object Detection）是在图像对一类或多类感兴趣的目标进行查找和分类，以便于确定他们的类别和位置。</p>
<p>图像识别任务：<br>1.分类（classification）：判断照片里的目标类别 （指示出CAT）<br>2.定位（location）：给定目标位置（红框定位）<br>3.检测（detection）：定位出目标位置并知道目标是什么（红框加CAT）<br>4.分割（segmention):分为实例分割（instance-level）和场景分割（scene-level），每一个像素属于哪个目标物或场景问题 （边缘图像加红框加CAT）  </p>
<p>目标检需要解决的问题：<br>1.分类问题<br>2.定位问题<br>3.大小问题<br>4.形状问题  </p>
<p>知识点：<br>1.map指标:综合衡量检测效果<br>2.IOU：真实值与预测值交集&#x2F;真实值与预测值并集<br>3.精度（precision）：TP&#x2F;（TP+FP)<br>4.查全率（recall):TP&#x2F;(TP+FN)<br>5.TP:正例判定为正例<br>FP:错例判定为正例<br>FN:正例判定为错例<br>TN:错例判定为错例<br>6.如何计算map呢？需要把所有阈值都考虑进来，map就是类别的平均。<br>建立Py ,Rx坐标图，计算围起来的面积，结果就是map。需要注意的是，要取p的最大值，画成长方形，求面积。</p>
<h1 id="2-YOLO-v1"><a href="#2-YOLO-v1" class="headerlink" title="2. YOLO v1"></a>2. YOLO v1</h1><h2 id="2-1-基本思想"><a href="#2-1-基本思想" class="headerlink" title="2.1 基本思想"></a>2.1 基本思想</h2><p>YOLO（you only look once）是将整张图片作为网络的输入，直接在输出层Bounding Box的位置和所属类别进行回归。  </p>
<p>YOLO将图片划分为7*7（SxS）个网络，每个网格可以检测出2个边框，也就是98个bounding box。每个格子用来预测B个Bounding Box及其置信度，以及C个类别的概率。其中每个Bounding Box包含{x,y,w,h,c}5个参数。每个格子输出的向量长度为SxSx(5xB+C)    </p>
<p>（x,y）：中心点相对于网格的偏移量<br>（w,h）：高度h与宽度w是相对于整张图象预测的比例<br>置信度c(confidence)：预测框与实际边界框之间的IOU，置信度分数反映了该模型对框是否包含目标的可靠程度，以及预测框的准确程度。如果单元格中不存在目标，则置信度应该为0,置信度分数等于预测框与真实值之间联合的部分交集（IOU） </p>
<p>理解1:<br>每个格子输出的向量长度为SxSx(5xB+C)的理解：根据YOLO的设计，输入图像被划分为 7x7 的网格（grid），输出张量中的 7x7 就对应着输入图像的 7x7 网格。或者我们把 7x7x30 的张量看作 7x7&#x3D;49个30维的向量，也就是输入图像中的每个网格对应输出一个30维的向量。  </p>
<p>理解2:<br>30维向量的理解：30维的向量包含2个bbox的位置和置信度以及该网格属于20个类别的概率。20个对象分类指Yolo支持识别20种不同的对象（人、鸟、猫、汽车、椅子等），所以这里有20个值表示该网格位置存在任一种对象的概率。  </p>
<p>理解3:<br>20个对象分类的概率：对于输入图像中的每个对象，先找到其中心点。中心点在黄色圆点位置，中心点落在黄色网格内，所以这个黄色网格对应的30维向量中，该物体种类的概率是1，其它对象的概率是0。所有其它48个网格的30维向量中，该物体种类的概率就都是0。这就是所谓的”中心点所在的网格对预测该对象负责”。</p>
<p>损失函数：损失就是网络实际输出值与样品标签值之间的偏差，包括:位置误差，置信度误差（含有object的），置信度误差（不含有object),分类误差  </p>
<p>理解1：<br>位置误差：选择IOu最大的那个进行计算。公式加根号原因：物体大的就计算小一点，物体小的就计算精确点（敏感）。<br>理解2：<br>置信度误差（含有object的）：先预定一个置信度阈值如0.5，再找大于阈值的IOU，若多个大于择选最大的，在计算置信度（实际预测出来的结果）与真实值（1）之间差异<br>理解3：<br>置信度误差（不含有object)：先预定一个置信度阈值如0.5，再找大于阈值的IOU，若多个大于择选最大的，在计算置信度（实际预测出来的结果）与真实值（0）之间差异.前有个入说明：前景少，但是背景多，为了增加前景重要性。</p>
<p>例：<br>样品标签：00…1…0|1 0|目标边框坐标|坐标2<br>网络输出：0.03 0.1 …0.6…0.05|0.7 0.2|预测边框坐标1|预测边框坐标2  </p>
<h2 id="2-2-模型训练"><a href="#2-2-模型训练" class="headerlink" title="2.2 模型训练"></a>2.2 模型训练</h2><p>YOLO v1网络结构有24个卷积层，后面是2个全连接层。仅使用1×1卷积降维层，后面是3×3卷积层。Yolo先使用ImageNet数据集对前20层卷积网络进行预训练，然后使用完整的网络，在PASCAL VOC数据集上进行对象识别和定位的训练。Yolo的最后一层采用线性激活函数，其它层都是Leaky ReLU。训练中采用了drop out和数据增强（data augmentation）来防止过拟合。 </p>
<p>input image-&gt;卷积层-&gt;全连接层（第二层是1470&#x3D;7x7x30）-&gt;</p>
<h2 id="2-3-训练过程"><a href="#2-3-训练过程" class="headerlink" title="2.3 训练过程"></a>2.3 训练过程</h2><p>预训练：采用前20个卷积层、平均池化层、全连接层进行了大约一周的预训练；<br>输入：输入数据为224×224和448×448大小的图像。<br>采用相对坐标：通过图像宽度和高度来规范边界框的宽度和高度，使它们落在0和1之间，边界框 x x x和 y y y坐标参数化为特定网格单元位置的偏移量，边界也在0和1之间。<br>损失函数：由坐标预测、是否包含目标物体置信度、类别预测构成。如果目标存在于该网格单元中（前面讨论的条件类别概率），则损失函数仅惩罚分类错误。如果预测器“负责”实际边界框（即该网格单元中具有最高IOU的预测器），则它也仅惩罚边界框坐标错误。<br>学习率：第一个迭代周期，慢慢地将学习率从0.01提高到0.1，然后继续以0.1的学习率训练75个迭代周期，用0.01的学习率训练30个迭代周期，最后用0.001的学习率训练30个迭代周期。<br>避免过拟合策略：使用dropout和数据增强来避免过拟合。</p>
<h2 id="2-4-优缺点"><a href="#2-4-优缺点" class="headerlink" title="2.4 优缺点"></a>2.4 优缺点</h2><p>优点：<br>（1）YOLO检测物体速度较快。<br>（2）YOLO在训练和测试时都能看到一整张图的信息（而不像其它算法看到局部图片信息），因此YOLO在检测物体是能很好利用上下文信息，从而不容易在背景上预测出错误的物体信息。<br>（3）YOLO可以学到物体泛化特征<br>缺点：<br>（1）精度低于其它state-of-the-art的目标检测网络。<br>（2）对小物体检测效果不好，尤其是密集的小物体，因为一个栅格只能检测2个物体，限制检测效率。<br>（3）容易产生定位错误  </p>
<p>知识点：<br>1.NMS（非极大值抑制）：识别时很多框，选一个置信度高的，其他的舍弃</p>
<h1 id="3-YOLO-v2"><a href="#3-YOLO-v2" class="headerlink" title="3. YOLO v2"></a>3. YOLO v2</h1><h2 id="3-1-基本思想"><a href="#3-1-基本思想" class="headerlink" title="3.1 基本思想"></a>3.1 基本思想</h2><p>YOLO v2相对v1版本，在继续保持处理速度的基础上，从预测更准确（Better），速度更快（Faster），识别对象更多（Stronger）这三个方面进行了改进。在精度上利用一些列训练技巧，在速度上应用了新的网络模型DarkNet19（实际输入为416x416），在分类任务上采用联合训练方法，结合wordtree等方法，使YOLO v2的检测种类扩充到了上千种。舍弃了全连接层（FC),5次采样（13x13），卷积后全部加入批量归一化。1*1卷积节省了很多参数。</p>
<h2 id="3-2-改进部分"><a href="#3-2-改进部分" class="headerlink" title="3.2 改进部分"></a>3.2 改进部分</h2><h3 id="3-2-1-批量归一化（Batch-Normalization）"><a href="#3-2-1-批量归一化（Batch-Normalization）" class="headerlink" title="3.2.1 批量归一化（Batch Normalization）"></a>3.2.1 批量归一化（Batch Normalization）</h3><p>批量归一化有助于解决反向传播过程中的梯度消失和梯度爆炸问题，降低对一些超参数（比如学习率、网络参数的大小范围、激活函数的选择）的敏感性，并且每个batch分别进行归一化的时候，起到了一定的正则化效果（YOLO v2不再使用dropout），从而能够获得更好的收敛速度和收敛效果。(可以简单理解为将杂乱无章的x输入变量通过处理变成转化成按正太分布分布的数据x^,从而使得输入网络的数据分布较近，有利于网络的迭代优化，使得工作效率更快。)</p>
<p>通常，一次训练会输入一批样本（batch）进入神经网络。批量归一化在神经网络的每一层，在网络（线性变换）输出后和激活函数（非线性变换）之前增加一个批归一化层（BN）。  </p>
<p>具体推到如下<a target="_blank" rel="noopener" href="https://blog.csdn.net/wjinjie/article/details/105028870">BN算法实现公式推导</a> </p>
<h3 id="3-2-2-高分辨率图像分类器（High-resolution-classifier）"><a href="#3-2-2-高分辨率图像分类器（High-resolution-classifier）" class="headerlink" title="3.2.2 高分辨率图像分类器（High resolution classifier）"></a>3.2.2 高分辨率图像分类器（High resolution classifier）</h3><p>YOLO v1在预训练的时候采用的是224×224的输入，然后在detection的时候采用448×448的输入，这会导致从分类模型切换到检测模型的时候，模型还要适应图像分辨率的改变。而YOLO v2则将预训练分成两步：先用224×224的输入从头开始训练网络，大概160个epoch（表示将所有训练数据循环跑160次），然后再将输入调整到448×448，再训练10个epoch。注意这两步都是在ImageNet数据集上操作。最后再在检测的数据集上微调（fine-tuning），也就是detection的时候用448×448的图像作为输入就可以顺利过渡了。  </p>
<h3 id="3-2-3-带Anchor-Boxes的卷积（Convolutional-With-Anchor-Boxes）"><a href="#3-2-3-带Anchor-Boxes的卷积（Convolutional-With-Anchor-Boxes）" class="headerlink" title="3.2.3 带Anchor Boxes的卷积（Convolutional With Anchor Boxes）"></a>3.2.3 带Anchor Boxes的卷积（Convolutional With Anchor Boxes）</h3><p>YOLO v1利用全连接层直接对边界框进行预测，导致丢失较多空间信息，定位不准。YOLO v2去掉了YOLO v1中的全连接层，使用Anchor Boxes预测边界框，同时为了得到更高分辨率的特征图，YOLO v2还去掉了一个池化层。由于图片中的物体都倾向于出现在图片的中心位置，特别是那种比较大的物体，若特征图恰好有一个中心位置，利用这个中心位置预测中心点落入该位置的物体，对这些物体的检测会更容易。所以总希望得到的特征图的宽高都为奇数。YOLO v2通过缩减网络，使用416×416的输入，模型下采样的总步长为32，最后得到13×13的特征图，然后对13×13的特征图的每个cell预测5个anchor boxes，对每个anchor box预测边界框的位置信息、置信度和一套分类概率值。  </p>
<p>理解1：<br>图片输入分辨率为416 * 416原因：目的是为了让后面产生的卷积特征图宽高都为奇数（下采样步长为32），最终得到13 * 13的卷积特征图（416&#x2F;32&#x3D;13），（32&#x3D;2^5)这样就可以只有一个center cell。因为，大物体通常占据了图像的中间位置， 就可以只用中心的一个cell来预测这些物体的位置，否则就要用中间的4个cell来进行预测，这个技巧可稍稍提升效率。  </p>
<p>理解2：<br>加入了anchor boxes后，召回率上升，准确率下降的原因：假设每个cell预测9个anchor boxs，那么总共会预测13 * 13 * 9 &#x3D; 1521个boxes，而之前的网络仅仅预测7 * 7 * 2 &#x3D; 98个boxes。  </p>
<p>理解3：<br>关于输出tensor：对于YOLOv1，每个cell都预测2个boxes，每个boxes包含5个值：( x , y , w , h , c )，前4个值是边界框位置与大小，最后一个值是置信度（confidence scores，包含两部分：含有物体的概率以及预测框与ground truth的IOU）。但是每个cell只预测一套分类概率值（class predictions，其实是置信度下的条件概率值）,供2个boxes共享。YOLOv2使用了anchor boxes之后，每个位置的各个anchor box都单独预测一套分类概率值。<br>例：<br>YOLO v1:  box1:4-x,y,w,h|box2:1-c|20-classes<br>YOLO v2:  box1:4-x,y,w,h|1-c|20-classes|box2:4-x,y,w,h|1-c|20-classes|…  </p>
<h3 id="3-2-4-维度聚类（Dimension-Clusters）"><a href="#3-2-4-维度聚类（Dimension-Clusters）" class="headerlink" title="3.2.4 维度聚类（Dimension Clusters）"></a>3.2.4 维度聚类（Dimension Clusters）</h3><p>在使用anchor的时候遇到了两个问题，第一个是anchor boxes的宽高维度往往是精选的先验框（hand-picked priors），虽说在训练过程中网络也会学习调整boxes的宽高维度，最终得到准确的bounding boxes。但是，如果一开始就选择了更好的、更有代表性的先验boxes维度，那么网络就更容易学到准确的预测位置。</p>
<p>使用K-means聚类方法类训练bounding boxes，可以自动找到更好的boxes宽高维度。传统的K-means聚类方法使用的是欧氏距离函数，也就意味着较大的boxes会比较小的boxes产生更多的error，聚类结果可能会偏离。并且，先验框的主要目的是为了使得预测框和ground truth的IOU更好，所以聚类分析时选用box与聚类中心box之间的IOU值作为距离标准，这样，error就和box的尺度无关了，最终的距离函数为：d(box,centroid)&#x3D;1−IOU(box,centroid)  </p>
<p>简单理解：很多框，按照大小分类（k&#x3D;5时最优），每类取出一个最体现这类特征的值作为预选框。</p>
<p>深入研究<a target="_blank" rel="noopener" href="https://blog.csdn.net/xiaomifanhxx/article/details/81215051">K-means原理</a></p>
<h3 id="3-2-5-直接定位预测（Direct-location-Prediction）"><a href="#3-2-5-直接定位预测（Direct-location-Prediction）" class="headerlink" title="3.2.5 直接定位预测（Direct location Prediction）"></a>3.2.5 直接定位预测（Direct location Prediction）</h3><p>YOLO v2沿用YOLO v1的方法，根据所在网格单元的位置来预测坐标，则Ground Truth的值介于0到1之间。网络中将得到的网络预测结果再输入sigmoid函数中，让输出结果介于0到1之间。设一个网格相对于图片左上角的偏移量是 Cx,Cy,先验框的宽度和高度分别是Pw和Ph​，则预测的边界框相对于特征图的中心坐标(bx​,by​)和宽度bw​，bh​的计算公式如下图所示。其中，σ为sigmoid函数，tx​，ty​是预测的坐标偏移值（中心点坐标），tw​，th​是尺度缩放，分别经过sigmoid，输出0-1之间的偏移量，与cx​,cy​相加后得到bounding box中心点的位置。  </p>
<p>简单理解：<br>YOLO v1:<br>1.bbox:中心为（xp,yp);宽和高为（wp,hp),则x&#x3D;xp+wp<em>tx;y&#x3D;up+hp</em>ty。<br>2.当tx&#x3D;1时，将bbox在x轴向右移动wp;tx&#x3D;-1，则将其向左移动wp。<br>3.这样会导致收敛问题，模型不稳定，尤其是刚开始进行训练的时候<br>4.v2中没有使用偏移量，而是选择相对grid cell的偏移量  </p>
<p>YOLO v2：<br>1.公式：bx&#x3D;o(tx)+cx;by&#x3D;o(ty)+cy;bw&#x3D;pwe^tw;bh&#x3D;phe^th<br>2.先验框的pw,ph为实机照片大小除以32，表示的是在特征图里的位置。o(tx)与o（ty）表示的是预测的中心点相对于实际框（13x13中的一个）左上角点的偏移量。<br>3.例如：预测值（o(tx),o(ty),tw,th)&#x3D;(0.2,0.1,0.2,0.32),先验框为pw&#x3D;3.19275,ph&#x3D;4.00944<br>则<br>在特征图位置：bx&#x3D;0.2+1&#x3D;1.2,by&#x3D;0.1+1&#x3D;1.1,bw&#x3D;3.19275<em>e^0.2&#x3D;3.89963,bh&#x3D;4.00944</em>e^0.32&#x3D;5.5215<br>在原位置：bx&#x3D;1.2<em>32&#x3D;38.4,by&#x3D;1.1</em>32&#x3D;35.2,bw&#x3D;3.89963<em>32&#x3D;124.78,bh&#x3D;5.52151</em>32&#x3D;176.68  </p>
<h3 id="3-2-6-细粒度特征（Fine-Grained-Features）"><a href="#3-2-6-细粒度特征（Fine-Grained-Features）" class="headerlink" title="3.2.6 细粒度特征（Fine-Grained Features）"></a>3.2.6 细粒度特征（Fine-Grained Features）</h3><p>yolov2最终在13 * 13的特征图上进行预测，虽然这足以胜任大尺度物体的检测，但是用上细粒度特征的话，这可能对小尺度的物体检测有帮助。这里使用了一种不同的方法，简单添加了一个转移层（ passthrough layer），这一层要把浅层特征图（分辨率为26 * 26，是底层分辨率4倍）连接到深层特征图。</p>
<p>转移层（ passthrough layer）就是将深层特征与浅层特征进行连接，这里采用的是通道上的连接而非空间位置上的连接，即叠加特征到不同的通道。yolov2把26 * 26 * 512的特征图变换为13 * 13 * 2048的特征图，将这个特征图与原来的特征相连接。yolov2使用的就是经过连接的特征图，它可以拥有更好的细粒度特征，使得模型的性能获得了1%的提升。</p>
<p>关于特征融合的具体方法为：passthrough layer，具体来说就是特征重排（不涉及到参数学习），前面26 * 26 * 512的特征图使用按行和按列隔行采样的方法，就可以得到4个新的特征图，维度都是13 * 13 * 512，然后做concat操作，得到13 * 13 * 2048的特征图，将其拼接到后面的层，相当于做了一次特征融合，有利于检测小目标。  </p>
<p>简单理解：<br>1.概述来说就是特征图上的点能看到原始图像多大区域。越大的感受野越能体现图像特征。<br>2.最后一层时感受野太大了，小目标可能丢失了，需融合之前的特征。</p>
<p>思考：<br>1.如果堆叠3个3<em>3的卷积层，并且保持滑动窗口步长为1，其感受野就是7</em>7的了，这和使用7<em>7卷积核结果是一样的，那为什么要丢及3个小卷积呢？<br>假设输入大小都是h</em>w<em>c，并且都使用c个卷积核（得到C个特征图），可以计算一下各自需要的参数：<br>一个7</em>7卷积核所需参数：&#x3D;Cx(7x7xc)&#x3D;49c^2;一个3*3卷积核所需参数：&#x3D;3xc(3x3xc)&#x3D;27c^2<br>很明显，堆叠小的卷积核所需的参数更小一些，并且卷积过程越多，特征提取也会越细致，加入的非线性变换也随着增多，还不会增大权重参数个数，用小的卷积核来完成体积特征提取操作。</p>
<h3 id="3-2-7-多尺度训练（Multi-Scale-Training）"><a href="#3-2-7-多尺度训练（Multi-Scale-Training）" class="headerlink" title="3.2.7 多尺度训练（Multi-Scale Training）"></a>3.2.7 多尺度训练（Multi-Scale Training）</h3><p>yolov1网络使用固定的448 * 448的图片作为输入，在加入anchor boxes后，yolov2的输入变成了416 * 416。由于yolov2只用到了卷积层和池化层，那么就可以进行动态调整（意思是可检测任意大小图片）。</p>
<p>不同于固定输入网络的图片尺寸的方法，在几次迭代后就会微调网络。每经过10次训练（10 epoch），就会随机选择新的图片尺寸。YOLO网络使用的降采样参数为32，那么就使用32的倍数进行尺度池化{320,352，…，608}，最终最小的尺寸为320 * 320，最大的尺寸为608 * 608，接着按照输入尺寸调整网络进行训练。这种机制使得网络可以更好地预测不同尺寸的图片，意味着同一个网络可以进行不同分辨率的检测任务，在小尺寸图片上YOLOv2运行更快，在速度和精度上达到了平衡。  </p>
<h2 id="YOLO-V2反思"><a href="#YOLO-V2反思" class="headerlink" title="YOLO V2反思"></a>YOLO V2反思</h2><p>1.YOLO v1,v2中使用IOU为置信度标签有什么不好？<br>（1）很多预测框与中心点的IOU最高只有0.7（最好的学生只有70分，取乎其中的乎其下）<br>（2）coco中的小目标IOU对像素偏移很敏感无法有效学习。  </p>
<h1 id="4-YOLO-v3"><a href="#4-YOLO-v3" class="headerlink" title="4.YOLO v3"></a>4.YOLO v3</h1><h2 id="4-1-基础概念"><a href="#4-1-基础概念" class="headerlink" title="4.1 基础概念"></a>4.1 基础概念</h2><p>YOLO v3总结了在YOLO v2的基础上做了一些尝试性的改进。其中有两个值得一提的亮点，一个是使用残差模型，进一步加深了网络结构；另一个是使用FPN架构实现多尺度检测。特征做的更细致，融入多持续特征信息来预测不同规格的物体。先验框更丰富了，3种scale，每种3个规格，一共9种。  </p>
<h2 id="4-2-改进部分"><a href="#4-2-改进部分" class="headerlink" title="4.2 改进部分"></a>4.2 改进部分</h2><p>1.引入FPN（特征金字塔网络），实现多尺度预测 。<br>2.使用新网络结构darknet-53（类似于ResNet引入残差结构）。<br>3.分类器不在使用Softmax，分类损失采用二分类交叉损失熵（binary cross-entropy loss)<br>主要考虑因素有两个：<br>Softmax使得每个框分配一个类别（score最大的一个），而对于Open Images这种数据集，目标可能有重叠的类别标签，因此Softmax不适用于多标签分类。Softmax可被独立的多个logistic分类器替代，且准确率不会下降。  </p>
<h3 id="4-2-1-多尺度预测"><a href="#4-2-1-多尺度预测" class="headerlink" title="4.2.1 多尺度预测"></a>4.2.1 多尺度预测</h3><p>YOLO v3在基本特征提取器上添加几个卷积层，其中最后一个卷积层预测了一个三维张量——边界框，目标和类别预测。 在COCO实验中，为每个尺度预测3个框，4个边界框偏移量，1个目标预测和80个类别预测，张量的大小为 N × N × [ 3 × ( 4 + 1 + 80 ) ] N×N×[3 ×(4 + 1 + 80)] N×N×[3×(4+1+80)]。</p>
<p>YOLO v3还从主干网络层中获取特征图，并使用按元素相加的方式将其与上采样特征图进行合并。这种方法能够从上采样的特征图中获得更有意义的语义信息，同时可以从更前的层中获取更细粒度的信息。然后，再添加几个卷积层来处理这个组合的特征图，并最终预测出一个类似的张量。</p>
<p>每种尺度预测3个box， anchor的设计方式仍然使用聚类，得到9个聚类中心，将其按照大小均分给3个尺度。</p>
<p>尺度1: 在基础网络之后经过卷积输出13×13特征层信息（32倍下采样）13<em>13</em>3*（4+1+80）假如80个分类<br>尺度2: 从尺度1中倒数第二13×13卷积层上采样(x2)后与主干网络26x26大小的特征图相堆叠，再通过卷积后输出26×26特征层信息（16倍下采样）。<br>尺度3: 与尺度2类似卷积后输出52×52特征层信息（8倍下采样）。  </p>
<p>简单理解：<br>1.13为老年人，26为中年人，52为年轻人。一般是三个人分别提取特征值。YOLO v3方法则是将老年人与上一层的中年人特征值融合，中年人再与年轻人特征值融合。最后老年人的特征值就融合的很全面。<br>2.采用多个scale融合的方式做预测。原来的YOLO v2有一个层叫：passthrough layer，假设最后提取的feature map的size是13<em>13，那么这个层的作用就是将前面一层的26</em>26的feature map和本层的13*13的feature map进行连接。操作也是为了加强YOLO算法对小目标检测的精确度，在多个scale的feature map上做检测，对于小目标的检测效果提升还是比较明显的。     </p>
<p>思考：<br>在YOLO v3中每个grid cell预测3个bounding box，看起来比YOLO v2中每个grid cell预测5个bounding box要少呢？  </p>
<p>因为YOLO v3采用了多个scale的特征融合，所以boundign box的数量要比之前多很多，以输入图像为416<em>416为例：（13</em>13+26<em>26+52</em>52）<em>3和13</em>13*5相比哪个更多应该很清晰了  </p>
<h3 id="4-2-2-多标签分类"><a href="#4-2-2-多标签分类" class="headerlink" title="4.2.2 多标签分类"></a>4.2.2 多标签分类</h3><p>YOLOv3在类别预测方面将YOLOv2的单标签分类改进为多标签分类，在网络结构中将YOLOv2中用于分类的softmax层修改为逻辑分类器。在YOLOv2中，算法认定一个目标只从属于一个类别，根据网络输出类别的得分最大值，将其归为某一类。然而在一些复杂的场景中，单一目标可能从属于多个类别。</p>
<p>为实现多标签分类就需要用逻辑分类器来对每个类别都进行二分类。逻辑分类器主要用到了sigmoid函数，它可以把输出约束在0到1，如果某一特征图的输出经过该函数处理后的值大于设定阈值，那么就认定该目标框所对应的目标属于该类。（例如：大于阈值0.7，则小猫大猫都属于猫）</p>
<h3 id="4-2-3-初始尺寸"><a href="#4-2-3-初始尺寸" class="headerlink" title="4.2.3 初始尺寸"></a>4.2.3 初始尺寸</h3><p>3、关于bounding box的初始尺寸还是采用YOLO v2中的k-means聚类的方式来做，不过数量变了。这种先验知识对于bounding box的初始化帮助还是很大的，毕竟过多的bounding box虽然对于效果来说有保障，但是对于算法速度影响还是比较大的。作者在COCO数据集上得到的9种聚类结果：(10<em>13); (16</em>30); (33<em>23); (30</em>61); (62<em>45); (59</em>119); (116<em>90); (156</em>198); (373<em>326)，这应该是按照输入图像的尺寸是416</em>416计算得到的。</p>
<h2 id="4-3-YOLO-v3反思总结"><a href="#4-3-YOLO-v3反思总结" class="headerlink" title="4.3 YOLO v3反思总结"></a>4.3 YOLO v3反思总结</h2><p>1.结合ResNet骨干网络发现，三个分支输出特征图的大小分别为32倍下采样、16倍下采样和8倍下采样的大小，这样做有什么好处？ </p>
<p>在特征提取骨干网络中，下采样倍数越多，感受野越大。所以32倍下采样特征图中，每个点的感受野是最大的，映射到原图可以看到更大的区域，所以用于预测大目标，16倍和8倍下采样同理。</p>
<p>2.边框预测公式中tx ty为什么要sigmoid？  </p>
<p>预测的tx和ty可能会比较大，tx,ty&gt;1。用sigmoid将tx,ty压缩到(0,1)区间内，可以有效的确保目标中心处于执行预测的网格单元中，防止偏移过多。  </p>
<p>举个栗子，我们一直都说网络不会预测边界框中心的坐标而是预测与预测目标的grid cell左上角相关的便宜tx,ty。例如在图2中，某目标的中心点偏移值预测假设为(0.4,0.7)，Cx&#x3D;3，Cy&#x3D;3，该物体的在feature map中的中心实际坐标显然是（3.4，3.7），此时是正常的。但若预测出的tx和ty大于1，如(1.6,0.8)，则物体的在feature map中的中心实际坐标显然是（3.6，3.8），注意此时物体中心在这个所属grid cell外面了，但(3,3)这个grid cell却检测出这个单元格内含有目标的中心（YOLO系列是物体中心点落在哪个grid cell，那么就由哪个grid cell进行预测），这样就矛盾了，因为左上角为(3,3)的grid cell负责预测这个物体，这个物体中心必须出现在这个grid cell中，所以一旦tx和ty大于1就会出现矛盾，因此必须进行归一化，也就是通过sigmoid函数将tx和ty显示在（0,1）。  </p>
<p>3.边框预测公式中tw和th为什么指数？  </p>
<p>tw，th是log尺度缩放到对数空间了，需要指数回来。<br>网络预测出偏移值后，若在推理阶段，根据边框预测公式得到最终的坐标值即可。不同的是，在训练阶段，我们需要计算损失，然后反向传播更新网络参数，所以我们需要了解YOLOv3的损失函数。  </p>
<p>4.Ground Truth的计算  </p>
<p>既然网络预测的是偏移值，那么在计算损失时，也是按照偏移值计算损失。现在我们有预测的值，还需要真值Ground Truth的偏移值。Ground Truth的tw，th是log尺度缩放到对数空间了，所以在预测时需要指数回来。这就是答案。  </p>
<p>5.为什么在计算Ground Truth的tw，th时需要缩放到对数空间？  </p>
<p>tw和th是物体所在边框的长宽和anchor box长宽之间的比率。不直接回归bounding box的长宽，是为避免训练带来不稳定的梯度。将尺度缩放到对数空间，因为如果直接预测相对形变tw 和 th，那么要求tw,th&gt;0，因为框的宽高不可能是负数，这样的话是在做一个有不等式条件约束的优化问题，没法直接用SGD来做，所以先取一个对数变换，将其不等式约束去掉就可以了。  </p>
<p>6.正负样本的确定  </p>
<p>一个尺度的feature有三个anchors，那么对于某个ground truth框，究竟是哪个anchor负责匹配它呢？<br>与yolov1一样，对于训练图片中的ground truth，若其中心点落在某个cell内，那么该cell内的3个anchor box负责预测它，具体是哪个anchor box预测它，需要在训练中确定，即由那个与ground truth的IOU最大的anchor box预测它，而剩余的两个anchor box不予该GT框匹配。（YOLOv3需要假定每个cell至多含有一个ground truth，而在实际上基本不会出现多于1个的情况。）  </p>
<p>每个GT目标仅与一个anchor相关联，与GT匹配的anchor box计算坐标误差、置信度误差（此时target为1）以及分类误差，而其他anchor box只计算置信度误差（此时target为0）。对于重叠大于等于0.5的其他预测框，忽略，不算损失。  </p>
<p>总的来说，正样本是与GT的IOU最大的框。负样本是与GT的IOU&lt;0.5的框。忽略的样本是与GT的IOU&gt;0.5 但不是最大的框。  </p>
<h1 id="4-YOLO-v4"><a href="#4-YOLO-v4" class="headerlink" title="4. YOLO v4"></a>4. YOLO v4</h1><h2 id="4-1-基本概念"><a href="#4-1-基本概念" class="headerlink" title="4.1 基本概念"></a>4.1 基本概念</h2><p>说在开头 YOLO v4实在是太多tricks啦，我要吐血了。。引用了很多图来便于理解，不然看一会就变成天线宝宝了，阿巴阿巴  </p>
<p>YOLOv4对深度学习中一些常用Tricks进行了大量的测试，最终选择了这些有用的Tricks：WRC、CSP、CmBN、SAT、 Mish activation、Mosaic data augmentation、CmBN、DropBlock regularization 和 CIoU loss。</p>
<p>是一个高效而强大的目标检测网咯。它使我们每个人都可以使用 GTX 1080Ti 或 2080Ti 的GPU来训练一个超快速和精确的目标检测器。对当前先进的目标检测方法进行了改进，使之更有效，并且更适合在单GPU上训练；这些改进包括CBN、PAN、SAM等。</p>
<h2 id="4-2-网络框架"><a href="#4-2-网络框架" class="headerlink" title="4.2 网络框架"></a>4.2 网络框架</h2><p> YOLOv4 &#x3D; CSPDarknet53（主干） + SPP附加模块（颈） + PANet路径聚合（颈） + YOLOv3（头部）</p>
<h2 id="4-3-框架介绍"><a href="#4-3-框架介绍" class="headerlink" title="4.3 框架介绍"></a>4.3 框架介绍</h2><h3 id="4-3-1-CSPDarknet53"><a href="#4-3-1-CSPDarknet53" class="headerlink" title="4.3.1 CSPDarknet53"></a>4.3.1 CSPDarknet53</h3><p>CSPNet（Cross Stage Partial Network），用来解决以往网络结构需要大量推理计算的问题。作者将问题归结于网络优化中的重复梯度信息。CSPNet在ImageNet dataset和MS COCO数据集上有很好的测试效果，同时它易于实现，在ResNet、ResNeXt和DenseNet网络结构上都能通用。</p>
<p>CSPNet的主要目的是能够实现更丰富的梯度组合，同时减少计算量。这个目标是通过将基本层的特征图分成两部分，然后通过一个跨阶段的层次结构合并它们来实现的。</p>
<p>而在YOLOv4中，将原来的Darknet53结构换为了CSPDarknet53，这在原来的基础上主要进行了两项改变：</p>
<p>（1）将原来的Darknet53与CSPNet进行结合。在前面的YOLOv3中，我们了解了Darknet53的结构，它是由一系列残差结构组成。进行结合后，CSPnet的主要工作就是将原来的残差块的堆叠进行拆分，把它拆分成左右两部分：主干部分继续堆叠原来的残差块，支路部分则相当于一个残差边，经过少量处理直接连接到最后。</p>
<p>理解：v3拆分，v4将拆分后的再拆成两部分，一部分进行处理再与另外一部分进行连接。</p>
<p>（2）使用MIsh激活函数代替了原来的Leaky ReLU。在YOLOv3中，每个卷积层之后包含一个批量归一化层和一个Leaky ReLU。而在YOLOv4的主干网络CSPDarknet53中，使用Mish代替了原来的Leaky ReLU。</p>
<p>理解：数学公式是如此的神奇。</p>
<p>深入研究<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1911.11929.pdf">CSPNet论文</a><br><a target="_blank" rel="noopener" href="https://github.com/WongKinYiu/CrossStagePartialNetworks">CSPNet开源地址</a>  </p>
<h3 id="4-3-2-SPP"><a href="#4-3-2-SPP" class="headerlink" title="4.3.2 SPP"></a>4.3.2 SPP</h3><p>SPP最初的设计目的是用来使卷积神经网络不受固定输入尺寸的限制。在YOLOv4中，作者引入SPP，是因为它显著地增加了感受野，分离出了最重要的上下文特征，并且几乎不会降低的YOLOv4运行速度。SPP中经典的空间金字塔池化层。  </p>
<p>在YOLOv4中，具体的做法就是：分别利用四个不同尺度的最大池化对上层输出的feature map进行处理。最大池化的池化核大小分别为13x13、9x9、5x5、1x1，其中1x1就相当于不处理。<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/img_convert/25a6c1825b22733dfab2fedcc8713d2d.png" alt="avatar"><br>注意：这里最大池化采用padding操作，移动的步长为1，比如13×13的输入特征图，使用5×5大小的池化核池化，padding&#x3D;2，因此池化后的特征图仍然是13×13大小。</p>
<p>深入研究<a href="https://link.csdn.net/?target=https://link.springer.com/content/pdf/10.1007/978-3-319-10578-9_23.pdf">SPP论文</a></p>
<h3 id="4-3-3-PANet"><a href="#4-3-3-PANet" class="headerlink" title="4.3.3 PANet"></a>4.3.3 PANet</h3><p>PANet整体上可以看做是在Mask R-CNN上做多处改进，充分利用了特征融合，比如引入Bottom-up path augmentation结构，充分利用网络浅特征进行分割；引入Adaptive feature pooling使得提取到的ROI特征更加丰富；引入Fully-conneFcted fusion，通过融合一个前背景二分类支路的输出得到更加精确的分割结果。</p>
<p>主要包含FPN、Bottom-up path augmentation、Adaptive feature pooling、Fully-connected fusion四个部分   </p>
<p><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/20200730142333938.png" alt="avatar">  </p>
<p>FPN:主要是通过融合高低层特征提升目标检测的效果，尤其可以提高小尺寸目标的检测效果。<br>Bottom-up Path Augmentation:主要是考虑网络浅层特征信息对于实例分割非常重要，因为浅层特征一般是边缘形状等特征。<br>Adaptive Feature Pooling:用来特征融合。也就是用每个ROI提取不同层的特征来做融合，这对于提升模型效果显然是有利无害。<br>Fully-connected Fusion:针对原有的分割支路（FCN）引入一个前背景二分类的全连接支路，通过融合这两条支路的输出得到更加精确的分割结果。  </p>
<p>理解：<br>在YOLOv4中，作者使用PANet代替YOLOv3中的FPN作为参数聚合的方法，针对不同的检测器级别从不同的主干层进行参数聚合。并且对原PANet方法进行了修改, 使用张量连接(concat)代替了原来的捷径连接(shortcut connection)。  </p>
<p>思考：<br>YOLO v3中：<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/img_convert/18dba1a64b98c8d019befc37598f2ea2.png" alt="avatar"><br>其中neck部分：我们将Neck部分用立体图画出来，更直观的看下两部分之间是如何通过FPN结构融合的。<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/img_convert/70ac463cf2d23b2ec96283a0b78cf983.png" alt="avatar"><br>以及最后的Prediction中用于预测的三个特征图①19<em>19</em>255、②38<em>38</em>255、③76<em>76</em>255。[注：255表示80类别(1+4+80)×3&#x3D;255]<br>如图所示，FPN是自顶向下的，将高层的特征信息通过上采样的方式进行传递融合，得到进行预测的特征图。  </p>
<p>YOLO v4中：<br>而Yolov4中Neck这部分除了使用FPN外，还在此基础上使用了PAN结构：<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/img_convert/273816f9e51dbcbd75e6a7ee9967ea4b.png" alt="avatar"><br>前面CSPDarknet53中讲到，每个CSP模块前面的卷积核都是3<em>3大小，步长为2，相当于下采样操作。因此可以看到三个紫色箭头处的特征图是76</em>76、38<em>38、19</em>19。以及最后Prediction中用于预测的三个特征图：①76<em>76</em>255，②38<em>38</em>255，③19<em>19</em>255。我们也看下Neck部分的立体图像，看下两部分是如何通过FPN+PAN结构进行融合的。<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/img_convert/7a77751f54d9115247ff00705c013952.png" alt="avatar"><br>和Yolov3的FPN层不同，Yolov4在FPN层的后面还添加了一个自底向上的特征金字塔。<br>其中包含两个PAN结构。<br>这样结合操作，FPN层自顶向下传达强语义特征，而特征金字塔则自底向上传达强定位特征，两两联手，从不同的主干层对不同的检测层进行参数聚合,这样的操作确实很皮。  </p>
<p>注意：<br>注意一：Yolov3的FPN层输出的三个大小不一的特征图①②③直接进行预测。但Yolov4的FPN层，只使用最后的一个76x76特征图①，而经过两次PAN结构，输出预测的特征图②和③。这里的不同也体现在cfg文件中，这一点有很多同学之前不太明白，比如Yolov3.cfg最后的三个Yolo层，第一个Yolo层是最小的特征图19x19，mask&#x3D;6,7,8，对应最大的anchor box。第二个Yolo层是中等的特征图38x38，mask&#x3D;3,4,5，对应中等的anchor box第三个Yolo层是最大的特征图76x76，mask&#x3D;0,1,2，对应最小的anchor box。而Yolov4.cfg则恰恰相反第一个Yolo层是最大的特征图76x76，mask&#x3D;0,1,2，对应最小的anchor box。第二个Yolo层是中等的特征图38x38，mask&#x3D;3,4,5，对应中等的anchor box。第三个Yolo层是最小的特征图19*19，mask&#x3D;6,7,8，对应最大的anchor box。  </p>
<p>注意点二：原本的PANet网络的PAN结构中，两个特征图结合是采用shortcut操作，而Yolov4中则采用concat（route）操作，特征图融合后的尺寸发生了变化。<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/img_convert/467b7d2a4e3bb426b8d03203e4ffaa53.png" alt="avatar"></p>
<h3 id="4-3-4-YOLOv3-Head"><a href="#4-3-4-YOLOv3-Head" class="headerlink" title="4.3.4 YOLOv3 Head"></a>4.3.4 YOLOv3 Head</h3><p>在YOLOv4中，继承了YOLOv3的Head进行多尺度预测，提高了对不同size目标的检测性能。YOLOv3的完整结构在上文已经详细介绍，下面我们截取了YOLOv3的Head进行分析：  </p>
<p>YOLOv4学习了YOLOv3的方式，采用三个不同层级的特征图进行融合，并且继承了YOLOv3的Head。在COCO数据集上训练时，YOLOv4的3个输出张量的shape分别是：（19，19，225）、（38，38，255）、（76，76，225）。这是因为COCO有80个类别，并且每一个网格对应3个Anchor boxes，而每个要预测的bounding box对应的5个值（tx、ty、tw、th、to),所以有:3 x (80+5)&#x3D;255  </p>
<h3 id="4-3-5-各种Tricks总结"><a href="#4-3-5-各种Tricks总结" class="headerlink" title="4.3.5 各种Tricks总结"></a>4.3.5 各种Tricks总结</h3><p>作者将所有的Tricks可以分为两类：</p>
<p>在不增加推理成本的前提下获得更好的精度，而只改变训练策略或只增加训练成本的方法，作着称之为 “免费包”（Bag of freebies）；<br>只增加少量推理成本但能显著提高目标检测精度的插件模块和后处理方法，称之为“特价包”（Bag of specials）</p>
<p>下面分别对这两类技巧进行介绍</p>
<p>（1）免费包<br>以数据增强方法为例，虽然增加了训练时间，但不增加推理时间，并且能让模型泛化性能和鲁棒性更好。像这种不增加推理成本，还能提升模型性能的方法，作者称之为”免费包”，非常形象。<br>下面总结了一些常用的数据增强方法：</p>
<p>随机缩放<br>翻转、旋转<br>图像扰动、加噪声、遮挡<br>改变亮度、对比对、饱和度、色调<br>随机裁剪（random crop）<br>随机擦除（random erase）<br>Cutout<br>MixUp<br>CutMix  </p>
<p>常见的正则化方法有：</p>
<p>DropOut<br>DropConnect<br>DropBlock  </p>
<p>传统的Dropout很简单，一句话就可以说的清：随机删除减少神经元的数量，使网络变得更简单。<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/img_convert/671aad4ffe97e70163213479eec419e5.png" alt="avatar"><br>而Dropblock和Dropout相似，比如下图:<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/img_convert/b25adf044a54c9259a8475982bc990fa.png" alt="avatar">   </p>
<p>中间Dropout的方式会随机的删减丢弃一些信息，但Dropblock的研究者认为，卷积层对于这种随机丢弃并不敏感，因为卷积层通常是三层连用：卷积+激活+池化层，池化层本身就是对相邻单元起作用。而且即使随机丢弃，卷积层仍然可以从相邻的激活单元学习到相同的信息。因此，在全连接层上效果很好的Dropout在卷积层上效果并不好。<br>所以右图Dropblock的研究者则干脆整个局部区域进行删减丢弃。<br>这种方式其实是借鉴2017年的cutout数据增强的方式，cutout是将输入图像的部分区域清零，而Dropblock则是将Cutout应用到每一个特征图。而且并不是用固定的归零比率，而是在训练时以一个小的比率开始，随着训练过程线性的增加这个比率。   </p>
<p>Dropblock的研究者与Cutout进行对比验证时，发现有几个特点：<br>优点一：Dropblock的效果优于Cutout<br>优点二：Cutout只能作用于输入层，而Dropblock则是将Cutout应用到网络中的每一个特征图上<br>优点三：Dropblock可以定制各种组合，在训练的不同阶段可以修改删减的概率，从空间层面和时间层面，和Cutout相比都有更精细的改进。<br>Yolov4中直接采用了更优的Dropblock，对网络的正则化过程进行了全面的升级改进。</p>
<p>平衡正负样本的方法有：</p>
<p>Focal loss<br>OHEM(在线难分样本挖掘)  </p>
<p>除此之外，还有回归 损失方面的改进：</p>
<p>GIOU<br>DIOU<br>CIoU  </p>
<p>（1）CIOU_loss<br>目标检测任务的损失函数一般由Classificition Loss（分类损失函数）和Bounding Box Regeression Loss（回归损失函数）两部分构成。<br>Bounding Box Regeression的Loss近些年的发展过程是：Smooth L1 Loss-&gt; IoU Loss（2016）-&gt; GIoU Loss（2019）-&gt; DIoU Loss（2020）-&gt;CIoU Loss（2020）<br>我们从最常用的IOU_Loss开始，进行对比拆解分析，看下Yolov4为啥要选择CIOU_Loss。<br>a.IOU_Loss<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/img_convert/34096cdf5a9a844bc9049cc50da95858.png" alt="avatar"><br>可以看到IOU的loss其实很简单，主要是交集&#x2F;并集，但其实也存在两个问题。<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/img_convert/dedcabfce67835e2b61d9ed9a79da46f.png" alt="avatar"><br>问题1：即状态1的情况，当预测框和目标框不相交时，IOU&#x3D;0，无法反应两个框距离的远近，此时损失函数不可导，IOU_Loss无法优化两个框不相交的情况。  </p>
<p>问题2：即状态2和状态3的情况，当两个预测框大小相同，两个IOU也相同，IOU_Loss无法区分两者相交情况的不同。</p>
<p>因此2019年出现了GIOU_Loss来进行改进。<br>b.GIOU_Loss<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/img_convert/afa33fb2b025a1ba4832b72cda5495a4.png" alt="avatar"><br>可以看到右图GIOU_Loss中，增加了相交尺度的衡量方式，缓解了单纯IOU_Loss时的尴尬。<br>但为什么仅仅说缓解呢？<br>因为还存在一种不足：<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/img_convert/dc250542414037a625b7a0332a9f34b3.png" alt="avatar"><br>问题：状态1、2、3都是预测框在目标框内部且预测框大小一致的情况，这时预测框和目标框的差集都是相同的，因此这三种状态的GIOU值也都是相同的，这时GIOU退化成了IOU，无法区分相对位置关系。<br>基于这个问题，2020年的AAAI又提出了DIOU_Loss。<br>c.DIOU_Loss<br>好的目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比。<br>针对IOU和GIOU存在的问题，作者从两个方面进行考虑<br>一：如何最小化预测框和目标框之间的归一化距离？<br>二：如何在预测框和目标框重叠时，回归的更准确？<br>针对第一个问题，提出了DIOU_Loss（Distance_IOU_Loss）<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/img_convert/f114b3726da623a4bb6dfcc151fc3d61.png" alt="avatar"><br>DIOU_Loss考虑了重叠面积和中心点距离，当目标框包裹预测框的时候，直接度量2个框的距离，因此DIOU_Loss收敛的更快。</p>
<p>但就像前面好的目标框回归函数所说的，没有考虑到长宽比。<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/img_convert/086fb149562f4574145a9334a6eb9d49.png" alt="avatar"><br>比如上面三种情况，目标框包裹预测框，本来DIOU_Loss可以起作用。<br>但预测框的中心点的位置都是一样的，因此按照DIOU_Loss的计算公式，三者的值都是相同的。<br>针对这个问题，又提出了CIOU_Loss，不对不说，科学总是在解决问题中，不断进步！！  </p>
<p>d.CIOU_Loss<br>CIOU_Loss和DIOU_Loss前面的公式都是一样的，不过在此基础上还增加了一个影响因子，将预测框和目标框的长宽比都考虑了进去。<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/img_convert/aec09181721c0fc8f7efd1d53f065a2b.png" alt="avatar"><br>人类好强，这式子怎么推导出来的啊，我跪了<br>其中v是衡量长宽比一致性的参数。<br>这样CIOU_Loss就将目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比全都考虑进去了。<br>再来综合的看下各个Loss函数的不同点：<br>IOU_Loss：主要考虑检测框和目标框重叠面积。<br>GIOU_Loss：在IOU的基础上，解决边界框不重合时的问题。<br>DIOU_Loss：在IOU和GIOU的基础上，考虑边界框中心点距离的信息。<br>CIOU_Loss：在DIOU的基础上，考虑边界框宽高比的尺度信息。<br>Yolov4中采用了CIOU_Loss的回归方式，使得预测框回归的速度和精度更高一些。  </p>
<p>（2）DIOU_nms  </p>
<p>Nms主要用于预测框的筛选，常用的目标检测算法中，一般采用普通的nms的方式。   </p>
<p>问题思考：<br>1.这里为什么不用CIOU_nms，而用DIOU_nms?</p>
<p>答：因为前面讲到的CIOU_loss，是在DIOU_loss的基础上，添加的影响因子，包含groundtruth标注框的信息，在训练时用于回归。但在测试过程中，并没有groundtruth的信息，不用考虑影响因子，因此直接用DIOU_nms即可。<br>总体来说，YOLOv4的论文称的上良心之作，将近几年关于深度学习领域最新研究的tricks移植到Yolov4中做验证测试，将Yolov3的精度提高了不少。虽然没有全新的创新，但很多改进之处都值得借鉴，借用Yolov4作者的总结。<br>Yolov4 主要带来了 3 点新贡献：<br>（1）提出了一种高效而强大的目标检测模型，使用 1080Ti 或 2080Ti 就能训练出超快、准确的目标检测器。<br>（2）在检测器训练过程中，验证了最先进的一些研究成果对目标检测器的影响。<br>（3）改进了 SOTA 方法，使其更有效、更适合单 GPU 训练。<br>（2）特价包  </p>
<p>增大感受野技巧：SPP、ASPP、RFB  </p>
<p>SPP模块:<br>源于空间金字塔匹配（SPM），SPMs最初的方法是将特征映射分成若干个d×d相等的块，其中d可以是{1,2,3，…}，从而形成空间金字塔，然后提取bag-of-word的特征。SPP将SPM集成到CNN中，并使用最大池化操作代替bag-of-word操作。<br>在YOLOv3的设计中，Redmon和Farhadi将SPP模块改进为最大池输出与核大小k×k的串联，其中k&#x3D;{1,5,9,13}，步长等于1。在这种设计下，一个较大的k×k最大池化有效地增加了主干特征的感受野。在添加了SPP模块的改进版本后，YOLOv3-608在MS-COCO目标检测任务上将AP50 升级了2.7%，但增加了0.5%的计算量。<br>ASPP模块：<br>与改进的SPP模块在运算上的差异主要是源于原始的k×k核大小，在扩张卷积运算中，最大池化的步幅等于1到几个3×3核大小，扩张比等于k，步长等于1。<br>RFB模块：<br>是利用k×k核的几个展开卷积，展开比为k，步长为1，得到比ASPP更全面的空间覆盖。RFB仅需7%的额外推理时间，即可将COCO上的SSD的AP50提高5.7%。</p>
<p>注意力机制：Squeeze-and-Excitation (SE)、Spatial Attention Module (SAM)<br>（目标检测中常用的注意模块主要分为通道式注意和点式注意。这两种注意模型的代表分别是：挤压与激励（SE）和 空间注意模块（SAM））<br>SE模块：<br>在ImageNet图像分类任务中可以将ResNet50的能力提高1%的top1精度。但在GPU上通常会增加10%左右的推理时间，因此更适合用于移动设备。</p>
<p>SAM模块：<br>只需要额外消耗0.1%的计算量，就可以提高resnet50se在ImageNet图像分类任务中的0.5%的top1精度。最重要的是，它根本不影响GPU上的推理速度。  </p>
<p>特征融合集成：FPN、SFAM、ASFF、BiFPN （出自于大名鼎鼎的EfficientDet）  </p>
<p>（在特征集成方面，早期的方法是使用跳转连接（skip connection）或超列（hyper-column ）将低级物理特征集成到高级语义特征。 随着FPN等多尺度预测方法的广泛应用，人们提出了许多集成不同特征金字塔的轻量级模块。这类模块包括SFAM、ASFF和BiFPN。）  </p>
<p>SFAM：主要思想是利用SE模块对多尺度级联特征映射进行通道级加权。<br>ASFF：它使用softmax作为逐点加权，然后添加不同比例尺的特征地图。<br>BiFPN：提出了多输入加权残差连接进行尺度层次加权，然后加入不同尺度的特征映射  </p>
<p>更好的激活函数：ReLU、LReLU、PReLU、ReLU6、SELU、Swish、hard-Swish  </p>
<p>当梯度小于零的时候，首要的目的是解梯度小于零的问题。至于ReLU6和hard Swish，它们是专门为量化网络设计的。对于神经网络的自规范化，提出了SELU激活函数来满足这一目标。需要注意的是Swish和Mish都是连续可微的激活函数。基于深度学习的目标检测中常用的后处理方法是非极大抑制（NMS），它可以用来过滤那些对同一目标预测不好的Bounding box，只保留响应较高的候选Bounding box，NMS试图改进的方法与优化目标函数的方法是一致的。</p>
<p>后处理非极大值抑制算法：soft-NMS、DIoU NMS  </p>
<h3 id="4-3-6-改进方法"><a href="#4-3-6-改进方法" class="headerlink" title="4.3.6 改进方法"></a>4.3.6 改进方法</h3><p>（1）Mosaic：<br>这是作者提出的一种新的数据增强方法，该方法借鉴了CutMix数据增强方式的思想。CutMix数据增强方式利用两张图片进行拼接，但是Mosaic使利用四张图片进行拼接。Mosaic数据增强方法有一个优点：拥有丰富检测目标的背景，并且在BN计算的时候一次性会处理四张图片。<br>（2）SAT<br>SAT是一种自对抗训练数据增强方法，这一种新的对抗性训练方式。在第一阶段，神经网络改变原始图像而不改变网络权值。以这种方式，神经网络对自身进行对抗性攻击，改变原始图像，以制造图像上没有所需对象的欺骗。在第二阶段，用正常的方法训练神经网络去检测目标。<br>（3）CmBN<br>CmBN的全称是Cross mini-Batch Normalization，定义为跨小批量标准化（CmBN）。CmBN 是 CBN 的改进版本，它用来收集一个batch内多个mini-batch内的统计数据。<br>（４）修改过的SAM<br>作者在原SAM（Spatial Attention Module）方法上进行了修改，将SAM从空间注意修改为点注意。如下图所示，对于常规的SAM，最大值池化层和平均池化层分别作用于输入的feature map，得到两组shape相同的feature map，再将结果输入到一个卷积层，接着是一个 Sigmoid 函数来创建空间注意力。<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/20200729172700153.png" alt="avatar"><br>将SAM（Spatial Attention Module）应用于输入特征，能够输出精细的特征图。<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/20200729172909382.jpg" alt="avatar"><br>在YOLOv4中，对原来的SAM方法进行了修改。如下图所示，修改后的SAM直接使用一个卷积层作用于输入特征，得到输出特征，然后再使用一个Sigmoid 函数来创建注意力。作者认为，采用这种方式创建的是点注意力。<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/20200729173158239.png" alt="avatar"><br>（５）修改过的PAN<br>作者对原PAN(Path Aggregation Network)方法进行了修改, 使用张量连接(concat)代替了原来的快捷连接(shortcut connection)。如下图所示：<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/20200730003639808.png" alt="avatar">   </p>
<h1 id="5-YOLO-v5"><a href="#5-YOLO-v5" class="headerlink" title="5. YOLO v5"></a>5. YOLO v5</h1><h2 id="5-1-基本概念"><a href="#5-1-基本概念" class="headerlink" title="5.1 基本概念"></a>5.1 基本概念</h2><p>YOLOv5是一个在COCO数据集上预训练的物体检测架构和模型系列，它代表了Ultralytics对未来视觉AI方法的开源研究，其中包含了经过数千小时的研究和开发而形成的经验教训和最佳实践。<br>YOLOv5是YOLO系列的一个延申，也可以看作是基于YOLOv3、YOLOv4的改进作品。YOLOv5没有相应的论文说明，但是作者在Github上积极地开放源代码。  </p>
<h2 id="5-2-基本框架"><a href="#5-2-基本框架" class="headerlink" title="5.2 基本框架"></a>5.2 基本框架</h2><p><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/img_convert/4af4b621a93a6811a6a69308d71284b6.png" alt="avatar"><br>由上图可知，YOLO v5主要由输入端、Backone、Neck以及Prediction四部分组成。其中：<br>(1) Backbone：在不同图像细粒度上聚合并形成图像特征的卷积神经网络。<br>(2) Neck：一系列混合和组合图像特征的网络层，并将图像特征传递到预测层。<br>(3) Head： 对图像特征进行预测，生成边界框和并预测类别。<br>下面介绍YOLO v5各部分网络包括的基础组件：<br>CBL：由Conv+BN+Leaky_relu激活函数组成<br>Res unit：借鉴ResNet网络中的残差结构，用来构建深层网络<br>CSP1_X：借鉴CSPNet网络结构，该模块由CBL模块、Res unint模块以及卷积层、Concate组成<br>CSP2_X：借鉴CSPNet网络结构，该模块由卷积层和X个Res unint模块Concate组成而成<br>Focus：首先将多个slice结果Concat起来，然后将其送入CBL模块中<br>SPP：采用1×1、5×5、9×9和13×13的最大池化方式，进行多尺度特征融合  </p>
<h2 id="5-3-框架详解"><a href="#5-3-框架详解" class="headerlink" title="5.3 框架详解"></a>5.3 框架详解</h2><h3 id="5-3-1-输入端详解"><a href="#5-3-1-输入端详解" class="headerlink" title="5.3.1 输入端详解"></a>5.3.1 输入端详解</h3><p>YOLO v5使用Mosaic数据增强操作提升模型的训练速度和网络的精度；并提出了一种自适应锚框计算与自适应图片缩放方法  </p>
<h3 id="5-3-1-1-Mosaic数据增强"><a href="#5-3-1-1-Mosaic数据增强" class="headerlink" title="5.3.1.1 Mosaic数据增强"></a>5.3.1.1 Mosaic数据增强</h3><p>Mosaic数据增强利用四张图片，并且按照随机缩放、随机裁剪和随机排布的方式对四张图片进行拼接，每一张图片都有其对应的框，将四张图片拼接之后就获得一张新的图片，同时也获得这张图片对应的框，然后我们将这样一张新的图片传入到神经网络当中去学习，相当于一下子传入四张图片进行学习了。该方法极大地丰富了检测物体的背景，且在标准化BN计算的时候一下子计算四张图片的数据，所以本身对batch size不是很依赖。（和v4一样，不要大惊小怪）</p>
<h2 id="5-3-1-2-自适应锚框计算"><a href="#5-3-1-2-自适应锚框计算" class="headerlink" title="5.3.1.2 自适应锚框计算"></a>5.3.1.2 自适应锚框计算</h2><p>在yolo系列算法中，针对不同的数据集，都需要设定特定长宽的锚点框。在网络训练阶段，模型在初始阶段，模型在初始锚点框的基础上输出对应的预测框，计算其与GT框之间的差距，并执行反向更新操作，从而更新整个网络的参数，因此设定初始锚点框是比较关键的一环。<br>在yolo V3和yolo V4中，训练不同的数据集，都是通过单独的程序运行来获得初始锚点框。<br>而在yoloV5中将此功能嵌入到代码中，每次训练，根据数据集的名称自适应的计算出最佳的锚点框，用户可以根据自己的需求将功能关闭或者打开，指令为：<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/img_convert/8fbfc6822901565729df4ffc08bfaeca.png" alt="avatar">  </p>
<h2 id="5-3-1-3-自适应图片缩放"><a href="#5-3-1-3-自适应图片缩放" class="headerlink" title="5.3.1.3 自适应图片缩放"></a>5.3.1.3 自适应图片缩放</h2><p>在目标检测算法中，不同的图片长宽都不相同，因此常用的方式是将原始图片统一缩放到一个标准尺寸，再送入检测网络中。而原始的缩放方法存在着一些问题，由于在实际的使用中的很多图片的长宽比不同，因此缩放填充之后，两端的黑边大小都不相同，然而如果填充的过多，则会存在大量的信息冗余，从而影响整个算法的推理速度。为了进一步提升YOLO v5的推理速度，该算法提出一种方法能够自适应的添加最少的黑边到缩放之后的图片中。具体的实现步骤如下所述：</p>
<p>(1) 根据原始图片大小以及输入到网络的图片大小计算缩放比例<br>如：800x600照片缩放成416x416<br>a.416&#x2F;800&#x3D;0.52 416&#x2F;600&#x3D;0.69<br>b.根据原始图片大小与缩放比例计算缩放后的图片大小<br>800x0.52&#x3D;416 600x0.52&#x3D;312<br>c.计算黑边填充数值<br>416-312&#x3D;104 np.mod(104,32)&#x3D;8 8&#x2F;2&#x3D;4<br>其中，416表示YOLO v5网络所要求的图片宽度，312表示缩放后图片的宽度。首先执行相减操作来获得需要填充的黑边长度104；然后对该数值执行取余操作，即104取余32&#x3D;8，使用32是因为整个YOLOv5网络执行了5次下采样操作。最后对该数值除以2，即将填充的区域分散到两边。这样将416<em>416大小的图片缩小到416</em>320大小，因而极大的提升了算法的推理速度。  </p>
<h3 id="5-3-2-Backone网络"><a href="#5-3-2-Backone网络" class="headerlink" title="5.3.2 Backone网络"></a>5.3.2 Backone网络</h3><h4 id="5-3-2-1-Focus结构"><a href="#5-3-2-1-Focus结构" class="headerlink" title="5.3.2.1 Focus结构"></a>5.3.2.1 Focus结构</h4><p>Focus对图片进行切片操作，具体操作是在一张图片中每隔一个像素拿到一个值，类似于邻近下采样，这样就拿到了四张图片，四张图片互补，长的差不多，但是没有信息丢失，这样一来，将W、H信息就集中到了通道空间，输入通道扩充了4倍，即拼接起来的图片相对于原先的RGB三通道模式变成了12个通道，最后将得到的新图片再经过卷积操作，最终得到了没有信息丢失情况下的二倍下采样特征图。如下图所示，原始输入图片大小为608<em>608</em>3，经过Slice与Concat操作之后输出一个304<em>304</em>12的特征映射；接着经过一个通道个数为32的Conv层，输出一个304<em>304</em>32大小的特征映射。(图二解释：Focus重要的是切片操作，4x4x3的图像切片后变成2x2x12的特征图,，原始608x608x3的图像输入Focus结构，采用切片操作，先变成304x304x12的特征图，再经过一次32个卷积核的卷积操作，最终变成304x304x32的特征图。)<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/20210212091942352.jpg" alt="avatar"><br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/7fda44666b5c474ba9287e856a11ab74.png#pic_center" alt="avatar"></p>
<h3 id="5-3-2-2-CSP结构"><a href="#5-3-2-2-CSP结构" class="headerlink" title="5.3.2.2 CSP结构"></a>5.3.2.2 CSP结构</h3><p>CSPNet主要是将feature map拆成两个部分，一部分进行卷积操作，另一部分和上一部分卷积操作的结果进行concate。在分类问题中，使用CSPNet可以降低计算量，但是准确率提升很小；<br>在目标检测问题中，使用CSPNet作为Backbone带来的提升比较大，可以有效增强CNN的学习能力，同时也降低了计算量。yolo V5设计了两种CSP结构，CSP1_X结构应用于Backbone网络中，CSP2_X结构应用于Neck网络中。  </p>
<h3 id="5-3-2-3-Neck网络"><a href="#5-3-2-3-Neck网络" class="headerlink" title="5.3.2.3 Neck网络"></a>5.3.2.3 Neck网络</h3><p>在YOLO v4中开始使用FPN-PAN。FPN层自顶向下传达强语义特征，而PAN塔自底向上传达定位特征.<br>yoloV5的Neck仍采用了FPN+PAN结构，但是在它的基础上做了一些改进操作，yoloV4的Neck结构中，采用的都是普通的卷积操作，而yoloV5的Neck中，采用CSPNet设计的CSP2结构，从而加强了网络特征融合能力。(和v4一样啊啊啊啊啊啊)  </p>
<h3 id="5-3-2-4-Head网络"><a href="#5-3-2-4-Head网络" class="headerlink" title="5.3.2.4 Head网络"></a>5.3.2.4 Head网络</h3><p>YOLO v5采用CIOU_LOSS 作为bounding box 的损失函数。V4中已经对IOU_Loss、GIOU_Loss、DIOU_Loss以及CIOU_Loss介绍完了，向上翻。  </p>
<h1 id="6-YOLO-v6"><a href="#6-YOLO-v6" class="headerlink" title="6. YOLO v6"></a>6. YOLO v6</h1><h2 id="6-1-基本概念"><a href="#6-1-基本概念" class="headerlink" title="6.1 基本概念"></a>6.1 基本概念</h2><p>YOLO v6框架支持模型训练、推理及多平台部署等全链条的工业应用需求，并在网络结构、训练策略等算法层面进行了多项改进和优化，在 COCO 数据集上，YOLOv6 在精度和速度方面均超越其他同体量算法。</p>
<h2 id="6-2-基本框架"><a href="#6-2-基本框架" class="headerlink" title="6.2 基本框架"></a>6.2 基本框架</h2><p>YOLOv6 主要在 Backbone、Neck、Head 以及训练策略等方面进行了诸多的改进：  </p>
<p>（1）统一设计了更高效的 Backbone 和 Neck ：受到硬件感知神经网络设计思想的启发，基于 RepVGG style设计了可重参数化、更高效的骨干网络 EfficientRep Backbone 和 Rep-PAN Neck。  </p>
<p>（2）优化设计了更简洁有效的 Efficient Decoupled Head，在维持精度的同时，进一步降低了一般解耦头带来的额外延时开销。  </p>
<p>（3）在训练策略上，我们采用Anchor-free 无锚范式，同时辅以 SimOTA 标签分配策略以及 SIoU边界框回归损失来进一步提高检测精度。  </p>
<h3 id="6-2-1-Hardware-friendly-的骨干网络设计"><a href="#6-2-1-Hardware-friendly-的骨干网络设计" class="headerlink" title="6.2.1 Hardware-friendly 的骨干网络设计"></a>6.2.1 Hardware-friendly 的骨干网络设计</h3><p>YOLOv5&#x2F;YOLOX 使用的 Backbone 和 Neck 都基于 CSPNet搭建，采用了多分支的方式和残差结构。对于 GPU 等硬件来说，这种结构会一定程度上增加延时，同时减小内存带宽利用率。  </p>
<p>基于硬件感知神经网络设计的思想，对 Backbone 和 Neck 进行了重新设计和优化。对上述重新设计的两个检测部件，我们在 YOLOv6 中分别称为 EfficientRep Backbone 和 Rep-PAN Neck，其主要贡献点在于：引入了 RepVGGstyle 结构。基于硬件感知思想重新设计了 Backbone 和 Neck。</p>
<p>RepVGG Style 结构是一种在训练时具有多分支拓扑，而在实际部署时可以等效融合为单个 3x3 卷积的一种可重参数化的结构。通过融合成的 3x3 卷积结构，可以有效利用计算密集型硬件计算能力（比如 GPU）。</p>
<p>通过上述策略，YOLOv6 减少了在硬件上的延时，并显著提升了算法的精度，让检测网络更快更强。以 nano 尺寸模型为例，对比 YOLOv5-nano 采用的网络结构，本方法在速度上提升，同时精度提升。<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/img_convert/968c775928fdc160dce740aab3b9ce33.png" alt="avatar"><br>EfficientRep Backbone：在 Backbone 设计方面，基于以上 Rep 算子设计了一个高效的Backbone。相比于 YOLOv5 采用的 CSP-Backbone，该Backbone 能够高效利用硬件（如 GPU）算力的同时，还具有较强的表征能力。</p>
<p>下图 4 为 EfficientRep Backbone 具体设计结构图，我们将 Backbone 中 stride&#x3D;2 的普通 Conv 层替换成了 stride&#x3D;2 的 RepConv层。同时，将原始的 CSP-Block 都重新设计为 RepBlock，其中 RepBlock 的第一个 RepConv 会做 channel 维度的变换和对齐。另外，我们还将原始的 SPPF 优化设计为更加高效的 SimSPPF。<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/img_convert/fd3b9eae4967a4e2c01a0edae4e0fe9d.png" alt="avatar"><br>Rep-PAN：在 Neck 设计方面，为了让其在硬件上推理更加高效，以达到更好的精度与速度的平衡，我们基于硬件感知神经网络设计思想，为 YOLOv6 设计了一个更有效的特征融合网络结构。</p>
<p>Rep-PAN 基于 PAN[6] 拓扑方式，用 RepBlock 替换了 YOLOv5 中使用的 CSP-Block，同时对整体 Neck 中的算子进行了调整，目的是在硬件上达到高效推理的同时，保持较好的多尺度特征融合能力（Rep-PAN 结构图如下图 5 所示）。<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/img_convert/6c88fd43f84d634e2a86fe4774a9ce7f.png" alt="avatar">  </p>
<h3 id="6-2-2-更简洁高效的-Decoupled-Head"><a href="#6-2-2-更简洁高效的-Decoupled-Head" class="headerlink" title="6.2.2 更简洁高效的 Decoupled Head"></a>6.2.2 更简洁高效的 Decoupled Head</h3><p>在 YOLOv6 中，采用了解耦检测头（Decoupled Head）结构，并对其进行了精简设计。原始 YOLOv5 的检测头是通过分类和回归分支融合共享的方式来实现的。</p>
<p>因此，对解耦头进行了精简设计，同时综合考虑到相关算子表征能力和硬件上计算开销这两者的平衡，采用 Hybrid Channels 策略重新设计了一个更高效的解耦头结构，在维持精度的同时降低了延时，缓解了解耦头中 3x3 卷积带来的额外延时开销。通过在 nano 尺寸模型上进行消融实验，对比相同通道数的解耦头结构，精度提升的同时，速度提升。<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/img_convert/52103b3dfc018e6877b189b8bf9da48e.png" alt="avatar">  </p>
<h3 id="6-2-3-更有效的训练策略"><a href="#6-2-3-更有效的训练策略" class="headerlink" title="6.2.3 更有效的训练策略"></a>6.2.3 更有效的训练策略</h3><p>为了进一步提升检测精度，我们吸收借鉴了学术界和业界其他检测框架的先进研究进展：Anchor-free 无锚范式 、SimOTA 标签分配策略以及 SIoU 边界框回归损失。<br>(1)Anchor-free 无锚范式  </p>
<p>YOLOv6 采用了更简洁的 Anchor-free 检测方法。由于 Anchor-based检测器需要在训练之前进行聚类分析以确定最佳 Anchor 集合，这会一定程度提高检测器的复杂度；同时，在一些边缘端的应用中，需要在硬件之间搬运大量检测结果的步骤，也会带来额外的延时。而 Anchor-free 无锚范式因其泛化能力强，解码逻辑更简单，在近几年中应用比较广泛。经过对 Anchor-free 的实验调研，我们发现，相较于Anchor-based 检测器的复杂度而带来的额外延时，Anchor-free 检测器在速度上有51%的提升。  </p>
<p>(2)SimOTA 标签分配策略  </p>
<p>为了获得更多高质量的正样本，YOLOv6 引入了 SimOTA 算法动态分配正样本，进一步提高检测精度。YOLOv5 的标签分配策略是基于 Shape 匹配，并通过跨网格匹配策略增加正样本数量，从而使得网络快速收敛，但是该方法属于静态分配方法，并不会随着网络训练的过程而调整。  </p>
<p>(3)SIoU 边界框回归损失  </p>
<p>为了进一步提升回归精度，YOLOv6 采用了 SIoU边界框回归损失函数来监督网络的学习。目标检测网络的训练一般需要至少定义两个损失函数：分类损失和边界框回归损失，而损失函数的定义往往对检测精度以及训练速度产生较大的影响。  </p>
<p>近年来，常用的边界框回归损失包括IoU、GIoU、CIoU、DIoU loss等等，这些损失函数通过考虑预测框与目标框之前的重叠程度、中心点距离、纵横比等因素来衡量两者之间的差距，从而指导网络最小化损失以提升回归精度，但是这些方法都没有考虑到预测框与目标框之间方向的匹配性。SIoU 损失函数通过引入了所需回归之间的向量角度，重新定义了距离损失，有效降低了回归的自由度，加快网络收敛，进一步提升了回归精度。通过在 YOLOv6s 上采用 SIoU loss 进行实验，对比 CIoU loss，平均检测精度提升  </p>
<h1 id="7-YOLO-v7"><a href="#7-YOLO-v7" class="headerlink" title="7. YOLO v7"></a>7. YOLO v7</h1><h2 id="7-1-基本概念"><a href="#7-1-基本概念" class="headerlink" title="7.1 基本概念"></a>7.1 基本概念</h2><p>YOLOv4的团队刚出炉的最新工作- YOLOv7，就是一个字——新鲜，啊，是两个字。。。  </p>
<h2 id="7-2-框架"><a href="#7-2-框架" class="headerlink" title="7.2 框架"></a>7.2 框架</h2><p><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/eecd9a030a7e488eaa2a9ec981f7e6e6.png#pic_center" alt="avatar"><br>我们先整体来看下 YOLOV7，首先对输入的图片 resize 为 640x640 大小，输入到 backbone 网络中，然后经 head 层网络输出三层不同 size 大小的 feature map，经过 Rep 和 conv输出预测结果，这里以 coco 为例子，输出为 80 个类别，然后每个输出(x ,y, w, h, o) 即坐标位置和前后背景，3 是指的 anchor 数量，因此每一层的输出为 (80+5)x3 &#x3D; 255再乘上 feature map 的大小就是最终的输出</p>
<h3 id="7-2-1-扩展的高效层聚合网络"><a href="#7-2-1-扩展的高效层聚合网络" class="headerlink" title="7.2.1 扩展的高效层聚合网络"></a>7.2.1 扩展的高效层聚合网络</h3><p>在大多数关于设计高效架构的文献中，主要考虑因素不超过参数的数量、计算量和计算密度。Ma 等人还从内存访问成本的特点出发，分析了输入&#x2F;输出通道比、架构的分支数量以及 element-wise 操作对网络推理速度的影响。多尔阿尔等人在执行模型缩放时还考虑了激活，即更多地考虑卷积层输出张量中的元素数量。<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/627addfdb10148a99e78e6cf01f3b49d.png" alt="avatar"><br>图 2（b）中 CSPVoVNet 的设计是 VoVNet 的一种变体。CSPVoVNet 的架构除了考虑上述基本设计问题外，还分析了梯度路径，以使不同层的权重能够学习到更多样化的特征。上述梯度分析方法使推理更快、更准确。<br>图 2 (c) 中的 ELAN 考虑了以下设计策略——“如何设计一个高效的网络？”。他们得出了一个结论：通过控制最短最长的梯度路径，更深的网络可以有效地学习和收敛。  </p>
<p>在本文中，作者提出了基于 ELAN 的Extended-ELAN (E-ELAN)，其主要架构如图 2（d）所示。  </p>
<p>无论梯度路径长度和大规模 ELAN 中计算块的堆叠数量如何，它都达到了稳定状态。如果无限堆叠更多的计算块，可能会破坏这种稳定状态，参数利用率会降低。作者提出的E-ELAN使用expand、shuffle、merge cardinality来实现在不破坏原有梯度路径的情况下不断增强网络学习能力的能力。  </p>
<p>在架构方面，E-ELAN 只改变了计算块的架构，而过渡层的架构完全没有改变。策略是使用组卷积来扩展计算块的通道和基数。将对计算层的所有计算块应用相同的组参数和通道乘数。然后，每个计算块计算出的特征图会根据设置的组参数g被打乱成g个组，然后将它们连接在一起。此时，每组特征图的通道数将与原始架构中的通道数相同。最后，添加 g 组特征图来执行合并基数。E-ELAN除了保持原有的ELAN设计架构外，还可以引导不同组的计算块学习更多样化的特征。  </p>
<h3 id="7-2-2-基于concatenate模型的模型缩放"><a href="#7-2-2-基于concatenate模型的模型缩放" class="headerlink" title="7.2.2 基于concatenate模型的模型缩放"></a>7.2.2 基于concatenate模型的模型缩放</h3><p>模型缩放的主要目的是调整模型的一些属性，生成不同尺度的模型，以满足不同推理速度的需求。例如，EfficientNet的缩放模型考虑了宽度、深度和分辨率。对于Scale-yolov4，其缩放模型是调整阶段数。Doll‘ar等人分析了卷积和群卷积对参数量和计算量的影响，并据此设计了相应的模型缩放方法。<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/046bbb8f1b8a49cc80878f7356d71c9d.png" alt="avatar"><br>上述方法主要用于诸如PlainNet或ResNet等架构中。当这些架构在执行放大或缩小过程时，每一层的in-degree和out-degree都不会发生变化，因此可以独立分析每个缩放因子对参数量和计算量的影响。然而，如果这些方法应用于基于concatenate的架构时会发现当扩大或缩小执行深度，基于concatenate的转换层计算块将减少或增加，如图3(a)和(b).所示</p>
<p>从上述现象可以推断，对于基于concatenate的模型不能单独分析不同的缩放因子，而必须一起考虑。以scaling-up depth为例，这样的动作会导致transition layer的输入通道和输出通道的比例发生变化，这可能会导致模型的硬件使用率下降。</p>
<p>因此，必须为基于concatenate的模型提出相应的复合模型缩放方法。当缩放一个计算块的深度因子时，还必须计算该块的输出通道的变化。然后，将对过渡层进行等量变化的宽度因子缩放，结果如图3（c）所示。本文提出的复合缩放方法可以保持模型在初始设计时的特性并保持最佳结构。  </p>
<h2 id="7-3-训练方法"><a href="#7-3-训练方法" class="headerlink" title="7.3 训练方法"></a>7.3 训练方法</h2><h3 id="7-3-1-Planned-re-parameterized-convolution"><a href="#7-3-1-Planned-re-parameterized-convolution" class="headerlink" title="7.3.1 Planned re-parameterized convolution"></a>7.3.1 Planned re-parameterized convolution</h3><p>尽管RepConv在VGG基础上取得了优异的性能，但当将它直接应用于ResNet、DenseNet和其他架构时，它的精度将显著降低。作者使用梯度流传播路径来分析重参数化的卷积应该如何与不同的网络相结合。作者还相应地设计了计划中的重参数化的卷积。  </p>
<p>RepConv实际上结合了3×3卷积，1×1卷积，和在一个卷积层中的id连接。通过分析RepConv与不同架构的组合及其性能，作者发现RepConv中的id连接破坏了ResNet中的残差和DenseNet中的连接，为不同的特征图提供了更多的梯度多样性。<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/b5ab930a3e7641b7aef3b72fe42f4582.png" alt="avatar"><br>基于上述原因，作者使用没有id连接的RepConv(RepConvN)来设计计划中的重参数化卷积的体系结构。在作者的思维中，当具有残差或连接的卷积层被重新参数化的卷积所取代时，不应该存在id连接。图4显示了在PlainNet和ResNet中使用的“Planned re-parameterized convolution”的一个示例。对于基于残差的模型和基于concatenate的模型中Planned re-parameterized convolution实验，它将在消融研究环节中提出。  </p>
<h3 id="7-3-2-标签匹配"><a href="#7-3-2-标签匹配" class="headerlink" title="7.3.2 标签匹配"></a>7.3.2 标签匹配</h3><p>深度监督是一种常用于训练深度网络的技术。其主要概念是在网络的中间层增加额外的auxiliary Head，以及以auxiliary损失为导向的浅层网络权值。即使对于像ResNet和DenseNet这样通常收敛得很好的体系结构，深度监督仍然可以显著提高模型在许多任务上的性能。图5(a)和(b)分别显示了“没有”和“有”深度监督的目标检测器架构。在本文中，将负责最终输出的Head为lead Head，将用于辅助训练的Head称为auxiliary Head。  </p>
<p>过去，在深度网络的训练中，标签分配通常直接指GT，并根据给定的规则生成硬标签。然而，近年来，如果以目标检测为例，研究者经常利用网络预测输出的质量和分布，然后结合GT考虑，使用一些计算和优化方法来生成可靠的软标签。例如，YOLO使用边界框回归预测和GT的IoU作为客观性的软标签。在本文中，将网络预测结果与GT一起考虑，然后将软标签分配为“label assigner”的机制。<br><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/e7097071ffaf4dc08f8c483e1b858222.png" alt="avatar"><br>这里应该算是本文除过E-ELAN第二个贡献很大的地方了。<br>作者使用深度监督思路，如图5的（a）-&gt; (b)， 在网络的中间层添加了额外的辅助头结构（auxiliary head），以此作为辅助损失（assistant loss）来给浅层网络权重提供指导。即网络有auxiliary head和最终的lead head两个头结构。<br>所以针对此模式还尚无专门的标签匹配方法，作者说常用的就是( c )这种给两个头分别安排标签匹配工作。</p>
<p>这里作者先介绍了之前的标签匹配有hard label和soft label两种，其中hard label就是根据固定规则设置正负样本（比如使用iou阈值）。而soft label的生成 就是动态标签匹配，即根据预测输出的质量和分布与ground truth ，使用一些计算和优化方法来生成soft label（比如TAL, OTA, ASSD, SimOTA，YOLO使用BBOX的预测和真值的IOU作为置信度label）。作者将soft label的生成称作“ label assigner”。  </p>
<p>文章中作者提出了图5（d）Lead head guided label assigner 和（e）Coarse-to-fine lead head guided label assigner 两种思路：  </p>
<p>Lead head guided label assigner ：使用lead-head的预测值和真值通过优化过程生成soft label, soft label 用于auxiliary head 和lead-head, 作者将这个学习过程成为一种广义残差学习，通过让更浅层的auxiliary head 直接学习lead-head已经学习到的信息，lead-head将能够更集中的学习还为学习过的residual 信息。   </p>
<p>Coarse-to-fine lead head guided label assigner ： 生成两种soft label， coarse label 和fine label. 其中fine label生成方式和(d）中相同，而coarse label 则通过扩充正样本（更多网格被视为positive target），从而放宽正样本匹配过程，得到更多的正样本。目的是为了让auxiliary head 更专注recall 的优化。由于有些粗标签和细标签的额外权重接近，会造成不好的最终预测结果，所以作者在decoder增加了限制，使coarse positive grid 不能很好的生成soft label。  </p>
<p>作者提的两种思路也仅仅是两种思路，具体的soft label assignment 使用的是什么优化策略，以及loss等都没有介绍，文章说都在appendix中，可惜附录文件也没看到。代码里应该有。  </p>
<h3 id="7-3-3-其他Tricks"><a href="#7-3-3-其他Tricks" class="headerlink" title="7.3.3 其他Tricks"></a>7.3.3 其他Tricks</h3><p>这些免费的训练细节将在附录中详细说明，包括：<br>(1) conv-bn-activation topology中的Batch normalization：这部分主要将batch normalization layer直接连接到卷积层。这样做的目的是在推理阶段将批归一化的均值和方差整合到卷积层的偏差和权重中。  </p>
<p>简单理解：其实就是在推理阶段将BN和BN前的Conv融合，得到一个Conv。</p>
<p>(2) 隐性知识在YOLOR中结合卷积特征图的加法和乘法方式：YOLOR中的隐式知识可以在推理阶段通过预计算简化为向量。该向量可以与前一个或后一个卷积层的偏差和权重相结合。  </p>
<p>(3) EMA 模型（Exponential Moving Average）指数移动平均值：EMA 是一种在 mean teacher 中使用的技术，在系统中使用 EMA 模型纯粹作为最终的推理模型。  </p>
<h2 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h2><p>这里补充一下关于soft label的详细选取方法：采用yolo v5正负样本匹配法（hard label）+simOTA(soft label， 源自YOLOX)的组合方式<br>流程如下：<br>①使用yolov5正负样本分配策略分配正样本。（YOLO v5）<br>②计算每个样本对每个GT的Reg+Cla loss（Loss aware）（simOTA）<br>③使用每个GT的预测样本确定它需要分配到的正样本数（Dynamic k） （simOTA）<br>3.1 先获取和GT的IOU前10的样本，<br>3.2 然后对10个IOU求和取整，作为当前GT的Dynamic k，至少为1<br>3.3 10这个数字不敏感，5~15都可以取<br>④为每个GT取loss最小的前dynamic k个样本作为正样本 （simOTA）<br>⑤人工去掉同一个样本被分配到多个GT的正样本的情况（全局信息） （simOTA）  </p>
<p>YOLO v7的缺点应该是参数量比较大，不适合CPU等和一些低算力NPU，不过作者也说了他面向的对象是各种GPU。我感觉对于一些算力好的NPU应该效果也会很好，毕竟36.9也不是很大。  </p>
<h1 id="8-参考文献与文章"><a href="#8-参考文献与文章" class="headerlink" title="8. 参考文献与文章"></a>8. 参考文献与文章</h1><p>1.<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1506.02640.pdf">YOLO v1论文</a><br>2.<a target="_blank" rel="noopener" href="https://github.com/AlexeyAB/darknet">YOLO v2论文</a><br>3.<a target="_blank" rel="noopener" href="https://pjreddie.com/media/files/papers/YOLOv3.pdf">YOLO v3论文</a><br>4.<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2004.10934.pdf">YOLO v4论文</a><br>5.<a target="_blank" rel="noopener" href="https://tech.meituan.com/2022/06/23/yolov6-a-fast-and-accurate-target-detection-framework-is-opening-source.html">YOLO v6论文</a><br>6.<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.02696.pdf">YOLO v7论文</a><br>7.<a target="_blank" rel="noopener" href="https://github.com/pjreddie/darknet">YOLO v1234代码</a><br>8.<a target="_blank" rel="noopener" href="https://github.com/ultralytics/yolov5">YOLO v5代码</a><br>9.<a target="_blank" rel="noopener" href="https://github.com/meituan/YOLOv6">YOLO v6代码</a><br>10.<a target="_blank" rel="noopener" href="https://github.com/WongKinYiu/yolov7">YOLO v7代码</a><br>11.<a target="_blank" rel="noopener" href="https://ai-wx.blog.csdn.net/article/details/107509243?spm=1001.2014.3001.5506">YOLO综述</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="FlowerXin">FlowerXin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://flowerxin.github.io/post/undefined.html">http://flowerxin.github.io/post/undefined.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://FlowerXin.github.io" target="_blank">FlowerXin's Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/YOLO/">-YOLO</a></div><div class="post_share"><div class="social-share" data-image="https://img-blog.csdnimg.cn/9f087efcba3946f787555283b4172d64.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/assets/imgs/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/friend_404.gif" data-original="/assets/imgs/wechat.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/assets/imgs/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/friend_404.gif" data-original="/assets/imgs/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-full"><a href="/post/10b7cf25.html"><img class="prev-cover" src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/eb6cbb2bb90749098a6e82946318e093.jpeg" onerror="onerror=null;src='/assets/imgs/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Hexo个人博客搭建教程（一）</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/friend_404.gif" data-original="/assets/imgs/avatar.JPG" onerror="this.onerror=null;this.src='/assets/imgs/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">FlowerXin</div><div class="author-info__description">善始者众，善终者寡</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">4</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">1</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/FlowerXin"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/FlowerXin" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2812828248@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">嘿嘿！你终于来啦！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-YOLO%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">1.</span> <span class="toc-text">1. YOLO目标检测基本概念</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-YOLO-v1"><span class="toc-number">2.</span> <span class="toc-text">2. YOLO v1</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 基本思想</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 模型训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-number">2.3.</span> <span class="toc-text">2.3 训练过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">2.4.</span> <span class="toc-text">2.4 优缺点</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-YOLO-v2"><span class="toc-number">3.</span> <span class="toc-text">3. YOLO v2</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 基本思想</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E6%94%B9%E8%BF%9B%E9%83%A8%E5%88%86"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 改进部分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%EF%BC%88Batch-Normalization%EF%BC%89"><span class="toc-number">3.2.1.</span> <span class="toc-text">3.2.1 批量归一化（Batch Normalization）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2-%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%99%A8%EF%BC%88High-resolution-classifier%EF%BC%89"><span class="toc-number">3.2.2.</span> <span class="toc-text">3.2.2 高分辨率图像分类器（High resolution classifier）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-3-%E5%B8%A6Anchor-Boxes%E7%9A%84%E5%8D%B7%E7%A7%AF%EF%BC%88Convolutional-With-Anchor-Boxes%EF%BC%89"><span class="toc-number">3.2.3.</span> <span class="toc-text">3.2.3 带Anchor Boxes的卷积（Convolutional With Anchor Boxes）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-4-%E7%BB%B4%E5%BA%A6%E8%81%9A%E7%B1%BB%EF%BC%88Dimension-Clusters%EF%BC%89"><span class="toc-number">3.2.4.</span> <span class="toc-text">3.2.4 维度聚类（Dimension Clusters）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-5-%E7%9B%B4%E6%8E%A5%E5%AE%9A%E4%BD%8D%E9%A2%84%E6%B5%8B%EF%BC%88Direct-location-Prediction%EF%BC%89"><span class="toc-number">3.2.5.</span> <span class="toc-text">3.2.5 直接定位预测（Direct location Prediction）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-6-%E7%BB%86%E7%B2%92%E5%BA%A6%E7%89%B9%E5%BE%81%EF%BC%88Fine-Grained-Features%EF%BC%89"><span class="toc-number">3.2.6.</span> <span class="toc-text">3.2.6 细粒度特征（Fine-Grained Features）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-7-%E5%A4%9A%E5%B0%BA%E5%BA%A6%E8%AE%AD%E7%BB%83%EF%BC%88Multi-Scale-Training%EF%BC%89"><span class="toc-number">3.2.7.</span> <span class="toc-text">3.2.7 多尺度训练（Multi-Scale Training）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#YOLO-V2%E5%8F%8D%E6%80%9D"><span class="toc-number">3.3.</span> <span class="toc-text">YOLO V2反思</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-YOLO-v3"><span class="toc-number">4.</span> <span class="toc-text">4.YOLO v3</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5"><span class="toc-number">4.1.</span> <span class="toc-text">4.1 基础概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E6%94%B9%E8%BF%9B%E9%83%A8%E5%88%86"><span class="toc-number">4.2.</span> <span class="toc-text">4.2 改进部分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-1-%E5%A4%9A%E5%B0%BA%E5%BA%A6%E9%A2%84%E6%B5%8B"><span class="toc-number">4.2.1.</span> <span class="toc-text">4.2.1 多尺度预测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-2-%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB"><span class="toc-number">4.2.2.</span> <span class="toc-text">4.2.2 多标签分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-3-%E5%88%9D%E5%A7%8B%E5%B0%BA%E5%AF%B8"><span class="toc-number">4.2.3.</span> <span class="toc-text">4.2.3 初始尺寸</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-YOLO-v3%E5%8F%8D%E6%80%9D%E6%80%BB%E7%BB%93"><span class="toc-number">4.3.</span> <span class="toc-text">4.3 YOLO v3反思总结</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-YOLO-v4"><span class="toc-number">5.</span> <span class="toc-text">4. YOLO v4</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">5.1.</span> <span class="toc-text">4.1 基本概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E7%BD%91%E7%BB%9C%E6%A1%86%E6%9E%B6"><span class="toc-number">5.2.</span> <span class="toc-text">4.2 网络框架</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D"><span class="toc-number">5.3.</span> <span class="toc-text">4.3 框架介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1-CSPDarknet53"><span class="toc-number">5.3.1.</span> <span class="toc-text">4.3.1 CSPDarknet53</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-2-SPP"><span class="toc-number">5.3.2.</span> <span class="toc-text">4.3.2 SPP</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-3-PANet"><span class="toc-number">5.3.3.</span> <span class="toc-text">4.3.3 PANet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-4-YOLOv3-Head"><span class="toc-number">5.3.4.</span> <span class="toc-text">4.3.4 YOLOv3 Head</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-5-%E5%90%84%E7%A7%8DTricks%E6%80%BB%E7%BB%93"><span class="toc-number">5.3.5.</span> <span class="toc-text">4.3.5 各种Tricks总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-6-%E6%94%B9%E8%BF%9B%E6%96%B9%E6%B3%95"><span class="toc-number">5.3.6.</span> <span class="toc-text">4.3.6 改进方法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-YOLO-v5"><span class="toc-number">6.</span> <span class="toc-text">5. YOLO v5</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">6.1.</span> <span class="toc-text">5.1 基本概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-%E5%9F%BA%E6%9C%AC%E6%A1%86%E6%9E%B6"><span class="toc-number">6.2.</span> <span class="toc-text">5.2 基本框架</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-%E6%A1%86%E6%9E%B6%E8%AF%A6%E8%A7%A3"><span class="toc-number">6.3.</span> <span class="toc-text">5.3 框架详解</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-1-%E8%BE%93%E5%85%A5%E7%AB%AF%E8%AF%A6%E8%A7%A3"><span class="toc-number">6.3.1.</span> <span class="toc-text">5.3.1 输入端详解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-1-1-Mosaic%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA"><span class="toc-number">6.3.2.</span> <span class="toc-text">5.3.1.1 Mosaic数据增强</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-1-2-%E8%87%AA%E9%80%82%E5%BA%94%E9%94%9A%E6%A1%86%E8%AE%A1%E7%AE%97"><span class="toc-number">6.4.</span> <span class="toc-text">5.3.1.2 自适应锚框计算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-1-3-%E8%87%AA%E9%80%82%E5%BA%94%E5%9B%BE%E7%89%87%E7%BC%A9%E6%94%BE"><span class="toc-number">6.5.</span> <span class="toc-text">5.3.1.3 自适应图片缩放</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-2-Backone%E7%BD%91%E7%BB%9C"><span class="toc-number">6.5.1.</span> <span class="toc-text">5.3.2 Backone网络</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-3-2-1-Focus%E7%BB%93%E6%9E%84"><span class="toc-number">6.5.1.1.</span> <span class="toc-text">5.3.2.1 Focus结构</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-2-2-CSP%E7%BB%93%E6%9E%84"><span class="toc-number">6.5.2.</span> <span class="toc-text">5.3.2.2 CSP结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-2-3-Neck%E7%BD%91%E7%BB%9C"><span class="toc-number">6.5.3.</span> <span class="toc-text">5.3.2.3 Neck网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-2-4-Head%E7%BD%91%E7%BB%9C"><span class="toc-number">6.5.4.</span> <span class="toc-text">5.3.2.4 Head网络</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-YOLO-v6"><span class="toc-number">7.</span> <span class="toc-text">6. YOLO v6</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">7.1.</span> <span class="toc-text">6.1 基本概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-%E5%9F%BA%E6%9C%AC%E6%A1%86%E6%9E%B6"><span class="toc-number">7.2.</span> <span class="toc-text">6.2 基本框架</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-1-Hardware-friendly-%E7%9A%84%E9%AA%A8%E5%B9%B2%E7%BD%91%E7%BB%9C%E8%AE%BE%E8%AE%A1"><span class="toc-number">7.2.1.</span> <span class="toc-text">6.2.1 Hardware-friendly 的骨干网络设计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-2-%E6%9B%B4%E7%AE%80%E6%B4%81%E9%AB%98%E6%95%88%E7%9A%84-Decoupled-Head"><span class="toc-number">7.2.2.</span> <span class="toc-text">6.2.2 更简洁高效的 Decoupled Head</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-3-%E6%9B%B4%E6%9C%89%E6%95%88%E7%9A%84%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5"><span class="toc-number">7.2.3.</span> <span class="toc-text">6.2.3 更有效的训练策略</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-YOLO-v7"><span class="toc-number">8.</span> <span class="toc-text">7. YOLO v7</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">8.1.</span> <span class="toc-text">7.1 基本概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-2-%E6%A1%86%E6%9E%B6"><span class="toc-number">8.2.</span> <span class="toc-text">7.2 框架</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-1-%E6%89%A9%E5%B1%95%E7%9A%84%E9%AB%98%E6%95%88%E5%B1%82%E8%81%9A%E5%90%88%E7%BD%91%E7%BB%9C"><span class="toc-number">8.2.1.</span> <span class="toc-text">7.2.1 扩展的高效层聚合网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-2-%E5%9F%BA%E4%BA%8Econcatenate%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A8%A1%E5%9E%8B%E7%BC%A9%E6%94%BE"><span class="toc-number">8.2.2.</span> <span class="toc-text">7.2.2 基于concatenate模型的模型缩放</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-3-%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95"><span class="toc-number">8.3.</span> <span class="toc-text">7.3 训练方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-1-Planned-re-parameterized-convolution"><span class="toc-number">8.3.1.</span> <span class="toc-text">7.3.1 Planned re-parameterized convolution</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-2-%E6%A0%87%E7%AD%BE%E5%8C%B9%E9%85%8D"><span class="toc-number">8.3.2.</span> <span class="toc-text">7.3.2 标签匹配</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-3-%E5%85%B6%E4%BB%96Tricks"><span class="toc-number">8.3.3.</span> <span class="toc-text">7.3.3 其他Tricks</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86%E7%82%B9"><span class="toc-number">8.4.</span> <span class="toc-text">知识点</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE%E4%B8%8E%E6%96%87%E7%AB%A0"><span class="toc-number">9.</span> <span class="toc-text">8. 参考文献与文章</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/e395c9a7.html" title="labelme使用与安装教程"><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/65d29375a94d4081a4907455e0513fc0.webp" onerror="this.onerror=null;this.src='/assets/imgs/404.jpg'" alt="labelme使用与安装教程"/></a><div class="content"><a class="title" href="/post/e395c9a7.html" title="labelme使用与安装教程">labelme使用与安装教程</a><time datetime="2023-02-19T09:16:42.000Z" title="发表于 2023-02-19 17:16:42">2023-02-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/dfa6abd3.html" title="Tensorflow、pytorch、paddlepaddle总结对比分析"><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/f9af797fa90b4114a6bb35d13a5a9190.webp" onerror="this.onerror=null;this.src='/assets/imgs/404.jpg'" alt="Tensorflow、pytorch、paddlepaddle总结对比分析"/></a><div class="content"><a class="title" href="/post/dfa6abd3.html" title="Tensorflow、pytorch、paddlepaddle总结对比分析">Tensorflow、pytorch、paddlepaddle总结对比分析</a><time datetime="2022-09-21T04:42:11.000Z" title="发表于 2022-09-21 12:42:11">2022-09-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/4b6a0bb7.html" title="Hexo个人博客魔改教程（三）"><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/c9d8925d508a472aa5e11f17c5a2ade4.jpeg" onerror="this.onerror=null;this.src='/assets/imgs/404.jpg'" alt="Hexo个人博客魔改教程（三）"/></a><div class="content"><a class="title" href="/post/4b6a0bb7.html" title="Hexo个人博客魔改教程（三）">Hexo个人博客魔改教程（三）</a><time datetime="2022-08-01T09:22:11.000Z" title="发表于 2022-08-01 17:22:11">2022-08-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/a271c6f8.html" title="Hexo个人博客美化教程（二）"><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/5aded8d83fcd46d68560b7be03df9090.jpeg" onerror="this.onerror=null;this.src='/assets/imgs/404.jpg'" alt="Hexo个人博客美化教程（二）"/></a><div class="content"><a class="title" href="/post/a271c6f8.html" title="Hexo个人博客美化教程（二）">Hexo个人博客美化教程（二）</a><time datetime="2022-08-01T07:43:47.000Z" title="发表于 2022-08-01 15:43:47">2022-08-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/10b7cf25.html" title="Hexo个人博客搭建教程（一）"><img src="/img/friend_404.gif" data-original="https://img-blog.csdnimg.cn/eb6cbb2bb90749098a6e82946318e093.jpeg" onerror="this.onerror=null;this.src='/assets/imgs/404.jpg'" alt="Hexo个人博客搭建教程（一）"/></a><div class="content"><a class="title" href="/post/10b7cf25.html" title="Hexo个人博客搭建教程（一）">Hexo个人博客搭建教程（一）</a><time datetime="2022-07-28T08:06:42.000Z" title="发表于 2022-07-28 16:06:42">2022-07-28</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://img-blog.csdnimg.cn/9f087efcba3946f787555283b4172d64.jpeg')"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By FlowerXin</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: '0a3fa652e04f21c1bda3',
      clientSecret: '83a2deac9cd8f631a1a6e2efe407c07370c9c8dd',
      repo: 'Firsttext',
      owner: 'FlowerXin',
      admin: ['FlowerXin'],
      id: 'c8391255d1a2f5dcbc1df64a7d71bc41',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    getScript('https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !false) {
  if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="true" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-show-text.min.js" data-mobile="true" data-text="I,LOVE,YOU" data-fontsize="15px" data-random="true" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --><script data-pjax src="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.js"></script><script data-pjax>
  function gitcalendar_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<div class="container"><style>#git_container{min-height: 280px}@media screen and (max-width:650px) {#git_container{min-height: 0px}}</style><div id="git_loading" style="width:10%;height:100%;margin:0 auto;display: block;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animatetransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animatetransform></path></svg><style>#git_container{display: none;}</style></div><div id="git_container"></div></div>';
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      console.log('已挂载gitcalendar')
      }

    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
        gitcalendar_injector_config()
        GitCalendarInit("https://fxcapi.vercel.app/api?FlowerXin",['#e4dfd7', '#f9f4dc', '#f7e8aa', '#f7e8aa', '#f8df72', '#fcd217', '#fcc515', '#f28e16', '#fb8b05', '#d85916', '#f43e06'],'FlowerXin')
    }
  </script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/koharu.model.json"},"log":false});</script><script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(n){n.imageLazyLoadSetting.processImages=o;var e=n.imageLazyLoadSetting.isSPA,i=n.imageLazyLoadSetting.preloadRatio||1,r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]"));function o(){e&&(r=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")));for(var t,a=0;a<r.length;a++)0<=(t=(t=r[a]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(n.innerHeight*i||document.documentElement.clientHeight*i)&&function(){var t,e,n,i,o=r[a];t=o,e=function(){r=r.filter(function(t){return o!==t})},n=new Image,i=t.getAttribute("data-original"),n.onload=function(){t.src=i,e&&e()},t.src!==i&&(n.src=i)}()}o(),n.addEventListener("scroll",function(){var t,e;t=o,e=n,clearTimeout(t.tId),t.tId=setTimeout(function(){t.call(e)},500)})}(this);</script></body></html>