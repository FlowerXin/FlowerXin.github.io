<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>labelme使用与安装教程</title>
      <link href="/post/e395c9a7.html"/>
      <url>/post/e395c9a7.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>本文将详细介绍labelme的环境搭建、安装、配置、使用以及过程中会遇到的一些bug。<br><img src="https://img-blog.csdnimg.cn/8dca704ea81b4917843bc098b12fea8c.png" alt="3.8"></p><h1 id="labelme环境搭建"><a href="#labelme环境搭建" class="headerlink" title="labelme环境搭建"></a>labelme环境搭建</h1><p>labelme 是一款图像标注工具，主要用于神经网络构建前的数据集准备工作，因为是用 Python 写的，所以使用前需要先安装 Python 集成环境 anaconda。anaconda下载地址如下：<a href="https://www.anaconda.com/products/distribution">anaconda下载链接</a><br>在安装成功后从开始处进入AnacondaPrompt，输入如下命令，该命令表示创建虚拟环境labelme</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda create -n labelme python=3.8</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/img_convert/36c7c175bb010df8f28e48b3b1300e0c.png" alt="3.8"><br>输入如上命令，会运行几秒钟，正式开始创建前，会出现（[y]&#x2F;n）?字样，表示是·否同意创建的意思，输入 y，按 enter，等待运行结束。<br><font color=Red>[<em>注意</em>]:python的版本要用3.7或者3.8的，我之前用的3.10，会报错</font></p><p>输入查看当前已安装的虚拟环境:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda env list</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/img_convert/8abe4dc37326b90bebcbe45dce54f11d.png" alt="env lis"><br>创建好虚拟环境后，需要激活，用如下命令:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda activate labelme</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/img_convert/0fd3b7e30715348a020cf3a5740bcb36.png" alt="activate"></p><p>labelme 正常运转需要各种依赖的包，下面的 pypt 和 pillow 就是，它们用如下命令安装:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda install pyqt</span><br><span class="line">conda install pillow</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/img_convert/4cf9b39b274375adbfd2e8a679291925.png" alt="pillow包"><br>安装好 labelme 依赖的包之后，正式开始安装 labelme，用如下命令，先用 conda 命令，如果安装不成功，则用 pip 命令:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda install labelme=3.16.2</span><br><span class="line">#conda 安装命令如果出错也可以使用 pip 命令，使用逻辑等号&quot;==&quot;</span><br><span class="line">pip install labelme==3.16.2</span><br><span class="line">#也可以直接</span><br><span class="line">conda install labelme</span><br><span class="line"># 或者</span><br><span class="line">pip install labelme</span><br></pre></td></tr></table></figure><p>中间有可能会再次出现（[y]&#x2F;n）?，也有可能不出现，玄学，如果出现，则和之前的操作一样，输入y，按下 enter，等待安装结束。如果不出现，运行一段时间后，如果看到有 successfully installed labelme 等字样，则表示安装成功<br>这一步一定要注意安装的版本号，如果直接安装 labelme 不标注版本号在后续 json 到 dataset 的时候会出现异常，一般来说3.16的版本都可以</p><h1 id="labelme的使用"><a href="#labelme的使用" class="headerlink" title="labelme的使用"></a>labelme的使用</h1><p>以后每次使用 labelme 时，都需要桌面搜索进入 anaconda prompt，用如下命令激活 labelme 环境：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">activate labelme</span><br></pre></td></tr></table></figure><p>用如下命令打开 labelme</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">labelme</span><br></pre></td></tr></table></figure><p>输入如上命令后，会弹出 labelme 操作界面，如下：<br><img src="https://img-blog.csdnimg.cn/img_convert/5eef453eea61593c73172abc85a62453.png" alt="labelmejiemian">  </p><h1 id="图片打标实例"><a href="#图片打标实例" class="headerlink" title="图片打标实例"></a>图片打标实例</h1><p>点击 Open Dir，选择待标注图片所在文件夹，批量导入.根据需求，选择圆、矩形、多边形（默认）等开始标注，一般为多边形.一个区域标注完成后，会自动弹出对话框，键入标签名称.<br><img src="https://img-blog.csdnimg.cn/img_convert/0744725b368b7d28ee3d65dde1a803d5.png" alt="sanjiaojia"><br>所有区域标注完成后，点击左侧栏 Save，会自动保存对应的 json 数据。</p><h1 id="json转换格式"><a href="#json转换格式" class="headerlink" title="json转换格式"></a>json转换格式</h1><p>生成的 json 文件批量转成我们需要的数据格式。</p><h3 id="1-找到-json-to-dataset-py-文件，打开，替换为如下代码"><a href="#1-找到-json-to-dataset-py-文件，打开，替换为如下代码" class="headerlink" title="1.找到 json_to_dataset.py 文件，打开，替换为如下代码"></a>1.找到 json_to_dataset.py 文件，打开，替换为如下代码</h3><p>我的文件路径在：<br>D:\lijianxin\ruanjian\Anaconda3\envs\labelme\Lib\site-packages\labelme中</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import argparse</span><br><span class="line">import json</span><br><span class="line">import os</span><br><span class="line">import os.path as osp</span><br><span class="line">import base64</span><br><span class="line">import warnings</span><br><span class="line"> </span><br><span class="line">import PIL.Image</span><br><span class="line">import yaml</span><br><span class="line"> </span><br><span class="line">from labelme import utils</span><br><span class="line"> </span><br><span class="line">import cv2</span><br><span class="line">import numpy as np</span><br><span class="line">from skimage import img_as_ubyte</span><br><span class="line"> </span><br><span class="line"># from sys import argv</span><br><span class="line"> </span><br><span class="line">def main():</span><br><span class="line">    warnings.warn(&quot;This script is aimed to demonstrate how to convert the\n&quot;</span><br><span class="line">                  &quot;JSON file to a single image dataset, and not to handle\n&quot;</span><br><span class="line">                  &quot;multiple JSON files to generate a real-use dataset.&quot;)</span><br><span class="line"> </span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(&#x27;json_file&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;-o&#x27;, &#x27;--out&#x27;, default=None)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"> </span><br><span class="line">    json_file = args.json_file</span><br><span class="line"> </span><br><span class="line">    #freedom</span><br><span class="line">    list_path = os.listdir(json_file)</span><br><span class="line">    print(&#x27;freedom =&#x27;, json_file)</span><br><span class="line">    for i in range(0,len(list_path)):</span><br><span class="line">        path = os.path.join(json_file,list_path[i])</span><br><span class="line">        if os.path.isfile(path):</span><br><span class="line"> </span><br><span class="line">            data = json.load(open(path))</span><br><span class="line">            img = utils.img_b64_to_arr(data[&#x27;imageData&#x27;])</span><br><span class="line">            lbl, lbl_names = utils.labelme_shapes_to_label(img.shape, data[&#x27;shapes&#x27;])</span><br><span class="line"> </span><br><span class="line">            captions = [&#x27;%d: %s&#x27; % (l, name) for l, name in enumerate(lbl_names)]</span><br><span class="line"> </span><br><span class="line">            lbl_viz = utils.draw_label(lbl, img, captions)</span><br><span class="line">            out_dir = osp.basename(path).replace(&#x27;.&#x27;, &#x27;_&#x27;)</span><br><span class="line">            save_file_name = out_dir</span><br><span class="line">            out_dir = osp.join(osp.dirname(path), out_dir)</span><br><span class="line"> </span><br><span class="line">            if not osp.exists(json_file + &#x27;\\&#x27; + &#x27;labelme_json&#x27;):</span><br><span class="line">                os.mkdir(json_file + &#x27;\\&#x27; + &#x27;labelme_json&#x27;)</span><br><span class="line">            labelme_json = json_file + &#x27;\\&#x27; + &#x27;labelme_json&#x27;</span><br><span class="line"> </span><br><span class="line">            out_dir1 = labelme_json + &#x27;\\&#x27; + save_file_name</span><br><span class="line">            if not osp.exists(out_dir1):</span><br><span class="line">                os.mkdir(out_dir1)</span><br><span class="line"> </span><br><span class="line">            PIL.Image.fromarray(img).save(out_dir1+&#x27;\\&#x27;+save_file_name+&#x27;_img.png&#x27;)</span><br><span class="line">            PIL.Image.fromarray(lbl).save(out_dir1+&#x27;\\&#x27;+save_file_name+&#x27;_label.png&#x27;)</span><br><span class="line">                </span><br><span class="line">            PIL.Image.fromarray(lbl_viz).save(out_dir1+&#x27;\\&#x27;+save_file_name+</span><br><span class="line">            &#x27;_label_viz.png&#x27;)</span><br><span class="line"> </span><br><span class="line">            if not osp.exists(json_file + &#x27;\\&#x27; + &#x27;mask_png&#x27;):</span><br><span class="line">                os.mkdir(json_file + &#x27;\\&#x27; + &#x27;mask_png&#x27;)</span><br><span class="line">            mask_save2png_path = json_file + &#x27;\\&#x27; + &#x27;mask_png&#x27;</span><br><span class="line">            ################################</span><br><span class="line">            #mask_pic = cv2.imread(out_dir1+&#x27;\\&#x27;+save_file_name+&#x27;_label.png&#x27;,)</span><br><span class="line">            #print(&#x27;pic1_deep:&#x27;,mask_pic.dtype)</span><br><span class="line"> </span><br><span class="line">            mask_dst = img_as_ubyte(lbl)  #mask_pic</span><br><span class="line">            print(&#x27;pic2_deep:&#x27;,mask_dst.dtype)</span><br><span class="line">            cv2.imwrite(mask_save2png_path+&#x27;\\&#x27;+save_file_name+&#x27;_label.png&#x27;,mask_dst)</span><br><span class="line">            ##################################</span><br><span class="line"> </span><br><span class="line">            with open(osp.join(out_dir1, &#x27;label_names.txt&#x27;), &#x27;w&#x27;) as f:</span><br><span class="line">                for lbl_name in lbl_names:</span><br><span class="line">                    f.write(lbl_name + &#x27;\n&#x27;)</span><br><span class="line"> </span><br><span class="line">            warnings.warn(&#x27;info.yaml is being replaced by label_names.txt&#x27;)</span><br><span class="line">            info = dict(label_names=lbl_names)</span><br><span class="line">            with open(osp.join(out_dir1, &#x27;info.yaml&#x27;), &#x27;w&#x27;) as f:</span><br><span class="line">                yaml.safe_dump(info, f, default_flow_style=False)</span><br><span class="line"> </span><br><span class="line">            print(&#x27;Saved to: %s&#x27; % out_dir1)</span><br><span class="line"> </span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h3 id="2-替换好之后，找到-labelme-json-to-dataset-exe-这个文件，主要是复制它的路径"><a href="#2-替换好之后，找到-labelme-json-to-dataset-exe-这个文件，主要是复制它的路径" class="headerlink" title="2.替换好之后，找到 labelme_json_to_dataset.exe 这个文件，主要是复制它的路径"></a>2.替换好之后，找到 labelme_json_to_dataset.exe 这个文件，主要是复制它的路径</h3><p>我的文件里路径在：D:\lijianxin\ruanjian\Anaconda3\envs\labelme\Scripts</p><p><font color=Red>[<em>注意</em>]：python中也有一个exe的文件，不能使用那个路径，不然后面会报错</font></p><h3 id="3-桌面搜索-anaconda，再次进入Anaconda-Prompt，激活-labelme-环境，用如下命令"><a href="#3-桌面搜索-anaconda，再次进入Anaconda-Prompt，激活-labelme-环境，用如下命令" class="headerlink" title="3.桌面搜索 anaconda，再次进入Anaconda Prompt，激活 labelme 环境，用如下命令"></a>3.桌面搜索 anaconda，再次进入Anaconda Prompt，激活 labelme 环境，用如下命令</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">activate labelme </span><br></pre></td></tr></table></figure><h3 id="4-进入-labelme-json-to-dataset-exe-文件所在路径，也就是第2步你复制的路径，进入命令如下"><a href="#4-进入-labelme-json-to-dataset-exe-文件所在路径，也就是第2步你复制的路径，进入命令如下" class="headerlink" title="4.进入 labelme_json_to_dataset.exe 文件所在路径，也就是第2步你复制的路径，进入命令如下"></a>4.进入 labelme_json_to_dataset.exe 文件所在路径，也就是第2步你复制的路径，进入命令如下</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd D:\Anaconda3\envs\labelme\Scripts</span><br></pre></td></tr></table></figure><h3 id="5-输入-labelme-json-to-dataset-exe-空格-【你待转化的-json-文件所在路径】"><a href="#5-输入-labelme-json-to-dataset-exe-空格-【你待转化的-json-文件所在路径】" class="headerlink" title="5.输入 labelme_json_to_dataset.exe+空格+【你待转化的 json 文件所在路径】"></a>5.输入 labelme_json_to_dataset.exe+空格+【你待转化的 json 文件所在路径】</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">labelme_json_to_dataset.exe 【你待转化的 json 文件所在路径】</span><br></pre></td></tr></table></figure><p>这里我的输入是：labelme_json_to_dataset.exe D:\yanjiushengcheliangxiangmuhuanjing\js</p><p><font color=Red>[<em>注意</em>]：其中js是我要转换的文件夹，里面只能有json文件，不然会报错</font></p><h3 id="6-等待运行，运行一段时间后，如果末尾出现以下两个save，表示转换成功"><a href="#6-等待运行，运行一段时间后，如果末尾出现以下两个save，表示转换成功" class="headerlink" title="6.等待运行，运行一段时间后，如果末尾出现以下两个save，表示转换成功"></a>6.等待运行，运行一段时间后，如果末尾出现以下两个save，表示转换成功</h3><p><img src="https://img-blog.csdnimg.cn/img_convert/09bfe607d41000810fcf6f92c905b899.png" alt="save"></p><h3 id="7-检查转换结果-如下显示，表示转换成功"><a href="#7-检查转换结果-如下显示，表示转换成功" class="headerlink" title="7.检查转换结果,如下显示，表示转换成功"></a>7.检查转换结果,如下显示，表示转换成功</h3><p><img src="https://img-blog.csdnimg.cn/img_convert/ee04d7060a1822e85f1d064afff28a9d.png" alt="zhuanhuan"></p><h1 id="遇到的bug总结："><a href="#遇到的bug总结：" class="headerlink" title="遇到的bug总结："></a>遇到的bug总结：</h1><h2 id="1-labelme安装及运行时崩溃"><a href="#1-labelme安装及运行时崩溃" class="headerlink" title="1.labelme安装及运行时崩溃"></a>1.labelme安装及运行时崩溃</h2><p>根源在于在第一句命令时将python的版本号记成3.10，此时将会报错如图所示：<br><img src="https://img-blog.csdnimg.cn/img_convert/71fd39d175d9a2c0ebfa704f5ef35f31.png" alt="zhuanhuan"></p><h3 id="1-1-思考原因"><a href="#1-1-思考原因" class="headerlink" title="1.1 思考原因"></a>1.1 思考原因</h3><p>这些bug出现的原因是由于labelme的版本问题导致的，labelme代码中的会用到QPoint(float,float) or QSize(float,float) 这些函数，但QT并不支持float类型的参数，所以我们需要对labelme包中这些函数的参数进行强制转化，以适配QT。</p><h3 id="1-2-1-解决办法1-舍"><a href="#1-2-1-解决办法1-舍" class="headerlink" title="1.2.1 解决办法1(舍)"></a>1.2.1 解决办法1(舍)</h3><p>我们根据报错的信息来修改一下QPoint()函数的传入参数类型，根据报错信息，打开函数所在文件，找到QPoint()函数所在的第531行，将输入参数强制转换为int。<br><img src="https://img-blog.csdnimg.cn/img_convert/1f6e7677006027352e81927da4834b2b.png" alt="zhuanhuan"></p><h3 id="1-2-2-解决办法2（用）"><a href="#1-2-2-解决办法2（用）" class="headerlink" title="1.2.2 解决办法2（用）"></a>1.2.2 解决办法2（用）</h3><p>但是后来打开软件会出现闪退的情况，每次的报错行数都不一样，改来改去很麻烦。我猜可能是版本不兼容的缘故，尝试用python3.7，3.8重新安装了一下，之后运行就很正常，不会出现闪退情况。</p><h2 id="2-cmd报错：PermissionError-Errno-13-Permission-denied"><a href="#2-cmd报错：PermissionError-Errno-13-Permission-denied" class="headerlink" title="2.cmd报错：PermissionError: [Errno 13] Permission denied"></a>2.cmd报错：PermissionError: [Errno 13] Permission denied</h2><p>报错如图：<br><img src="https://img-blog.csdnimg.cn/img_convert/bac3643264a9606176bef8b99afd8631.png" alt="zhuanhuan"></p><h3 id="2-1思考原因"><a href="#2-1思考原因" class="headerlink" title="2.1思考原因"></a>2.1思考原因</h3><p>首先我要解释一下，很多时候出现这个问题，并不是你的文件有毛病，很可能是你代码读取的路径不对，这就要追究到代码本身。批量转化可以在json_to_dataset文件里直接运行代码实现。<br>报错翻译过来是：权限错误：[errno 13]权限被拒绝：<br>错误产生的原因是文件无法打开，可能产生的原因是文件找不到，或者被占用，或者无权限访问，或者打开的不是文件，而是一个目录。<br>解决方案</p><h3 id="2-2-1解决方案（都要满足，缺一就会报错）："><a href="#2-2-1解决方案（都要满足，缺一就会报错）：" class="headerlink" title="2.2.1解决方案（都要满足，缺一就会报错）："></a>2.2.1解决方案（都要满足，缺一就会报错）：</h3><p>1.检查对应路径下的文件是否存在，且被占用。如果文件不存在，就找到对应文件即可；如果文件存在，被占用，将占用程序暂时关闭。<br>2.修改cmd的权限，以管理员身份运行。<br>3.检查是否是打开了文件夹。<br>4.路径检查是否正确，labelme_json_to_dataset.exe 这个文件我之前用的是python路径下的，之后改成annoca下的之后，在cmd中cd到label_json_to_dataset.py路径下，然后输入，就好了。<br>5.js是我要转换的文件夹，里面只能有json文件，不然会报错  </p><p>我的python错误路径如图：<br><img src="https://img-blog.csdnimg.cn/img_convert/79430cab751530703c1153aca41ab4f1.png" alt="zhuanhuan"><br>正确的路径如图所示：<br><img src="https://img-blog.csdnimg.cn/img_convert/963163ad4712c4c1b0407b120e3626aa.png" alt="zhuanhuan"></p><h2 id="3-ModuleNotFoundError-No-module-named-‘cv2’"><a href="#3-ModuleNotFoundError-No-module-named-‘cv2’" class="headerlink" title="3.ModuleNotFoundError: No module named ‘cv2’"></a>3.ModuleNotFoundError: No module named ‘cv2’</h2><p>环境中缺少cv2的包所以会出现No module named ‘cv2’的问题。然后cv2的包名并不叫cv2，所以使用pip install cv2不能安装。<br>解决方法：安装cv2<br>pip install opencv-python   （如果只用主模块，使用这个命令安装）<br>pip install opencv-contrib-python （如果需要用主模块和contrib模块，使用这个命令安装）<br>我用的第一个。</p><h2 id="4-解决ModuleNotFoundError-No-module-named-‘skimage‘"><a href="#4-解决ModuleNotFoundError-No-module-named-‘skimage‘" class="headerlink" title="4.解决ModuleNotFoundError: No module named ‘skimage‘"></a>4.解决ModuleNotFoundError: No module named ‘skimage‘</h2><p><img src="https://img-blog.csdnimg.cn/img_convert/5383d58731213e73ef071551937911aa.png" alt="zhuanhuan"><br>解决命令cmd:pip install scikit-image</p><h2 id="5-UnicodeDecodeError-‘gbk‘-codec-can‘t-decode-byte-0xff-in-position-0"><a href="#5-UnicodeDecodeError-‘gbk‘-codec-can‘t-decode-byte-0xff-in-position-0" class="headerlink" title="5.UnicodeDecodeError: ‘gbk‘ codec can‘t decode byte 0xff in position 0"></a>5.UnicodeDecodeError: ‘gbk‘ codec can‘t decode byte 0xff in position 0</h2><p>起初以为是json_to_dataset文件中的编码问题，但是当我重新新建一个文件夹，里面只有json文件时，问题就解决了。<br>原因：文件夹中存在非json文件</p><h2 id="6-module-‘labelme-utils’-has-no-attribute-‘draw-label’"><a href="#6-module-‘labelme-utils’-has-no-attribute-‘draw-label’" class="headerlink" title="6.module ‘labelme.utils’ has no attribute ‘draw_label’"></a>6.module ‘labelme.utils’ has no attribute ‘draw_label’</h2><p>解决方法：重新安装labelme</p><h2 id="7-Polygon-must-have-points-more-than-2"><a href="#7-Polygon-must-have-points-more-than-2" class="headerlink" title="7.Polygon must have points more than 2"></a>7.Polygon must have points more than 2</h2><p>原因：存在标注的点数太少了的区域，需要大于2。</p><h2 id="8-ModuleNotFoundError-No-module-named-‘skimage’"><a href="#8-ModuleNotFoundError-No-module-named-‘skimage’" class="headerlink" title="8.ModuleNotFoundError: No module named ‘skimage’"></a>8.ModuleNotFoundError: No module named ‘skimage’</h2><p>解决命令：conda install scikit-image</p>]]></content>
      
      
      
        <tags>
            
            <tag> -labelme </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tensorflow、pytorch、paddlepaddle总结对比分析</title>
      <link href="/post/dfa6abd3.html"/>
      <url>/post/dfa6abd3.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>本系列将从发展历史、现状、框架功能、社区生态构建、优势分析比较等方面详细介绍Tensorflow、pytorch、paddlepaddle深度学习框架。后续将更新单独的笔记，感兴趣的话就读下去吧~（不读，你嘎）<br><img src="https://img-blog.csdnimg.cn/70f0382835bd4b6cabdfc41fe7d3b28a.png" alt="avatar"></p><h1 id="1-发展历史与发展现状"><a href="#1-发展历史与发展现状" class="headerlink" title="1. 发展历史与发展现状"></a>1. 发展历史与发展现状</h1><h2 id="1-1-发展历程"><a href="#1-1-发展历程" class="headerlink" title="1.1 发展历程"></a>1.1 发展历程</h2><p>Tensor Flow的前身是2011年Google Brain内部孵化项目Dist Belief这是一个为深度神经网络构建的机器学习系统。经过Google内部的锤炼后，在2015年11月9日，以Apache License 2.0 的开源协议对外发布了TensorFlow，并于2017年2月发布了1.0.0 版本，这标志着TensorFlow稳定版的诞生。2018年9月TensorFlow 1.2版本发布，将Keras融入Tensor Flow，作为TensorFlow的高级 API，这也标志着TensorFlow在面向数百万新用户开源的道路上迈出重要的一步。2019年9月正式发布TensorFlow2.0版本，紧接着在11月，公布了TensorFlow2.1的RC版本，兼容之前的流行库，并还引入了众多新库，使得TensorFlow的功能空前强大。  </p><p>相比较而言，PyTorch则比较年轻。2017年1月，由Face-book人工智能研究院（FAIR）基于Torch推出了PyTorch，并于2018年5月正式公布PyTorch1.0版本，这个新的框架将PyTorch0.4与贾扬清的Caffe2合并，并整合ONNX格式，让开发者可以无缝地将AI模型从研究转到生产，而无需处理迁移。最新版PyTorch 1.3 于 2019 年 10 月上线，更新增加了很多的新特性，包括无缝移动设备部署、量化模型以加速推断、前端改进（如对张量进行命名和创建更干净的代码）等，PyTorch官方同时还开源了很多新工具和库，使得PyTorch的众多功能向TensorFlow趋同，同时保有自身原有特性，竞争力得到极大增强。  </p><p>自去年9月开源之后，PaddlePaddle已经成为了现在增长最快的深度学习框架之一。PaddlePaddle是一个易于使用、高效、灵活和可扩展的深度学习平台，该平台最初是由百度在 2014 年为其产品应用深度学习而开发的。现在已经使用PaddlePaddle的创新已经超过了50项，并支持包括搜索引擎、在线广告、Q＆A 和系统安全等15项百度产品。2016年9月，百度开源了PaddlePaddle，并且很快就吸引了很多来自百度外的参与者。<br><img src="https://img-blog.csdnimg.cn/cdff103eff9649899afad14523b9cd73.png" alt="avatar"><br>总而言之，自2015年后，以TensorFlow、PyTorch为代表的国外深度学习框架发展迅猛，占据了业界主导地位。中国首个自主研发的产业级深度学习平台飞桨，2016 年正式开源，目前在中国综合市场份额已超越PyTorch和TensorFlow，位居第一。《中国深度学习软件框架市场研究报告（2021）》认为，飞桨具备“世界领先的人工智能技术、支撑科研与产业共进的核心框架、拥有产业级开源模型库的开发平台、中国第一的开发者生态”等核心优势。根据基于应用、产品和生态能力构建的深度学习软件框架厂商竞争力评价模型，百度的飞桨PaddlePaddle综合竞争力领跑行业，其次是Meta的PYTorch和谷歌的Tensorflow，目前三者在细分领域项目上各有千秋。</p><h2 id="1-2-现状分析"><a href="#1-2-现状分析" class="headerlink" title="1.2 现状分析"></a>1.2 现状分析</h2><p>从GitHub讨论热度、各大顶级会议的选择而言，TensorFlow和PyTorch无疑是当前受众最广、热度最高的两种深度学习框架。  </p><p>康奈尔大学的 Horace He以及数据科学家、Rebel Desk的 COO、Medium 优秀作者Jeff Hale都对TensorFlow和PyTorch在研究领域、工业领域的现状进行了分析。研究领域的统计结果主要基于五大顶级会议论文的使用率来比较趋势，工业领域则是通过线上招聘启事中的提及率来比较趋势。研究领域的关键统计结果展示如表2所示。从表中我们可以发现，在研究领域，PyTorch的使用率在近两年飞速提升。69%的CVPR论文、75%以上的NAACL和ACL论文，以及50%以上的ICLR和ICML论文都选择使用PyTorch，可谓迅速获得研究人员的青睐，而TensorFlow则没有如此耀眼的数据。</p><p><img src="https://img-blog.csdnimg.cn/cce8f4a55feb4368a6da5b3b8e1cd787.png" alt="avatar">    </p><p>自2014年开始，全球人工智能学术界及产业各界研主体陆续开源旗下自主研发的深度学习软件框架，并以框架为主体搭建人工智能开源开放平台，推动人工智能产业生态的建立。由谷歌团队开发的TensorFlow及由Meta团队开发的PyTorch两款深度学习软件框架占据业界主体地位。</p><p>同时，中国正在快速形成开源框架的系统化布局，百度飞桨自主研发的开源深度学习软件框架加速升级。在2017年7月百度建立了国内首个飞桨人工智能产业赋值中心。该中心将充分发挥辐射带动作用，在产业赋能、人才培养和生态共建等方面展开工作。飞桨已汇聚406万开发者，创建47.6万个AI模型，累计服务15.7万家企事业单位，覆盖工业、农业、医疗、城市管理、交通、金融等众多领域。飞桨已经能够帮助传统企业在智能化升级中实现高性能开发、大规模训练、不同场景和不同软硬件平台敏捷部署。更重要的是，飞桨已经和包括百度昆仑芯、英特尔、英伟达在内的22家国内外硬件厂商，完成了31种芯片的适配和优化，覆盖全部国内外主流芯片，最大程度帮助企业降本增效。  </p><p>《深度学习平台发展报告（2022 年）》报告认为，在全球范围来看TensorFlow和PyTorch仍是深度学习框架的主流 “ 玩家 “，但与此同时，也认可了百度飞桨自2016年以来的成绩。例如在GitHub和Papers with Code的趋势榜单中，飞桨的 PaddleClas、PaddleDetection、PaddleGAN 等多个套件便登顶榜单。<br><img src="https://img-blog.csdnimg.cn/d581ec3ef2cb4f149b5de052737b8aaf.png" alt="avatar">  </p><h1 id="2-Tensorflow、pytorch、paddlepaddle框架功能对比"><a href="#2-Tensorflow、pytorch、paddlepaddle框架功能对比" class="headerlink" title="2.Tensorflow、pytorch、paddlepaddle框架功能对比"></a>2.Tensorflow、pytorch、paddlepaddle框架功能对比</h1><p>一个良好的深度学习框架应该具备优化的性能、易于理解的框架与编码、良好的社区支持、并行化的进程以及自动计算梯度等特征，TensorFlow、PyTorch、paddlepaddle在这些方面都有良好的表现，为了更为细致地比较三者之间的差异优势，下面将对最新版TensorFlow、PyTorch、paddlepaddle先从运行机制、训练模式、可视化情况、模型部署、调试等方面进行差异比较，然后再对数据加载、设备管理等方面进行定性比较，最后归类对应适用场景的建议。  </p><h2 id="2-1Tensorflow、pytorch、paddlepaddle应用领域"><a href="#2-1Tensorflow、pytorch、paddlepaddle应用领域" class="headerlink" title="2.1Tensorflow、pytorch、paddlepaddle应用领域"></a>2.1Tensorflow、pytorch、paddlepaddle应用领域</h2><table><thead><tr><th>序号</th><th>框架名称</th><th>开发支持</th><th>支持语言</th><th>开源&#x2F;发布时间</th></tr></thead><tbody><tr><td>1</td><td>Tensorflow</td><td>Google</td><td>Python&#x2F;C++&#x2F;Java&#x2F;Go&#x2F; R&#x2F;Swift&#x2F;C#&#x2F;JavaScript</td><td>2015-11</td></tr><tr><td>2</td><td>Pytorch</td><td>Facebook</td><td>Python&#x2F;C++</td><td>2017-01</td></tr><tr><td>3</td><td>Paddlepaddle</td><td>百度</td><td>Python&#x2F;C++</td><td>2016-09</td></tr></tbody></table><p>由图可知，在工业应用领域，TensorFlow依然保有优势，尤其在TensorFlow1.2版本融合Keras作为高级独立API之后，结合Keras的统计数据，TensorFlow在工业领域的优势则更加明显.TensorFlow在工业领域的领先优势得益于其诞生的时间较早，工业界较早引入TensorFlow框架，并已形成行业惯性，且工业界在对新事物的热情程度远不如研究界等因素。综合而言，近两年PyTorch发展势头强劲已是不争事实，尤其在研究领域迅速虏获一批研究人员的喜爱，而在工业领域则相对较弱。TensorFlow在研究领域、工业领域，依旧保持相对优势，只是发展势头相对放缓，业务面临被PyTorch、Keras、PaddlePaddle等分流的压力，但就当前现状而言，并随着TensorFlow自身的逐步完善，其占据首位的实力依旧不易撼动。  </p><p>除了Tensorflow和pytorch主流框架之外，飞桨平台的发展也不容小觑。百度推出中国首个开源框架飞桨paddlepaddle兼具效率与灵活性，有赶超的势头。各平台在稳定性、调试难度、执行速度、内存占用等方面各有所长。“开源＋巨头支持”是目前市面上深度学习框架的主流模式，主流框架普遍由行业头部企业介入并主导内部应用搭建。同时，飞桨将在不同的领域进行产值赋能，聚焦当地产业集群发展重点，深度赋能企业智能化升级。百度飞桨作为中国首个最成熟完备的深度学习开源框架，已广泛应用于智能制造、智能金融、智能医疗等个业务场景。<br><img src="https://img-blog.csdnimg.cn/6f474b54960f40059c7c8d6ad130afe9.png" alt="avatar"><br>从图中不难看出，在 2020 年之前，深度学习框架仍旧是 TensorFlow 和 PyTorch 等主流 “ 玩家 “ 来主导，国内仅有百度飞桨一枝独秀。但在此之后，国内深度学习框架异军突起，呈现了第一波集中式的爆发。产业界方面，独角兽旷视拿出了工业级深度学习框架天元（MegEngine），一流科技OneFlow、华为昇思（MindSpore）也在同年登场。学术界方面，清华大学则是开源了支持即时编译的深度学习框架计图（Jittor）。而信通院此番研究结果，正是基于国内外现如今 “ 百家争鸣 “ 般的深度学习框架。从报告中来看，信通院这次将百度飞桨定位为” 中国深度学习市场应用规模第一 “，主要是从三个维度进行的考量。</p><h2 id="2-2-运行机制"><a href="#2-2-运行机制" class="headerlink" title="2.2 运行机制"></a>2.2 运行机制</h2><p>TensorFlow和PyTorch两个框架都是使用张量进行运算，并将任意一个模型看成是有向非循环图（DAG），但TensorFlow遵循“数据即代码，代码即数据”的理念，当在TensorFlow中运行代码时，DAG是以静态方式定义的，若需要实现动态DAG，则需要借助TensorFlowFold 库；而PyTorch属于更Python化的框架，动态DAG是内置的，可以随时定义、随时更改、随时执行节点，并且没有特殊的会话接口或占位符，相当灵活。此外，在调试方面，由于PyTorch中的计算图是在运行的时候定义的，因此使用者可以使用任何一个喜欢的调试工具，比如PDB、IPDB、PyCharm调试器或者原始的print语句；而TensorFlow并不能这样，它需要借助特殊的调试工具tfdbg才能进行调试。</p><p>不同之处在于，TensorFlow基于静态图，PyTorch基于动态图。在 TensorFlow 中，图结构是静态的，也就是说图在「编译」之后再运行。需要先构建一个计算图，构建好了之后，这样一个计算图是不能够变的了，然后再传入不同的数据进去，进行计算。即固定了计算的流程，所以变得不太灵活。如果要去改变计算的逻辑，或者随着时间变化的计算逻辑，这样的动态计算TensorFlow是实现不了的，或者是很麻烦。而PyTorch创建和运行计算图可能是这两个框架差别最大的地方。在 PyTorch 中，图结构是动态的，也就是说图是在运行时创建的。即就和python的逻辑是一样的，要对变量做任何操作都是灵活的。</p><p>飞桨则与两者不同，它同时兼容声明式编程（也叫符号式编程）和命令式编程两种编程范式。飞桨以程序化“Program”的形式动态描述神经网络模型计算过程，并提供对顺序、分支和循环三种执行结构的支持，可以组合描述任意复杂的模型，并可在内部自动转化为中间表示的描述语言。“Program”的定义过程就像在写一段通用程序。使用声明式编程时，相当于将“Program”先编译再执行，可类比静态图模式。首先根据网络定义代码构造“Program”，然后将“Program”编译优化，最后通过执行器执行“Program”，具备高效性能 ；同时由于存在静态的网络结构信息，能够方便地完成模型的部署上线。而命令式编程，相当于将“Program”解释执行，可视为动态图模式，更加符合用户的编程习惯，代码编写和调试也更加方便。飞桨发表声明将会在后续增强静态图模式下的调试功能，方便开发调试；同时提升动态图模式的运行效率，加强动态图自动转静态图的能力，快速完成部署上线;同时更加完善接口的设计和功能，整体提升框架易用性。  </p><p><img src="https://img-blog.csdnimg.cn/5a0c10f27cfa4d3193b48e44f5103930.png" alt="avatar">   </p><p>注：<br>1.目前深度学习框架主要有动态图和静态图两种编程方式。静态图属于符号化编程，代码执行的时候不会返回运算的结果，需要事先定义神经网络的结构，然后执行整个图结构；而动态图属于命令式编程，代码能够直接返回运算的结果。通常来说，静态图能够对编译器做最大的优化，更有利于性能的提升，而动态图则非常便于用户对程序进行调试。<br>2.两种编程范式，声明式编程（也叫符号式编程）和命令式编程。命令式编程是一种非常直观的编程方式，程序的定义对应着执行，可立即返回结果，大部分Python 的程序都是命令式编程，比如运行“x&#x3D;10; y&#x3D;x+2;”这段代码，y 的值能够立马获取，这种编程方式非常方便调试。而声明式编程，程序在定义的时候并不会立即执行。对于深度学习模型开发来讲，相当于先是构建一个完整的网络结构，例如表示成一张图，然后再统一执行前向反向计算。由于在执行前可以进行整体优化，因此这种方式往往可以做到执行效率更高。但是由于执行和定义的分离，所以不太方便测试。</p><h2 id="2-3-训练模式"><a href="#2-3-训练模式" class="headerlink" title="2.3 训练模式"></a>2.3 训练模式</h2><p>在分布式训练中，TensorFlow和PyTorch的一个主要差异特点是数据并行化，TensorFlow持支持分布式执行、支持大规模分布式训练：在GPU的分布式计算上更为出色，在数据量巨大时效率比pytorch要高一些。用TensorFlow时，使用者必须手动编写代码，并微调要在特定设备上运行的每个操作，以实现分布式训练；而PyTorch支持支持分布式执行、暂时不支持分布式训练，利用异步执行的本地支持来实现的，其自身在分布式训练是比较欠缺的。</p><p>Paddle Hub 是飞桨预训练模型管理和迁移学习工具，便于用户将预训练模型更好地适配到自的应用场景。飞桨的训练框架源自百度海量规模的业务场景实践，提供了大规模分布式训练和工业级数据处理的能力，同时支持稠密参数和稀疏参数场景的超大规模深度学习并行训练，支持万亿规模参数、数百个节点的高效并行训练。面对实际业务上数据实时变化或者膨胀的特点，对模型做流式更新，可提供实时更新的能力。与此同时，飞桨框架同时充分考虑训练和预测部署问题，一体化设计支持多端多平台的推理引擎，在高性能、轻量化和普适性方面开展重点优化。基于此设计，在多个硬件平台多种模型的对照评估中，飞桨预测框架显示出了领先的性能优势。</p><h2 id="2-4-可视化情况"><a href="#2-4-可视化情况" class="headerlink" title="2.4 可视化情况"></a>2.4 可视化情况</h2><p>在可视化方面，TensorFlow内置的TensorBoard库非常强大，TensorBoard 是一个用于可视化训练机器学习模型各个方面的工具。它是 TensorFlow 项目产出的最有用的功能之一。仅需在训练脚本中加入少许代码，你就可以查看任何模型的训练曲线和验证结果。TensorBoard 作为一个网页服务运行，可以尤其方便地可视化存储在headless节点上的结果。能够显示模型图，绘制标量变量，实现图像、嵌入可视化，甚至是播放音频等功能；反观PyTorch的可视化情况，则显得有点差强人意。PyTorch 的可视化只能调用matplotlib 、seaborn​​​​​​​等库。目前 PyTorch并没有可以和Tensorboard 匹敌的工具，不过倒是存在一些集成功能。虽然也能用一些绘图工具比如matplotlib和seaborn开发者可以使用Visdom，但是Visdom提供的功能很简单且有限，可视化效果远远比不上TensorBoard。</p><p>飞桨开源了训练可视化（Visual DL）深度学习的工具组件，用于配合深度学习相关技术的开发、训练、部署和应用。Visual DL 是一个面向深度学习任务的可视化工具，原生支持Python 的使用，只需在模型中增加少量的代码，对Visual DL 接口进行调用，便可以为训练过程提供丰富的可视化支持。</p><h2 id="2-5-模型部署"><a href="#2-5-模型部署" class="headerlink" title="2.5 模型部署"></a>2.5 模型部署</h2><p>对于模型部署而言，TensorFlow具有绝对优势。TensorFlow支持移动和嵌入式部署：但是在 TensorFlow 上，要将模型部署到安卓或iOS上需要不小的工作量，但至少你不必使用Java或C++重写你模型的整个推理部分。对于高性能服务器上的部署，还有 TensorFlow Serving 可用。除了性能方面的优势，TensorFlow Serving 的另一个重要特性是无需中断服务，就能实现模型的热插拔。PyTorch不支持移动和嵌入式部署：而包括 PyTorch 在内的很多深度学习框架都没有这个能力，需要使用Flask或者另一种替代方法来基于模型编写一个RESTAPI。  </p><p>对于模型的部署来说，飞桨提供了针对服务器端在线服务（Paddle Serving）和移动端部署库（Paddle Lite）。Paddle  Serving 是在线预测部分，可以与模型训练环节无缝衔接，提供深度学习的预测云服务。开发者也可以对预测库编译或直接下载编译好的预测库后，调用预测应用程序编程接口（Application Programming  Interface，API）， 对模型进行C++预测。在移动端，Paddle Lite 支持将模型部署到 ARM CPU（Advanced  Reduced  instruction  set  computing Machine Central Processing Unit），Mali GPU（Graphics Processing  Unit），Adreno  GPU，树莓派等各种硬件以及安卓、苹果、Linux 等软件系统之上。</p><h2 id="2-6-调试"><a href="#2-6-调试" class="headerlink" title="2.6 调试"></a>2.6 调试</h2><p>TensorFlow不易调试。TensorFlow 可选择用一个叫tfdbg的特殊工具，它能让你在运行时评估TensorFlow表达式，浏览所有张量，在会话范围中操作。当然，无法用它调试Python代码，因此无需单独使用pdb。</p><p>PyTorch容易理解且易调试。简单的图构建方式更容易理解，但也许更重要的是也更容易调试。调试 PyTorch 代码就跟调试Python代码一样。你可以使用pdb，并且可以在任何地方设置断点。</p><p>飞桨的调试方式是用户将定义好的“Program” 传入Executor，Executor根据“Program”中的Operator的顺序依次调用运行。飞桨还提供了Executor的一个升级版Parallel Executor，它的执行对象是编译优化过的“Program”（Compiled Program），相当于优化过的计算图，比如显存优化、Operator 合并等等。</p><h2 id="2-7-数据加载"><a href="#2-7-数据加载" class="headerlink" title="2.7 数据加载"></a>2.7 数据加载</h2><p>TensorFlow 的数据加载比较复杂。还没找到TensorFlow的非常有用的数据加载工具（读取器、队列、队列运行器等等）。部分原因是要将你想并行运行的所有预处理代码加入到TensorFlow图中并不总是那么简单直接（比如计算频谱图）。另外，TensorFlow的API本身也更加冗长，学习起来也更难。</p><p>PyTorch的数据加载 API设计得很好。数据集、采样器和数据加载器的接口都是特定的。数据加载器可以接收一个数据集和一个采样器，并根据该采样器的调度得出数据集上的一个迭代器（iterator）。并行化数据加载很简单，只需为数据加载器传递一个num_workers参数即可。</p><p>飞桨针对大规模数据场景，研发了一整套完备的扩展性组件包，包括多种通信拓扑的支持、梯度智能聚合、通信与计算节点依赖分析、多流并发通信等。同时，在不同模型结构下，通过对扩展性组件包中的组件进行智能组合，实现对用户透明的最佳并行扩展。</p><h2 id="2-8-设备管理"><a href="#2-8-设备管理" class="headerlink" title="2.8 设备管理"></a>2.8 设备管理</h2><p>TensorFlow 的设备管理默认即可，设备管理的无缝性能非常好，通常你不需要指定任何东西，因为默认的设置就很好。比如说，TensorFlow假设如果存在可用的 GPU，你就希望在 GPU 上运行。TensorFlow 设备管理的唯一缺陷是它会默认占用所有可用的 GPU 上的所有内存，即使真正用到的只有其中一个。但也有一种简单的解决方案，就是指定 CUDA_VISIBLE_DEVICES。有时候人们会忘记这一点，就会让 GPU 看起来很繁忙，尽管实际上它们啥也没干。</p><p>PyTorch的设备管理必须指定：而在PyTorch中，你必须在启用了CUDA之后明确地将所有东西移到 GPU 上。</p><p>飞桨主要通过两种方式来实现对不同硬件的支持 ：人工调优的核函数实现和集成供应商优化库。<br>（1）针对 CPU 平台飞桨一方面提供了使用指令 Intrinsic 函数和借助于 xbyakJIT 汇编器实现的原生 Operator，深入挖掘编译时和运行时性能 ；另一方面，飞桨通过引 入 Open BLAS、Intel® MKL、Intel® MKL-DNN和 n Graph，对 Intel CXL 等新型芯片提供了性能保证。<br>（2）针对 GPU 平台飞桨既为大部分 Operator 用 CUDA C 实现了经过人工精心优化的核函数，也集成了 cu BLAS、cu DNN等供应商库的新接口、新特性。</p><h2 id="2-9-适用场景建议"><a href="#2-9-适用场景建议" class="headerlink" title="2.9 适用场景建议"></a>2.9 适用场景建议</h2><p>当需要拥有丰富的入门资源、开发大型生产模型、可视化要求较高、大规模分布式模型训练时，TensorFlow或许是当前最好的选择；而如果想要快速上手、对于功能性需求不苛刻、追求良好的开发和调试体验、擅长Python化的工具时，PyTorch或许是值得花时间尝试的框架。总体而言，TensorFlow在保持原有优势的同时进一步融合包括Keras在内的优质资源，极大增强其易用性与可调试性，而PyTorch虽然年轻，但增长的势头猛烈，并通过融合Caffe2来进一步强化自身优势。两者都在保留原有优势的同时，努力补齐自身短板，这使得在某种程度上两者有融合的趋势。  </p><p>飞桨开源了计算机视觉、自然语言处理、推荐和语音四大类 70 多个官方模型，覆盖了各领域的主流和前沿模型。其中飞桨视觉模型库（Paddle CV）提供了图像分类、目标检测、图像分割、关键点检测、图像生成、文字识别、度量学习和视频分类与动作定位等多类视觉算法模型。飞桨自然语言处理模型库（Paddle NLP）包括了词法分析、语言模型、语义表示和文本生成等多项自然语言处理任务的算法模型实现，包括百度自研的增强语义表示模型ERNIE（Enhanced Representation from k Nowledge Int Egration）。  </p><p>未来哪一种框架更具优势，现在现在定论必定过早，因此，在选择框架时，可参照上述内容，并结合项目的时效、成本、维护等多方面综合考量后再决定。</p><h1 id="3-Tensorflow、pytorch、paddlepaddle深度学习软件社区生态构建"><a href="#3-Tensorflow、pytorch、paddlepaddle深度学习软件社区生态构建" class="headerlink" title="3. Tensorflow、pytorch、paddlepaddle深度学习软件社区生态构建"></a>3. Tensorflow、pytorch、paddlepaddle深度学习软件社区生态构建</h1><p>在社区生态构建方面，从全球范围来看TensorFlow和PyTorch仍是深度学习框架的主流”玩家”，但与此同时,也认可了百度飞桨自2016年以来的成绩。《深度学习平台发展报告（2022 年）》指出”我国已成为全球开发框架生态发展最快的国家”。报告统计了近一年半以来，我国以飞桨为代表的深度学习框架在贡献人数、关注等方面，与主流”玩家”的增速对比。基于此，报告认为：整体来看，目前飞桨社区生态仅次于 PyTorch，位居国内市场次席。而且报告针对国内玩家,围绕”活跃度”、”关注度”和”贡献人数”也做了数据的对比统计：<br><img src="https://img-blog.csdnimg.cn/82172ac073e042eba1b9d2a410bd82e9.png" alt="avatar"></p><h2 id="3-1-Tensorflow-谷歌全面生态系统"><a href="#3-1-Tensorflow-谷歌全面生态系统" class="headerlink" title="3.1 Tensorflow:谷歌全面生态系统"></a>3.1 Tensorflow:谷歌全面生态系统</h2><p>谷歌大脑自2011年成立起开展了面向科学研究和谷歌产品开发的大规模深度学习应用研究，其早期工作即是Tensor-Flow的前身DistBelief。它在研究和商业应用中的不同Alphabet公司中的使用迅速增长。2015年11月，在DistBelief的基础上，谷歌大脑完成了对“第二代机器学习系统”TensorFlow的开发并对代码开源。相比于前作TensorFlow在性能上有显著改进、构架灵活性和可移植性也得到增强。<br><img src="https://img-blog.csdnimg.cn/14c85d374a8d4fb685c59dee61c23c07.png" alt="avatar"><br>TensorFlow主要用于进行机器学习和深度神经网络研究，同时也可以应用于众多领域。由于谷歌在深度学习领域的巨大影响力和强大的推广能力，TensorFlow-经推出就获得了极大的关注。此后TensorFlow快速发展，截至稳定API版本2.0，已拥有包含各类开发和研究项目的完整生态系统。<br><img src="https://img-blog.csdnimg.cn/0cb251183f104a429cc80818aff5c18b.png" alt="avatar">  </p><h2 id="3-2-PyTorch：Meta-python机器学习库"><a href="#3-2-PyTorch：Meta-python机器学习库" class="headerlink" title="3.2 PyTorch：Meta python机器学习库"></a>3.2 PyTorch：Meta python机器学习库</h2><p>PyTorch是Meta开发的用于训练神经网络的开源Python机器学习库，也是Meta倾力打造的首选深度学习软件框架。2017年1月首次推出，Meta人工智能研究院 (FAIR)在GitHub. 上开源了PyTorch，迅速占领了GitHub热度榜榜首。Meta用Python重写了基于Lua语言的深度学习库 Torch，PyTorch不是 简单的封装Torch提供Python接口，而是对Tensor.上的全部模块进行了重构，新增了自动求导系统，使其成为最流行的动态图框架，这使得PyTorch对于开发人员更为原生，与TensorFlow相比也更加年轻活力，PyTorch继承了Torch灵活、动态的编程环境和用户友好界面，支持以快速和灵活的方式构建动态神经网络，还允许在训练过程中快速更改代码而不妨碍其性能，即支持动态图形等尖端AI模型的能力，是快速实验的理想选择。PyTorch专注于快速原型设计和研究的灵活性，很快就成为AI研究人员的热门选择，流行度增长十分迅猛。自2017年开源以来，陆续推出了Tensor与Numpy互转、支持Windows系统、ONNX转换、支持Tensorboard、分布式训练等功能。<br><img src="https://img-blog.csdnimg.cn/518ced7103a74118aae8a2ac9ae03d7e.png" alt="avatar"><br><img src="https://img-blog.csdnimg.cn/f68e2776c3214c7fbfc5eecd1e6594ae.png" alt="avatar">  </p><h2 id="3-3-飞桨PaddlePaddle：百度产业级深度学习开源开放平台"><a href="#3-3-飞桨PaddlePaddle：百度产业级深度学习开源开放平台" class="headerlink" title="3.3 飞桨PaddlePaddle：百度产业级深度学习开源开放平台"></a>3.3 飞桨PaddlePaddle：百度产业级深度学习开源开放平台</h2><p>百度飞桨是中国首个自主研发、功能丰富、开源开放的产业级资深学习软件框架，核心技术积累深厚。2016年8月百度开源了深度学习框架PaddlePaddle，并于2019年正式命名为飞桨。截止2021年，飞桨平台已拥有406万开发者，服务15.7万企事业单位，创建高达47.6万个。市场份额方面，飞桨在中国综合市场份额已经超越了Tensorflow和Pytorch，位居中国深度学习市场第一。飞桨较为注重深度学习技术的产业应用能力，不仅在业内率先实现了动静统一的框架设计，而且在保证开发灵活性的同时，更加适配产业应用场景，从实际产业需求出发形成了一套满足产业级业务需求的深度学习开源平台。<br><img src="https://img-blog.csdnimg.cn/ddf2913ad941497dad5164c22f4afac5.png" alt="avatar"><br>过去几年来，百度飞桨官方发布的产业级开源算法模型已经超过了400个，并发布13个精度与性能平衡的产业级PP系列模型，覆盖工业、农业、交通、科学计算等20多个行业领域。近年来，凭借深厚的工业基因，飞桨在行业关键和前沿场景的渗透率不断提升。<br><img src="https://img-blog.csdnimg.cn/8281a1cdb182405d8ffea03e18b1cfd6.png" alt="avatar"><br>飞桨在中国市场上芯片适配数量稳居一位，形成软硬协同优势。飞桨与英特尔、英伟达、海光、飞腾等在内的25家国内外硬件厂商，完成了超过30种芯片或IP的适配和优化。作为英伟达支持的三大框架之一，飞桨是英伟达唯一深度适配的中国框架。<br><img src="https://img-blog.csdnimg.cn/730fadb4f1944d7597de0a11aeece378.png" alt="avatar">  </p><h1 id="4-Tensorflow、pytorch、paddlepaddle优势分析比较"><a href="#4-Tensorflow、pytorch、paddlepaddle优势分析比较" class="headerlink" title="4. Tensorflow、pytorch、paddlepaddle优势分析比较"></a>4. Tensorflow、pytorch、paddlepaddle优势分析比较</h1><h2 id="4-1-Tensorflow"><a href="#4-1-Tensorflow" class="headerlink" title="4.1 Tensorflow"></a>4.1 Tensorflow</h2><h3 id="4-1-1-Tensorflow优势"><a href="#4-1-1-Tensorflow优势" class="headerlink" title="4.1.1 Tensorflow优势:"></a>4.1.1 Tensorflow优势:</h3><p>(1)Tensorflow具有高度的灵活性。<br>Tensorflow并不是一个严格的“神经网络”数据库,任何用户都可以通过构建一个数据流图来表示驱动计算的内部循环:它能够为用户提供有用的工具,帮助用户组装被广泛应用于神经网络的“子图”,同时用户也可以Tensorflow的基础上编写自己的“上层库”。当然,如果用户发现找不到想要的底层数操作,可以自己编写C+代码来丰富底层的操作。<br>(2)Tensorflow具有真正的可移植性。<br>Tensorflow可以在CPU和GPU上运行,例如运行在台式机、服务器、手机上等: Tensorflow既可以帮助用户在没有特殊硬件的前提下，在自己的笔记本上运行机器学习的新想法,也可以帮助用户将自己的训练模型在多个CPU上规模化运算,而不用修改代码;Tensorflow可以帮助用户将训练好的模型作为产品的部分应用到手机App里,还可以帮助用户将自己的模型作为云端服务运行在自己的服务上,或者运行在Docker容器里。<br>(3)Tensorflow可以加强科研和产品之间的相关性。<br>过去,需要通过大量的代码重写才能够将科研中的新想法应用于产品;而现在,科学家用Tensorflow尝试新的算法,产品团队则用 Tensorflow来训练和测试新的计算模型,并直接提供给在线用户。使用 Tensorflow,既可以让应用型研究者将新想法迅速运用到产品中,也可以让学术型研究更方便地分享代码,从而提高科研效率。<br>(4)Tensorflow具有自动求微分的能力。<br>它能够使得一些基于梯度的机器学习算法更加快速且准确。作为Tensorflow用户,只需要定义需要预测的模型结构,将这个结构和目标函数结合在一起,并添加数据, Tensorflow将自动计算相关的微分导数。<br>(5)Tensorflow支持多语言。<br>Tensorflow有一个合理的C++使用界面,也有一个易用的Python使用界面来构建和执行用户的图片和视频。可以直接编写Python&#x2F;C++程序,也可以在交互式的 Ipython界面中用Tensorflow来尝试新想法。当然,这只是个起点。—这个平台将鼓励用户创造出自己最喜欢的语言界面,截至版本1.12.0, Tensorflow绑定完后成后并支持版本兼容运行的语言为C和 Python,其他绑定完成的语言为Javascript、C+、JavaGo和Swift,依然处于开发阶段的语言包括C、Haskell、Julia、Ruby、Rust和 Scala。<br>(6)Tensorflow可以最大化系统性能。<br>如果用户拥有一个32个CPU内核、4个GPU显卡的工作站,则 Tensorflow可以帮助用户将工作站的计算潜能全部发挥出来。Tensorflow支持线程、队列、异步操作等,可以帮助用户将硬件的计算潜能全部发挥出来。用户可以自由地将Tensorflow数据流图中的计算元素分配到不同的设备上, Tensorflow可以帮助用户管理好这些不同的副本。<br>(7)Tensorflow支持分布式执行。<br>2017年10月31日,谷歌发布了Tensorflow EagerExecution(贪婪执行),为 Tensorflow添加了命令式编程接口。在此之前的 Tensorflow有一个很大的缺点,就是只支持静态图模型。也就是说,之前的Tensorflow在处理数据前必须预先定义好一个完整的模型,这要求数据非常完整,但是在实际工程操作中难免会有不完整的数据模型,这些模型处理起来会很麻烦。而现在,Tensorflow操作会立刻执行,不需要执行一个预先定义的数据流图,从而大大地提高了工作效率。<br>(8) Tensorflow可以进行迁移学习( Transfer Learning)。<br>许多Tensorflow模型都包含可训练、可优化示例,方便研发人员进行迁移学习。迁移学习就是在训练好的模型上继续训练其他内容,充分使用原来模型的权重,这样可以节省重复训练大型模型的时间,提高工作效率。因此,有新想法的用户可以不用花费时间重新训练一个无比庞大的神经网络,只需要在原来的神经网络上进行训练和测试即可。<br>(9)Tensorflow生态系统包含许多工具和库。<br>Tensorboard是一个Web应用程序,它可以把复杂的神经网络训练过程可视化,也可以帮助理解、调试并优化程序。TensorflowServing组件是一个为生产环境而设计的高性能的机器学习服务系统,它可以将Tensorflow训练好的模型导出,并部署成可以对外提高预测服务的接口。Jupyter Notebook是一款开放源代码的Web应用程序,用户可创建并共享代码和文档;它提供了一个环境,用户可以在其中记录、运行代码,并查看结果,可视化数据后再查看输出结果。 Facets是一款开源可视化工具,可以帮助用户理解并分析各类机器学习数据集。例如,用户可以查看训练和测试数据集,比较每个要素的特征,并按照具体特征对要素进行排序。<br>(10) Tensorflow支持CPU和GPU运行。<br>在程序中,设备使用字符串进行表示,CPU表示为“CPU:0O”。第一个GPU表示为“ &#x2F;device:GPU:O”,第二个GPU表示为“ &#x2F;device:GPU:1”,以此类推。如果 Tensorflow指令中兼有CPU和GPU实现,当该指令分配到设备时,则GPU设备的优先级高于CPU的。Tensorflow会尽可能地使用GPU内存,最理想的情况是进程只分配可用内存的一个子集,或者仅根据进程需要增加内存使用量,为此,启用会话时可通过两个编译选项来执行GPU进程。  </p><h3 id="4-1-2-Tensorflow基础"><a href="#4-1-2-Tensorflow基础" class="headerlink" title="4.1.2 Tensorflow基础"></a>4.1.2 Tensorflow基础</h3><p>Tensorflow是谷歌公司在Disbelief(分布式深度学习平台)的基础上改进的第二代人工智能学习系统,而它的名字则来源于其运行原理。Tensor(张量)意味着N维数组,Flow(流)意味着基于数据流图的计算,Tensorflow是一个将数据流图(Data Flow Graphs)应用于数值计算的开源软件库。</p><p>Tensorflow是一个将复杂的数据结构传输至人工智能神经网络中,并对其进行分析和处理的系统。其中,节点(Nodes)在数据流图中表示数学操作,而数据流图中的线((Edges))则表示在节点间相互联系的多维数据数组,即张量(Tensor)。它具有非常灵活的架构,能够帮助用户在多种平台上展开计算,也被广泛应用于语音识别或图像识别等多项机器学习和深度学习领域。</p><p>在硬件方面,TPU是谷歌为Tensorflow定制的专用芯片。TPU被应用于谷歌的云计算平台,并作为机器学习产品开放研究和商业使用。Tensorflow的神经网络API Estimator有支持TPU下可运行的版本Tpuestimator。Tpuestimator可以在本地进行学习&#x2F;调试并上传到谷歌云计算平台进行计算。使用云计算TPU设备,需要快速向TPU供给数据, 为此可使用tf data. Dataset API(Application Programming Interface,应用编程接口)从谷歌云储分区中构建输入管道。小数据集可使用 tf data. Dataset. cache完全加载到内存中,大数据可转化为 Tfrecord格式并使用tf.data.Tfrecorddataset进行读取。</p><h3 id="4-1-3-TensorFlow应用案例"><a href="#4-1-3-TensorFlow应用案例" class="headerlink" title="4.1.3 TensorFlow应用案例"></a>4.1.3 TensorFlow应用案例</h3><p>TensorFlow是Google开发出来的应用库，目前是开源的软件。根据深度学习的应用性，它目前在图形分类、音频处理、推荐系统和自然语言处理等场景下都有丰富的应用。国外很多著名高校的课程使用 Tensor Flow 作为授课和作业的编程语言，目前国内也有很多高校在筹备以Tensor Flow作为深度学习的框架课程软件。</p><h3 id="4-1-4-TensorFlow活跃程度"><a href="#4-1-4-TensorFlow活跃程度" class="headerlink" title="4.1.4 TensorFlow活跃程度"></a>4.1.4 TensorFlow活跃程度</h3><p>如今在谷歌内部，TensorFlow已经得到了广泛的应用。在2015年10月26日，谷歌正式宣布通过TensorFlow实现的排序系统RankBrain上线。相比一些传统的排序算法，使用RankBrain的排序结果更能满足用户需求。在2015年彭博（Bloomberg）的报道中，谷歌透露了在谷歌上千种排序算法中，RankBrain是第三重要的排序算法。基于TensorFlow的系统RankBrain能在谷歌的核心网页搜索业务中占据如此重要的地位，可见TensorFlow在谷歌内部的重要性。包括网页搜索在内，TensorFlow已经被成功应用到了谷歌的各款产品之中。如今，在谷歌的语音搜索、广告、电商、图片、街景图、翻译、YouTube等众多产品之中都可以看到基于TensorFlow的系统。在经过半年的尝试和思考之后，谷歌的DeepMind团队也正式宣布其之后所有的研究都将使用TensorFlow作为实现深度学习算法的工具。</p><p>除了在谷歌内部大规模使用之外，TensorFlow也受到了工业界和学术界的广泛关注。在Google I&#x2F;O 2016的大会上，Jeff Dean提到已经有1500多个GitHub的代码库中提到了TensorFlow，而只有5个是谷歌官方提供的。如今，包括优步（Uber）、Snapchat、Twitter、京东、小米等国内外科技公司也纷纷加入了使用TensorFlow的行列。正如谷歌在TensorFlow开源原因中所提到的一样，TensorFlow正在建立一个标准，使得学术界可以更方便地交流学术研究成果，工业界可以更快地将机器学习应用于生产之中。</p><h3 id="4-2-Pytorch"><a href="#4-2-Pytorch" class="headerlink" title="4.2 Pytorch"></a>4.2 Pytorch</h3><h3 id="4-2-1-Pytorch优势"><a href="#4-2-1-Pytorch优势" class="headerlink" title="4.2.1 Pytorch优势"></a>4.2.1 Pytorch优势</h3><p>(1)Pytorch灵活性好与速度快捷<br>可以混合前端,新的混合前端在急切模式和图形模式之间无缝转换,以提供灵活性和速度。<br>(2)Python语言优先。<br>Pytorch的深度集成允许用户在Python中使用流行的库和包编写神经网络层。<br>(3)Pytorch拥有丰富的工具和函数库。丰富的工具和库生态系统扩展了 Pytorch,使得 Python支持计算机视觉、NLP(Neuro-Linguistic Programming,神经语言程序学)等领域的开发;C前端是Pytorch的纯C++接口,它遵循已建立的Python前端的设计和体系结构。<br>(4)Pytorch可以快速实现。<br>在深度学习的训练过程中,用户会有很多奇思妙想,但这些新想法需要通过实验来验证。如果实现比较困难,而且在创新点的不确定性特别大时,用户会很容易放弃这个设想。而 Pytorch可以解放用户的思想,用Tensor的思维思考代码,即一切操作均在Tensor的基础上进行,一切Tensor能做的,Pytorch都能做到。<br>(5)Pytorch具有简洁易懂的代码。<br>如果用户不懂某个框架的源码,就不能完全掌握它的运行原理。在Pytorch框架中,任何一个操作,不论多么高级复杂,都能轻松地找到与它对应的Tensor操作。在大多数实际情况中,Pytorch都比Tensorflow更高效,但这并不是说Tensorflow速度慢,而是说要用Tensorflow写出同等速度的代码会稍微困难一些,仅仅是加载数据这一方面就会非常复杂且耗时。<br>(6)Pytorch具有强大的社区。<br>Pytorch论坛、文档一应俱全,而且它得到了Facebook的FAIR(Facebook的人工智能实验室)的强力支持。FAIR的几位工程师全力维护开发Pytorch,Github上基于 Pytorch框架的源码每天都有许多建议和讨论。<br>(7)Pytorch使用命令式热切式范式。<br>也就是说,构建图形的每行代码都定义了该图的一个组件,即使在图形完全构建之前,我们也可以独立地对这些组件进行计算。这被称为运行时定义法(Define-by-Run)。  </p><h3 id="4-2-2-Pytorch基础"><a href="#4-2-2-Pytorch基础" class="headerlink" title="4.2.2 Pytorch基础"></a>4.2.2 Pytorch基础</h3><p>Pytorch的基础主要包括以下三个方面:<br>(1)Numpy风格的Tensor操作. Pytorch中Tensor提供的AP参考了Numpy的设计, 因此熟悉Numpy的用户基本上能够借鉴原来的经验,自行创建和操作 Tensor.,同时 Torch中的数组和 Numpy数组对象也可以无缝对接。<br>(2)变量自动求导。在序列计算过程形成的计算图中,所有变量都可以方便且快速地计算出自己对目标函数的梯度值。这样就可以方便地实现神经网络的后向传播。<br>(3)神经网络层、损失函数和优化函数等高层被封装。网络层的封装存在torch.nn模块中,损失函数由 torch. nn. functional模块提供,优化函数由 torch. optim模块提供。</p><h3 id="4-2-3-Pytorch应用案例"><a href="#4-2-3-Pytorch应用案例" class="headerlink" title="4.2.3 Pytorch应用案例"></a>4.2.3 Pytorch应用案例</h3><p>近几年, Pytorch正在被广泛应用于机器学习的各种领域。最近的几个应用包括:加州大学伯克利分校计算机科学家所构建的项目,它基于循环一致对抗网络进行非配对图到图的转换,该项目通过使用一组对齐的图像训练集来学习图像输入和输出映射;科学家正在利用 Pytorch架构实现AOD-Net(一种网络模型)图片去雾以及 Faster R-CNN(一种网络模型)和 Maske-CNN(一种网络模型)的神经网络模型构建。自2016年1月初发布以来,许多研究人员已将 Pytorch作为一种实现库,因为它易于构建新颖且复杂的计算图。作为一个新的并且正在建设中的框架,我们也期待着 Pytorch能够尽快被更多数据科学从业者熟知并采用。</p><h3 id="4-2-4-Pytorch活跃程度"><a href="#4-2-4-Pytorch活跃程度" class="headerlink" title="4.2.4 Pytorch活跃程度"></a>4.2.4 Pytorch活跃程度</h3><p>从早期的学术框架 Caffe、Theano，到后来的PyTorch、TensorFlow，自 2012 年深度学习再度成为焦点以来，很多机器学习框架成为研究者和业界工作者的新宠。2018 年底，谷歌推出了全新的JAX框架，其受欢迎程度也一直在稳步提升。很多研究者对其寄予厚望，希望它可以取代 TensorFlow 等众多深度学习框架。不过，PyTorch 和 TensorFlow仍是ML框架领域的两大实力玩家，其他新生框架的力量暂时还无法匹敌。而PyTorch 和 TensorFlow之间则是此消彼长的关系，力量对比也在悄悄发生着变化。2019年10月，康奈尔大学本科生、曾在PyTorch 团队实习的HoraceHe曾对PyTorch和TensorFlow在学界的使用情况进行了数据统计。结果显示，研究者已经大批涌向了PyTorch，不过当时看来，业界的首选仍是TensorFlow。<br>如下图所示，从2019年中期开始，在统计的各大顶会中，PyTorch从使用率指标上就已完成了对TensorFlow的反超。截止目前，EMNLP、ACL、ICLR三家顶会的PyTorch的占比已经超过80%，这一占比数字在其他会议中也都保持在70%之上。短短两年间，TensorFlow的生存空间又大幅缩小。<br><img src="https://img-blog.csdnimg.cn/6fd3afb508764208b9f6713bd4650652.png" alt="avatar"></p><h2 id="4-3-Paddlepaddle"><a href="#4-3-Paddlepaddle" class="headerlink" title="4.3 Paddlepaddle"></a>4.3 Paddlepaddle</h2><h3 id="4-3-1-Paddlepaddle优势"><a href="#4-3-1-Paddlepaddle优势" class="headerlink" title="4.3.1 Paddlepaddle优势"></a>4.3.1 Paddlepaddle优势</h3><p>Paddlepaddle具有以下优点:<br>(1)代码易于理解,官方提供丰富的学习资料及工具,并且帮助用户迅速成为深度学习开发者。<br>(2)框架具备非常好的扩展性,并且提供了丰富全面的API,能够实现用户各种天马行空的创意。<br>(3)基于百度多年的AI技术积累以及大量的工程实践验证,框架安全稳定。<br>(4)架能够一键安装,针对CPU、GPU都做了众多优化,分布式性能强劲,并且具有很强的开放性。我们可以在Linux系统下使用最新版的pip(Python包管理工具)快捷地安装和运行Paddlepaddle  </p><h3 id="4-3-2-Paddlepaddle基础"><a href="#4-3-2-Paddlepaddle基础" class="headerlink" title="4.3.2 Paddlepaddle基础"></a>4.3.2 Paddlepaddle基础</h3><p>Paddlepaddle的前身是百度于2013年自主研发的深度学习平台。2016年9月1日百度世界大会上,百度首席科学家吴恩达首次宣布将百度深度学习平台对外开放,命名为Paddlepaddle。<br>2016年, Paddlepaddle已实现CPU&#x2F;GPU单机和分布式模式,同时支持海量数据训练数百台机器并行运算,可以轻松应对大规模的数据训练,这方面目前的开源框架中可能只有谷歌的Tensorflow能与之相比。此外, Paddlepaddle具有易用、高效、灵活和可伸缩等特点,且具备更丰富、更有价值的GPU代码。它提供了神经机翻译系统(Neural MachineTranslation)、推荐、图像分类、情感分析、语义角色标注(Semantic Role Labelling)等5个任务,每个任务都可迅速上手,且大部分任务可直接套用。特别值得一提的是,与此前的对比测试结果显示,在训练数据和效果相同的情况下, Paddlepaddle比谷歌的Tensorflow训速度可能会更快。据了解,这主要是由于Paddlepaddle的框架设计更具优势,并且未来有很大的潜力,速度可能会更快。</p><h3 id="4-3-3-Paddlepaddle应用案例"><a href="#4-3-3-Paddlepaddle应用案例" class="headerlink" title="4.3.3 Paddlepaddle应用案例"></a>4.3.3 Paddlepaddle应用案例</h3><p>飞桨开源了计算机视觉、自然语言处理、推荐和语音四大类70多个官方模型，覆盖了各领域的主流和前沿模型。其中飞桨视觉模型库（Paddle CV）提供了图像分类、目标检测、图像分割、关键点检测、图像生成、文字识别、度量学习和视频分类与动作定位等多类视觉算法模型。飞桨自然语言处理模型库（Paddle NLP）包括了词法分析、语言模型、语义表示和文本生成等多项自然语言处理任务的算法模型实现，包括百度自研的增强语义表示模型ERNIE（Enhanced Representation from k Nowledge Int Egration）。ERNIE 通过建模海量数据中的词、实体及实体关系，直接对先验语义知识单元进行建模。与谷歌语义表示模型 BERT（Bidirectional Encoder Representation  from Transformers）相比，BERT 仅学习原始语言信号，ERNIE 增强了模型语义表示能力。</p><p>2016年以来, Paddlepaddle已经在百度几十项主要产品和服务之中发挥了巨大的作用,如外卖的预估出餐时间、预判网盘故障时间点、精准推荐用户所需信息、海量图像识别分类、字得识(Optical Character Recognition,OCR)毒和垃位圾信息检测、机器翻译和自动驾驶等领域。相信在不久的将来, Paddlepaddle会被越来越多的用户使用,并且被应用于其他的深度学习领域。</p><h3 id="4-3-4-Paddlepaddle活跃程度"><a href="#4-3-4-Paddlepaddle活跃程度" class="headerlink" title="4.3.4 Paddlepaddle活跃程度"></a>4.3.4 Paddlepaddle活跃程度</h3><p>飞桨为企业开发者提供不同层次的培养体系，帮助开发者成长，助力企业AI创新。具体包括黄埔学院、AI 快车道、Paddle Camp 等。其中黄埔学院致力于为中国产业界培养第一批首席AI架构师；AI快车道侧重应用实战，为1000家企业提供深度学习技术应用落地支持；Paddle Camp 面向深度学习初学者进行入门培训。除了以上培训，飞桨还持续提供丰富的AI开发者竞赛，构建开发者生态体系。此外，飞桨还通过深度学习师资培训班、在线教育平台和深度学习工程师能力评估标准等助力高校与教育机构快速搭建完整的AI教学体系。</p><p>目前，飞桨在开源社区中的活跃度较高，已经覆盖超过16000家企业和130万名开发者。随着在开源社区的影响力逐步增强，包括英特尔、英伟达、华为、寒武纪、比特大陆等诸多芯片厂商也纷纷支持飞桨，并基于此开展合作。后续飞桨还需要在如下几个方面进一步完善 ：核心框架的高层API目前还比较缺乏，易用性有待进一步提升；基于飞桨的学术论文还不够多，生态还需要持续加大力度建设等。</p><h1 id="5-参考文献与资料-近三年"><a href="#5-参考文献与资料-近三年" class="headerlink" title="5. 参考文献与资料(近三年)"></a>5. 参考文献与资料(近三年)</h1><p>[1]薛先贵,黎路.深度学习工具Tensorflow的解析[J].福建电脑,2021,37(01):105-106.DOI:10.16707&#x2F;j.cnki.fjpc.2021.01.039.<br>[2]黄玉萍,梁炜萱,肖祖环.基于TensorFlow和PyTorch的深度学习框架对比分析[J].现代信息科技,2020,4(04):80-82+87.DOI:10.19850&#x2F;j.cnki.2096-4706.2020.04.021.<br>[3]本刊讯.Facebook发布深度学习框架PyTorch 1.3[J].数据分析与知识发现,2019,3(10):65.<br>[4]马艳军,于佃海,吴甜,王海峰.飞桨：源于产业实践的开源深度学习平台[J].数据与计算发展前沿,2019,1(05):105-115.<br>[5].百度PaddlePaddle联手Kubernetes[J].商业观察,2017(Z1):78-79.<br>[6]<a href="https://max.book118.com/html/2022/0520/5210142203004231.shtm">《中国深度学习软件框架市场研究报告（2021）》</a><br>[7]<a href="https://www.baogaoting.com/info/171743">《深度学习平台发展报告（2022年）》</a>  </p>]]></content>
      
      
      
        <tags>
            
            <tag> -Tensorflow、pytorch、paddlepaddle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo个人博客魔改教程（三）</title>
      <link href="/post/4b6a0bb7.html"/>
      <url>/post/4b6a0bb7.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>本博客魔改目前包括：音乐全局吸底APlayer、留言板魔改、2d看板娘、Gitcalendar、公告栏星空小人、CDN全站加速、优化博客文章链接、SEO百度索引。<br>注：博客美化是一个长期的过程，所以本文将随作者懒惰＋勤奋的心情不定期更新。感兴趣的话就读下去吧。<br><img src="https://img-blog.csdnimg.cn/20200131230320697.jpg" alt="avatar"></p><h1 id="留言板魔改"><a href="#留言板魔改" class="headerlink" title="留言板魔改"></a>留言板魔改</h1><p>（1）在在根目录下新建themes的butterfly的layout的includes的page的envelope.pug</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#article-container</span><br><span class="line">  if top_img === false</span><br><span class="line">    h1.page-title= page.title</span><br><span class="line">  #maincontent</span><br><span class="line">    #form-wrap</span><br><span class="line">      img#beforeimg(src=&#x27;https://cdn.jsdelivr.net/gh/Akilarlxh/Valine-Admin@v1.0/source/img/before.png&#x27;)</span><br><span class="line">      #envelope</span><br><span class="line">        form</span><br><span class="line">          .formmain</span><br><span class="line">            img.headerimg(src=url_for(theme.envelope_comment.cover))</span><br><span class="line">            .comments-main</span><br><span class="line">              h3.title3=`来自` + config.author + `的留言:`</span><br><span class="line">              .comments</span><br><span class="line">                each i in theme.envelope_comment.message</span><br><span class="line">                  div=`$&#123;i&#125;`</span><br><span class="line">              .bottomcontent</span><br><span class="line">                img.bottomimg(src=&#x27;https://ae01.alicdn.com/kf/U0968ee80fd5c4f05a02bdda9709b041eE.png&#x27;)</span><br><span class="line">              p.bottomhr=`$&#123;theme.envelope_comment.bottom&#125;`</span><br><span class="line">      img#afterimg(src=&#x27;https://cdn.jsdelivr.net/gh/Akilarlxh/Valine-Admin@v1.0/source/img/after.png&#x27;)</span><br><span class="line">  != page.content</span><br></pre></td></tr></table></figure><p>（2）在根目录下新建themes的butterfly的source的css的_layout的commentsbar.styl</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if hexo-config(&#x27;envelope_comment.enable&#x27;)</span><br><span class="line">  $hoverHeight = hexo-config(&#x27;envelope_comment.height&#x27;) ? convert(hexo-config(&#x27;envelope_comment.height&#x27;)) : 1050px</span><br><span class="line">  @media screen and (max-width: 600px)</span><br><span class="line">    #beforeimg,</span><br><span class="line">    #afterimg</span><br><span class="line">      display none !important</span><br><span class="line"></span><br><span class="line">  @media screen and (min-width: 600px)</span><br><span class="line">    #article-container</span><br><span class="line">      img</span><br><span class="line">        margin 0 auto 0rem</span><br><span class="line">    #form-wrap</span><br><span class="line">      overflow hidden</span><br><span class="line">      height 447px</span><br><span class="line">      position relative</span><br><span class="line">      top 0px</span><br><span class="line">      transition all 1s ease-in-out .3s</span><br><span class="line">      z-index 0</span><br><span class="line">      &amp;:hover</span><br><span class="line">        height $hoverHeight</span><br><span class="line">        top -200px</span><br><span class="line">    #maincontent</span><br><span class="line">      width 530px</span><br><span class="line">      margin 20px auto 0</span><br><span class="line">    #beforeimg</span><br><span class="line">      position absolute</span><br><span class="line">      bottom 126px</span><br><span class="line">      left 0px</span><br><span class="line">      background-repeat no-repeat</span><br><span class="line">      width 530px</span><br><span class="line">      height 317px</span><br><span class="line">      z-index -100</span><br><span class="line">      pointer-events none</span><br><span class="line">    #afterimg</span><br><span class="line">      position absolute</span><br><span class="line">      bottom -2px</span><br><span class="line">      left 0</span><br><span class="line">      background-repeat no-repeat</span><br><span class="line">      width 530px</span><br><span class="line">      height 259px</span><br><span class="line">      z-index 100</span><br><span class="line">      pointer-events none</span><br><span class="line">    #envelope</span><br><span class="line">      position relative</span><br><span class="line">      overflow visible</span><br><span class="line">      width 500px</span><br><span class="line">      margin 0px auto</span><br><span class="line">      transition all 1s ease-in-out .3s</span><br><span class="line">      padding-top 200px</span><br><span class="line"></span><br><span class="line">  .headerimg</span><br><span class="line">    width 100%</span><br><span class="line">    overflow hidden</span><br><span class="line">    pointer-events none</span><br><span class="line"></span><br><span class="line">  .formmain</span><br><span class="line">    background white</span><br><span class="line">    width 95%</span><br><span class="line">    max-width 800px</span><br><span class="line">    margin auto auto</span><br><span class="line">    border-radius 5px</span><br><span class="line">    border 1px solid</span><br><span class="line">    overflow hidden</span><br><span class="line">    -webkit-box-shadow 0px 0px 20px 0px rgba(0, 0, 0, 0.12)</span><br><span class="line">    box-shadow 0px 0px 20px 0px rgba(0, 0, 0, 0.18)</span><br><span class="line">  .comments-main</span><br><span class="line">    padding 5px 20px</span><br><span class="line">  .title3</span><br><span class="line">    text-decoration none</span><br><span class="line">    color $theme-color</span><br><span class="line">    text-align center</span><br><span class="line">  .comments</span><br><span class="line">    text-align center</span><br><span class="line">    border-bottom #ddd 1px solid</span><br><span class="line">    border-left #ddd 1px solid</span><br><span class="line">    padding-bottom 20px</span><br><span class="line">    background-color #eee</span><br><span class="line">    margin 15px 0px</span><br><span class="line">    padding-left 20px</span><br><span class="line">    padding-right 20px</span><br><span class="line">    border-top #ddd 1px solid</span><br><span class="line">    border-right #ddd 1px solid</span><br><span class="line">    padding-top 20px</span><br><span class="line"></span><br><span class="line">  .bottomcontent</span><br><span class="line">    text-align center</span><br><span class="line">    margin-top 40px</span><br><span class="line"></span><br><span class="line">  .bottomimg</span><br><span class="line">    width 100%</span><br><span class="line">    margin 5px auto 5px auto</span><br><span class="line">    display block</span><br><span class="line">    pointer-events none</span><br><span class="line"></span><br><span class="line">  .bottomhr</span><br><span class="line">    font-size 12px</span><br><span class="line">    text-align center</span><br><span class="line">    color #999</span><br><span class="line"></span><br><span class="line">  [data-theme=&#x27;dark&#x27;]</span><br><span class="line">    .formmain</span><br><span class="line">      background rgb(50, 50, 50)</span><br><span class="line">    .comments</span><br><span class="line">      background rgba(90, 90, 90, 0.8)</span><br></pre></td></tr></table></figure><p>（3）修改根目录下的themes的butterfly的layout的page.pug</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">block content</span><br><span class="line">  #page</span><br><span class="line">    case page.type</span><br><span class="line">      when &#x27;tags&#x27;</span><br><span class="line">        include includes/page/tags.pug</span><br><span class="line">      when &#x27;link&#x27;</span><br><span class="line">        include includes/page/flink.pug</span><br><span class="line">      when &#x27;categories&#x27;</span><br><span class="line">        include includes/page/categories.pug</span><br><span class="line">      when &#x27;artitalk&#x27;</span><br><span class="line">        include includes/page/artitalk.pug</span><br><span class="line">      when &#x27;envelope&#x27;</span><br><span class="line">        include includes/page/envelope.pug</span><br><span class="line">    default</span><br><span class="line">      include includes/page/default-page.pug</span><br></pre></td></tr></table></figure><p>修改根目录下的_config.butterfly.yml，新增配置项</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">envelope_comment:</span><br><span class="line">  enable: true #开关</span><br><span class="line">  cover: https://ae01.alicdn.com/kf/U5bb04af32be544c4b41206d9a42fcacfd.jpg #信笺封面图</span><br><span class="line">  message: #信笺内容，支持多行</span><br><span class="line">    - 有什么想问的？</span><br><span class="line">    - 有什么想说的？</span><br><span class="line">    - 有什么想吐槽的？</span><br><span class="line">    - 哪怕是有什么想吃的，都可以告诉我哦~</span><br><span class="line">  bottom: 自动书记小新竭诚为您服务！ #信笺结束语，只能单行</span><br><span class="line">  height: #调整信笺划出高度，默认1050px</span><br></pre></td></tr></table></figure><p>（5）新建留言板页面（已经有的不用重复操作）<br>在根目录下打开终端，输入</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo new page comments</span><br></pre></td></tr></table></figure><p>（6）打开根目录下的source的comments的index.md，将其修改为</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: 留言板</span><br><span class="line">top_img:</span><br><span class="line">type: &#x27;envelope&#x27;</span><br><span class="line">---</span><br></pre></td></tr></table></figure><h1 id="音乐全局吸底APlayer"><a href="#音乐全局吸底APlayer" class="headerlink" title="音乐全局吸底APlayer"></a>音乐全局吸底APlayer</h1><p>（1）npm</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install aplayer  </span><br><span class="line">npm install --save hexo-tag-aplayer  </span><br></pre></td></tr></table></figure><p>（2）添加相关代码<br>打开 themes\Butterfly\layout\includes\head.pug,在结尾加一句include .&#x2F;third-party&#x2F;aplayer.pug,如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">include ./head/config_site.pug  </span><br><span class="line">include ./head/noscript.pug  </span><br><span class="line">include ./third-party/aplayer.pug  </span><br></pre></td></tr></table></figure><p>然后在 themes\Butterfly\layout\includes\third-party\ 里面新建一个文件叫 aplayer.pug ,内容如下</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if theme.aplayer &amp;&amp; theme.aplayer.enable</span><br><span class="line">.aplayer(data-id=theme.aplayer.id data-server=theme.aplayer.server data-type=theme.aplayer.type data-fixed=theme.aplayer.fixed data-mini=theme.aplayer.mini data-listFolded=theme.aplayer.listFolded data-order=theme.aplayer.order data-preload=theme.aplayer.preload)</span><br><span class="line">each item in theme.aplayer.css</span><br><span class="line">link(rel=&#x27;stylesheet&#x27;, href=item)</span><br><span class="line">each item in theme.aplayer.js</span><br><span class="line">script(src=item)</span><br></pre></td></tr></table></figure><p>然后打开_config.butterfly.yml,加入以下内容:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">aplayer:</span><br><span class="line">  enable: true</span><br><span class="line">  js:</span><br><span class="line">    - https://cdn.bootcss.com/aplayer/1.10.1/APlayer.min.js</span><br><span class="line">    - https://cdn.jsdelivr.net/npm/meting@1.2.0/dist/Meting.min.js</span><br><span class="line">  css:</span><br><span class="line">    - https://cdn.bootcss.com/aplayer/1.10.1/APlayer.min.css</span><br><span class="line">  id: 7506016963</span><br><span class="line">  server: netease </span><br><span class="line">  type: playlist</span><br><span class="line">  fixed: &#x27;true&#x27;</span><br><span class="line">  order: random</span><br><span class="line">  preload: none</span><br><span class="line">  listFolded: &#x27;false&#x27;</span><br></pre></td></tr></table></figure><p>其中，id为网易云歌单id,在网易云点击分享歌单-复制链接,粘贴后情况如下:<br><a href="https://y.music.163.com/m/playlist?app_version=8.8.01&amp;id=7506016963&amp;userid=3301427219&amp;dlt=0846&amp;creatorId=3301427219">https://y.music.163.com/m/playlist?app_version=8.8.01&amp;id=7506016963&amp;userid=3301427219&amp;dlt=0846&amp;creatorId=3301427219</a><br>其中歌单id为: 7506016963  </p><h1 id="2d看板娘"><a href="#2d看板娘" class="headerlink" title="2d看板娘"></a>2d看板娘</h1><p>1.安装hexo-helper-live2d</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install --save hexo-helper-live2d</span><br></pre></td></tr></table></figure><p>2.安装live2d<br>其中&lt; live2d-widget-model-koharu&gt;替换成想要的，还有很多的model可供选择，[参考]<a href="https://github.com/xiazeyu/live2d-widget-models">https://github.com/xiazeyu/live2d-widget-models</a></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install &lt;live2d-widget-model-koharu&gt;</span><br></pre></td></tr></table></figure><p>其中的live2d-widget-model-koharu按照自己喜欢的角色替换。<br>安装live2d-widget-model-koharu</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install live2d-widget-model-koharu</span><br></pre></td></tr></table></figure><p>3.配置<br>在Hexo站点配置文件_config.yml，或者主题配置文件_config.yml中添加如下配置<br>至于每个配置项的作用看名字就很清楚，也可以修改值然后部署看下效果<br>更多详细的说明和配置，详见官网<a href="https://github.com/EYHN/hexo-helper-live2d">EYHN&#x2F;hexo-helper-live2d</a></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Live2D</span><br><span class="line">## https://github.com/EYHN/hexo-helper-live2d</span><br><span class="line">live2d:</span><br><span class="line">  enable: true</span><br><span class="line">  # enable: false</span><br><span class="line">  scriptFrom: local # 默认</span><br><span class="line">  pluginRootPath: live2dw/ # 插件在站点上的根目录(相对路径)</span><br><span class="line">  pluginJsPath: lib/ # 脚本文件相对与插件根目录路径</span><br><span class="line">  pluginModelPath: assets/ # 模型文件相对与插件根目录路径</span><br><span class="line">  # scriptFrom: jsdelivr # jsdelivr CDN</span><br><span class="line">  # scriptFrom: unpkg # unpkg CDN</span><br><span class="line">  # scriptFrom: https://cdn.jsdelivr.net/npm/live2d-widget@3.x/lib/L2Dwidget.min.js # 你的自定义 url</span><br><span class="line">  tagMode: false # 标签模式, 是否仅替换 live2d tag标签而非插入到所有页面中</span><br><span class="line">  debug: false # 调试, 是否在控制台输出日志</span><br><span class="line">  model:</span><br><span class="line">    use: live2d-widget-model-koharu # npm-module package name</span><br><span class="line">    # use: wanko # 博客根目录/live2d_models/ 下的目录名</span><br><span class="line">    # use: ./wives/wanko # 相对于博客根目录的路径</span><br><span class="line">    # use: https://cdn.jsdelivr.net/npm/live2d-widget-model-wanko@1.0.5/assets/wanko.model.json # 你的自定义 url</span><br></pre></td></tr></table></figure><h1 id="Gitcalendar"><a href="#Gitcalendar" class="headerlink" title="Gitcalendar"></a>Gitcalendar</h1><p>在根目录下cmd，指令如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo-filter-gitcalendar --save</span><br></pre></td></tr></table></figure><p>添加配置信息，在_config.yml或_config.butterfly.yml中添加</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># hexo-filter-gitcalendar</span><br><span class="line"># see https://akilar.top/posts/1f9c68c9/</span><br><span class="line">gitcalendar:</span><br><span class="line">  enable: true # 开关</span><br><span class="line">  priority: 5 #过滤器优先权</span><br><span class="line">  enable_page: / # 应用页面</span><br><span class="line">  # butterfly挂载容器</span><br><span class="line">  layout: # 挂载容器类型</span><br><span class="line">    type: id</span><br><span class="line">    name: recent-posts</span><br><span class="line">    index: 0</span><br><span class="line">  # volantis挂载容器</span><br><span class="line">  # layout:</span><br><span class="line">  #   type: class</span><br><span class="line">  #   name: l_main</span><br><span class="line">  #   index: 0</span><br><span class="line">  # matery挂载容器</span><br><span class="line">  # layout:</span><br><span class="line">  #   type: id</span><br><span class="line">  #   name: indexCard</span><br><span class="line">  #   index: 0</span><br><span class="line">  # mengd挂载容器</span><br><span class="line">  # layout:</span><br><span class="line">  #   type: class</span><br><span class="line">  #   name: content</span><br><span class="line">  #   index: 0</span><br><span class="line">  user: Akilarlxh #git用户名</span><br><span class="line">  apiurl: &#x27;https://gitcalendar.akilar.top&#x27;</span><br><span class="line">  minheight:</span><br><span class="line">    pc: 280px #桌面端最小高度</span><br><span class="line">    mibile: 0px #移动端最小高度</span><br><span class="line">  color: &quot;[&#x27;#e4dfd7&#x27;, &#x27;#f9f4dc&#x27;, &#x27;#f7e8aa&#x27;, &#x27;#f7e8aa&#x27;, &#x27;#f8df72&#x27;, &#x27;#fcd217&#x27;, &#x27;#fcc515&#x27;, &#x27;#f28e16&#x27;, &#x27;#fb8b05&#x27;, &#x27;#d85916&#x27;, &#x27;#f43e06&#x27;]&quot; #橘黄色调</span><br><span class="line">  # color: &quot;[&#x27;#ebedf0&#x27;, &#x27;#fdcdec&#x27;, &#x27;#fc9bd9&#x27;, &#x27;#fa6ac5&#x27;, &#x27;#f838b2&#x27;, &#x27;#f5089f&#x27;, &#x27;#c4067e&#x27;, &#x27;#92055e&#x27;, &#x27;#540336&#x27;, &#x27;#48022f&#x27;, &#x27;#30021f&#x27;]&quot; #浅紫色调</span><br><span class="line">  # color: &quot;[&#x27;#ebedf0&#x27;, &#x27;#f0fff4&#x27;, &#x27;#dcffe4&#x27;, &#x27;#bef5cb&#x27;, &#x27;#85e89d&#x27;, &#x27;#34d058&#x27;, &#x27;#28a745&#x27;, &#x27;#22863a&#x27;, &#x27;#176f2c&#x27;, &#x27;#165c26&#x27;, &#x27;#144620&#x27;]&quot; #翠绿色调</span><br><span class="line">  # color: &quot;[&#x27;#ebedf0&#x27;, &#x27;#f1f8ff&#x27;, &#x27;#dbedff&#x27;, &#x27;#c8e1ff&#x27;, &#x27;#79b8ff&#x27;, &#x27;#2188ff&#x27;, &#x27;#0366d6&#x27;, &#x27;#005cc5&#x27;, &#x27;#044289&#x27;, &#x27;#032f62&#x27;, &#x27;#05264c&#x27;]&quot; #天青色调</span><br><span class="line">  container: .recent-post-item(style=&#x27;width:100%;height:auto;padding:10px;&#x27;) #父元素容器，需要使用pug语法</span><br><span class="line">  gitcalendar_css: https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.css</span><br><span class="line">  gitcalendar_js: https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.js</span><br></pre></td></tr></table></figure><p>之后要<a href="https://akilar.top/posts/1f9c68c9/">自建API</a>，具体的参照这位大佬的博文。  </p><h1 id="优化博客文章链接"><a href="#优化博客文章链接" class="headerlink" title="优化博客文章链接"></a>优化博客文章链接</h1><p>在Hexo的默认设定中，你的博客文章链接是由:year&#x2F;:month&#x2F;:day&#x2F;:title&#x2F;构成的，即按照年：月：日：标题的格式来生成链接，如果你的文章标题中还含有中文的话，复制URL链接就会有一大串编码字符，想分享博客文章链接会出现超级长的链接，影响美观。</p><p>解决办法：<br>1.首先在博客根目录运行Git Bash，输入以下指令安装hexo-abbrlink：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo-abbrlink --save</span><br></pre></td></tr></table></figure><p>2.打开站点配置文件_config.yml，修改permalink为：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">permalink: posts/:abbrlink.html</span><br></pre></td></tr></table></figure><p>3.在站点配置文件_config.yml中添加以下代码：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#abbrlink配置</span><br><span class="line">abbrlink:</span><br><span class="line">  alg: crc32  # 算法：crc16(default) and crc32</span><br><span class="line">  rep: dec    # 进制：dec(default) and hex</span><br></pre></td></tr></table></figure><p>其中alg和rep为算法和进制，具体区别见下表：</p><table><thead><tr><th>算法</th><th>进制</th><th>生成链接</th></tr></thead><tbody><tr><td>crc16</td><td>hex</td><td><a href="https://flowerxin.github.io/posts/66c8.html">https://FlowerXin.github.io/posts/66c8.html</a></td></tr><tr><td>crc16</td><td>dec</td><td><a href="https://flowerxin.github.io/posts/65535.html">https://FlowerXin.github.io/posts/65535.html</a></td></tr><tr><td>crc32</td><td>hex</td><td><a href="https://flowerxin.github.io/posts/8ddf18fb.html">https://FlowerXin.github.io/posts/8ddf18fb.html</a></td></tr><tr><td>crc32</td><td>dec</td><td><a href="https://flowerxin.github.io/posts/1690090958.html">https://FlowerXin.github.io/posts/1690090958.html</a></td></tr></tbody></table><p>修改文件后，执行以下代码部署到GitHub<br>更多<a href="https://github.com/rozbo/hexo-abbrlink">这里</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> -hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo个人博客美化教程（二）</title>
      <link href="/post/a271c6f8.html"/>
      <url>/post/a271c6f8.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>好的，老铁，在初步搭建完博客框架之后，我们要开始进行主题配置了（butterfly主题），内容比较多建议导航栏选择性阅读。<br><img src="https://img-blog.csdnimg.cn/2019062215545565.jpeg" alt="avatar">  </p><h1 id="一、安装Butterfly主题"><a href="#一、安装Butterfly主题" class="headerlink" title="一、安装Butterfly主题"></a>一、安装Butterfly主题</h1><p>1、在hexo项目根目录下执行操作clone主题</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone -b master https://github.com/jerryc127/hexo-theme-butterfly.git themes/butterfly</span><br></pre></td></tr></table></figure><p>2、如果沒有 pug 以及 stylus 的渲染器，还需要下载，否则在项目运行时会报错：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo-renderer-pug hexo-renderer-stylus --save</span><br></pre></td></tr></table></figure><p>3、修改项目根目录下的_config.yml文件（称为站点配置文件），开启主题</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Extensions</span><br><span class="line">## Plugins: https://hexo.io/plugins/</span><br><span class="line">## Themes: https://hexo.io/themes/</span><br><span class="line">#theme: landscape</span><br><span class="line">theme: butterfly</span><br></pre></td></tr></table></figure><p>4、升级建议<br>为了減少升级主题带来的不便，我们还需要做以下操作：<br>把主题文件夹中的 _config.yml 复制到 Hexo 根目录下，同重命名为 _config.butterfly.yml<br>Hexo会自动合并主题中的_config.yml和 _config.butterfly.yml里的配置，如果存在同名配置，会使用_config.butterfly.yml的配置，其优先度较高。  </p><h1 id="二、设置网站个人资料"><a href="#二、设置网站个人资料" class="headerlink" title="二、设置网站个人资料"></a>二、设置网站个人资料</h1><p>修改根目录下的站点配置文件_config.yml。修改网站各种资料，例如标题、副标题和邮箱等个人资料</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Hexo Configuration</span><br><span class="line">## Docs: https://hexo.io/docs/configuration.html</span><br><span class="line">## Source: https://github.com/hexojs/hexo/</span><br><span class="line"></span><br><span class="line"># Site</span><br><span class="line">title: 荒岛 #标题</span><br><span class="line">subtitle: &#x27;&#x27;#副标题</span><br><span class="line">description: 善始者众，善终者寡#个性签名</span><br><span class="line">keywords:</span><br><span class="line">author: FloweXin#作者</span><br><span class="line">language: zh-CN#语言</span><br><span class="line">timezone: Asia/Shanghai    #中国的时区</span><br></pre></td></tr></table></figure><p>主题支持三种语言：</p><pre><code>default(en)zh-CN (简体中文)zh-TW (繁体中文)</code></pre><h1 id="三、导航菜单"><a href="#三、导航菜单" class="headerlink" title="三、导航菜单"></a>三、导航菜单</h1><p>修改主题配置文件 _config.butterfly.yml,我需要的功能比较少，因此设置的比较简洁。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">menu:</span><br><span class="line">  主页: / || fas fa-home</span><br><span class="line">  分类: /categories/ || fa fa-archive</span><br><span class="line">  标签: /tags/ || fa fa-tags</span><br><span class="line">  归档: /archives/ || fa fa-folder-open</span><br><span class="line">  留言板: /comments/ || fa fa-paper-plane</span><br><span class="line">  #留言板: /messageboard/ || fa fa-paper-plane</span><br><span class="line">  关于笔者: /about/ || fas fa-heart  </span><br></pre></td></tr></table></figure><h1 id="四、代码块显示设置"><a href="#四、代码块显示设置" class="headerlink" title="四、代码块显示设置"></a>四、代码块显示设置</h1><p>1、highlight_copy<br>开启代码复制功能, 修改主题配置文件_config.butterfly.yml： </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">highlight_copy: true</span><br></pre></td></tr></table></figure><p>2、highlight_shrink<br> true 全部代码框不展开，需点击 &gt; 打开<br> false 代码狂展开，有 &gt; 点击按钮<br> none 不显示 &gt; 按钮<br> 修改主题配置文件_config.butterfly.yml：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">highlight_shrink: false #代码框展开</span><br></pre></td></tr></table></figure><p>3、code_word_wrap<br>在默认情况下，hexo-highlight 在编译的时候不会实现代码自动换行。如果你不希望在代码块的区域里有横向滚动条的话，可以_config.butterfly.yml开启代码换行：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">code_word_wrap: true</span><br></pre></td></tr></table></figure><p>4、我的_config.butterfly.yml：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Code Blocks (代码块样式)</span><br><span class="line"># --------------------------------------</span><br><span class="line">highlight_theme: mac #  darker / pale night / light / ocean / mac / mac light / false</span><br><span class="line">highlight_copy: true # copy button 是否显示复制按钮</span><br><span class="line">highlight_lang: true # show the code language 展示代码块语言</span><br><span class="line">highlight_shrink: false # true: 打开文章默认折叠代码块内容 / false: 打开文章默认展开代码块内容</span><br><span class="line">highlight_height_limit: false # unit: px</span><br><span class="line">code_word_wrap: true #是否关闭滚动条</span><br></pre></td></tr></table></figure><p>站点配置文件_config.yml:将line_number的值改为false</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">highlight:</span><br><span class="line">  enable: true</span><br><span class="line">  line_number: false </span><br><span class="line">  auto_detect: false</span><br><span class="line">  tab_replace: &#x27;&#x27;</span><br></pre></td></tr></table></figure><h1 id="五、本地搜索功能"><a href="#五、本地搜索功能" class="headerlink" title="五、本地搜索功能"></a>五、本地搜索功能</h1><p>1、安装插件</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo-generator-search --save</span><br></pre></td></tr></table></figure><p>2、主题配置文件 _config.butterfly.yml：  </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Local search</span><br><span class="line">local_search:</span><br><span class="line">  enable: true</span><br><span class="line">  labels:</span><br><span class="line">    input_placeholder: Search for Posts</span><br><span class="line">    hits_empty: &quot;We didn&#x27;t find any results for the search: $&#123;query&#125;&quot; # 如果没有查到内容相关内容显示</span><br></pre></td></tr></table></figure><h1 id="六、创建文件夹"><a href="#六、创建文件夹" class="headerlink" title="六、创建文件夹"></a>六、创建文件夹</h1><p>1、分类</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo new page categories</span><br></pre></td></tr></table></figure><p>会出现source&#x2F;categories&#x2F;index.md文件<br>2、标签<br>命令行输入：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo new page tags</span><br></pre></td></tr></table></figure><p>会出现source&#x2F;tags&#x2F;index.md文件：<br>…以此类推创建自己要的子页面</p><h1 id="七、修改副标题"><a href="#七、修改副标题" class="headerlink" title="七、修改副标题"></a>七、修改副标题</h1><p>1、修改主题配置文件 _config.butterfly.yml:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># the subtitle on homepage (主頁subtitle)</span><br><span class="line">subtitle:</span><br><span class="line">  enable: true</span><br><span class="line">  # Typewriter Effect (打字效果)</span><br><span class="line">  effect: true</span><br><span class="line">  # loop (循環打字)</span><br><span class="line">  loop: true</span><br><span class="line">  # source 調用第三方服務</span><br><span class="line">  # source: false 關閉調用</span><br><span class="line">  # source: 1  調用一言網的一句話（簡體） https://hitokoto.cn/</span><br><span class="line">  # source: 2  調用一句網（簡體） http://yijuzhan.com/</span><br><span class="line">  # source: 3  調用今日詩詞（簡體） https://www.jinrishici.com/</span><br><span class="line">  # subtitle 會先顯示 source , 再顯示 sub 的內容</span><br><span class="line">  source: false</span><br><span class="line">  # 如果關閉打字效果，subtitle 只會顯示 sub 的第一行文字</span><br><span class="line">  sub: </span><br><span class="line">    - 宝剑锋从磨砺出，梅花香自苦寒来</span><br><span class="line">    - 凡是过去，皆为序章</span><br><span class="line">    - 我走得很慢，但我从不后退</span><br><span class="line">    - 浅水是喧哗的，深水是沉默的</span><br><span class="line">    - 浮名浮利，虚苦劳神。叹隙中驹，石中火，梦中身</span><br><span class="line">    - 黄昏把酒祝东风，且从容</span><br><span class="line">    - 于浩歌狂热之际中寒，于天上看见深渊，于一切眼中看见无所有，于无所希望中得救</span><br><span class="line">    - 我曾踏月而来，只因你在山中</span><br><span class="line">    - 独立天地间，清风洒兰雪</span><br><span class="line">    - 当我跨过沉沦的一切，向着永恒开战的时候，你是我的军旗</span><br><span class="line">    - 你今天很可爱</span><br></pre></td></tr></table></figure><p>2、副标题字体大小颜色<br>在\themes\butterfly\source\css_layout中的head.styl:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#site-subtitle</span><br><span class="line">    color: var(--white)   //此处修改为白色</span><br><span class="line">    font-size: 1.05em     // 字体大小</span><br><span class="line">    +minWidth768()</span><br><span class="line">    font-size: 1.40em   // 字体大小</span><br></pre></td></tr></table></figure><h1 id="八、图片设置"><a href="#八、图片设置" class="headerlink" title="八、图片设置"></a>八、图片设置</h1><p>图片可以用云图片链接或者放在本地文件夹位置：&#x2F;themes&#x2F;butterfly&#x2F;source&#x2F;img。引用图片可以自己建立图床或者本地引用，两者都比较麻烦，我想到了一个偷懒的方法就是，将我想引用的图片放在我的csdn文章里，然后再用链接引用到我的博客里。要注意的是csdn有防盗链，我们需要解决一下这个问题，很简单，在此不叙述怎么做了，自己百度一下。  </p><p>有时候想想，服务器是白嫖Github的，图片链接白嫖csdn的，只要他俩不倒闭，我的博客就能一直白嫖下去。。。白嫖怪就是我，我就是白嫖怪（骄傲）<br><img src="https://img-blog.csdnimg.cn/3f56d9e3b1d84283859250816ea6ffc5.png" alt="avatar"></p><p>在此注明：正常的话图片是在public的img中的，因为我后面魔改的时候出现了bug，所以将一部分照片放在了assert文件的img中。先写上，怕之后我忘记了。。。<br>1、网站图标</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Favicon（网站图标）</span><br><span class="line">favicon: /img/favicon.png</span><br></pre></td></tr></table></figure><p>2、头像</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">avatar:</span><br><span class="line">  img: /img/avatar.jpg #图片路径</span><br><span class="line">  effect: false #头像会一直转圈  </span><br></pre></td></tr></table></figure><p>3、主页封面图片</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># The banner image of home page</span><br><span class="line">index_img: /img/background.jpg</span><br></pre></td></tr></table></figure><p>4、文章详情页的顶部图片<br>当没有在front-matter设置top_img和cover的情况下会显示该图修改主题配置文件_config.butterfly.yml：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># If the banner of page not setting, it will show the top_img</span><br><span class="line">default_top_img: /img/default_top_img.jpg #我设置的本地图片</span><br></pre></td></tr></table></figure><p>5、归档页顶部图片</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#归档子标签页图片</span><br><span class="line">#The banner image of archive page</span><br><span class="line">archive_img: /img/archive.jpg</span><br></pre></td></tr></table></figure><p>6、tag页顶部图</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#tag页（标签页）</span><br><span class="line"># If the banner of tag page not setting, it will show the top_img</span><br><span class="line"># note: tag page, not tags page (子標籤頁面的 top_img)</span><br><span class="line">tag_img: /img/tag_img.jpg</span><br></pre></td></tr></table></figure><p>7、category页顶部图</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#category页</span><br><span class="line"># If the banner of category page not setting, it will show the top_img</span><br><span class="line"># note: category page, not categories page (子分類頁面的 top_img)</span><br><span class="line">category_img: /img/category_img.jpg</span><br></pre></td></tr></table></figure><p>8、文章封面<br>每篇文章都可以设置一张封面，可以为每篇文章设置默认封面.也可以修改每个md文件的front-matter的cover属性，会覆盖上面的默认封面。修改主题配置文件_config.butterfly.yml</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cover:</span><br><span class="line">  index_enable: true #  是否展示文章封面</span><br><span class="line">  aside_enable: true</span><br><span class="line">  archives_enable: true</span><br><span class="line">  position: both # 封面展示的位置 left/right/both</span><br><span class="line">  # 当没有设置cover时，默认展示的文章封面</span><br><span class="line">  default_cover:</span><br><span class="line">    # 当配置多张图片时，会随机选择一张作为 cover. 此时写法为</span><br><span class="line">    - https:</span><br><span class="line">    - http:</span><br><span class="line">    - http:</span><br><span class="line">    - http:</span><br><span class="line">    - http:</span><br><span class="line">    - http:</span><br></pre></td></tr></table></figure><p>9、错误页面<br>配置了该属性后会替换无法展示的图片</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Replace Broken Images (替換無法顯示的圖片)</span><br><span class="line">error_img:</span><br><span class="line">  flink: /img/friend_404.gif</span><br><span class="line">  post_page: /img/404.jpg</span><br></pre></td></tr></table></figure><h1 id="九、图片大图查看"><a href="#九、图片大图查看" class="headerlink" title="九、图片大图查看"></a>九、图片大图查看</h1><p>修改主题配置文件_config.butterfly.yml</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Lightbox (圖片大圖查看模式)</span><br><span class="line"># --------------------------------------</span><br><span class="line"># You can only choose one, or neither (只能選擇一個 或者 兩個都不選)</span><br><span class="line"></span><br><span class="line"># medium-zoom</span><br><span class="line"># https://github.com/francoischalifour/medium-zoom</span><br><span class="line">medium_zoom: false</span><br><span class="line"></span><br><span class="line"># fancybox</span><br><span class="line"># http://fancyapps.com/fancybox/3/</span><br><span class="line">fancybox: true</span><br></pre></td></tr></table></figure><h1 id="十、文章页样式"><a href="#十、文章页样式" class="headerlink" title="十、文章页样式"></a>十、文章页样式</h1><p>以下修改主题配置文件_config.butterfly.yml<br>1、复制的内容后面加上版权信息</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># copy settings</span><br><span class="line"># copyright: Add the copyright information after copied content (複製的內容後面加上版權信息)</span><br><span class="line">copy:</span><br><span class="line">  enable: true # 是否开启网站复制权限</span><br><span class="line">  copyright:  # 复制的内容后面加上版权信息</span><br><span class="line">    enable: false  # 是否开启复制版权信息添加</span><br><span class="line">    limit_count: 50 # 字数限制，当复制文字大于这个字数限制时</span><br></pre></td></tr></table></figure><p>2、文章版权信息<br>在底部会出现对应的作者、链接、声明</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">post_copyright:</span><br><span class="line">  enable: true</span><br><span class="line">  decode: true</span><br><span class="line">  license: CC BY-NC-SA 4.0</span><br><span class="line">  license_url: https://creativecommons.org/licenses/by-nc-sa/4.0/</span><br></pre></td></tr></table></figure><p>3、相关文章<br>在文章最下面出现推送</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Related Articles</span><br><span class="line">related_post:</span><br><span class="line">  enable: true</span><br><span class="line">  limit: 6 # Number of posts displayed</span><br><span class="line">  date_type: created # or created or updated 文章日期顯示創建日或者更新日</span><br></pre></td></tr></table></figure><p>4、打赏<br>给文章结尾设置打赏按钮，可以放上收款二维码</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Sponsor/reward</span><br><span class="line">reward:</span><br><span class="line">  enable: true</span><br><span class="line">  QR_code:</span><br><span class="line">     - img: /img/wechat.jpg</span><br><span class="line">       link:</span><br><span class="line">       text: 微信</span><br><span class="line">     - img: /img/alipay.jpg</span><br><span class="line">       link:</span><br><span class="line">       text: 支付宝</span><br></pre></td></tr></table></figure><h1 id="十一、侧边栏样式"><a href="#十一、侧边栏样式" class="headerlink" title="十一、侧边栏样式"></a>十一、侧边栏样式</h1><p>以下修改主题配置文件_config.butterfly.yml<br>1、调整侧边栏出现位置</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">aside:</span><br><span class="line">  enable: true</span><br><span class="line">  hide: false</span><br><span class="line">  button: true</span><br><span class="line">  mobile: true # display on mobile</span><br><span class="line">  position: right # left or right 我的是右边</span><br></pre></td></tr></table></figure><p>2、个人信息</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">card_author:</span><br><span class="line">    enable: true</span><br><span class="line">    description:</span><br><span class="line">    button:</span><br><span class="line">      enable: true</span><br><span class="line">      icon: fab fa-github</span><br><span class="line">      text: Follow Me #按钮文字</span><br><span class="line">      link: https://github.com/xxxxxx #可以放上自己的github地址</span><br></pre></td></tr></table></figure><p>3、公告栏设置</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">...</span><br><span class="line">card_announcement:</span><br><span class="line">    enable: true</span><br><span class="line">    content: This is my Blog #修改公告栏信息</span><br></pre></td></tr></table></figure><p>4、Toc目录</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># toc (目錄)</span><br><span class="line">toc:</span><br><span class="line">  enable: true</span><br><span class="line">  number: true</span><br><span class="line">  #style_simple: false</span><br><span class="line">  auto_open: true # auto open the sidebar</span><br></pre></td></tr></table></figure><h1 id="十二、特效-x2F-美化"><a href="#十二、特效-x2F-美化" class="headerlink" title="十二、特效&#x2F;美化"></a>十二、特效&#x2F;美化</h1><p>以下均为修改主题配置文件 _config.butterfly.yml:<br>1、鼠标点击的效果<br>有冒光特效、烟火特效、爱心特效、文字特效，选择其中一个将enable设置为true就可以</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Typewriter Effect (打字效果)</span><br><span class="line"># https://github.com/disjukr/activate-power-mode</span><br><span class="line">activate_power_mode:</span><br><span class="line">  enable: false</span><br><span class="line">  colorful: true # open particle animation (冒光特效)</span><br><span class="line">  shake: true #  open shake (抖動特效)</span><br><span class="line">  mobile: false</span><br><span class="line"></span><br><span class="line"># Mouse click effects: fireworks (鼠標點擊效果: 煙火特效)</span><br><span class="line">fireworks:</span><br><span class="line">  enable: false</span><br><span class="line">  zIndex: 9999 # -1 or 9999</span><br><span class="line">  mobile: false</span><br><span class="line"></span><br><span class="line"># Mouse click effects: Heart symbol (鼠標點擊效果: 愛心)</span><br><span class="line">click_heart:</span><br><span class="line">  enable: false</span><br><span class="line">  mobile: false</span><br><span class="line"></span><br><span class="line"># Mouse click effects: words (鼠標點擊效果: 文字)</span><br><span class="line">ClickShowText:</span><br><span class="line">  enable: true</span><br><span class="line">  text:</span><br><span class="line">     - I</span><br><span class="line">     - LOVE</span><br><span class="line">     - YOU</span><br><span class="line">  fontSize: 15px</span><br><span class="line">  random: true</span><br><span class="line">  mobile: true</span><br></pre></td></tr></table></figure><p>2、打字效果</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Typewriter Effect (打字效果)</span><br><span class="line"># https://github.com/disjukr/activate-power-mode</span><br><span class="line">activate_power_mode:</span><br><span class="line">  enable: true</span><br><span class="line">  colorful: true # open particle animation (冒光特效)</span><br><span class="line">  shake: true #  open shake (抖動特效)</span><br><span class="line">  mobile: true</span><br></pre></td></tr></table></figure><p>3、自定义主题色<br>可以修改部分的UI颜色</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 我的没改</span><br><span class="line"></span><br><span class="line"># theme_color:</span><br><span class="line">#   enable: true</span><br><span class="line">#   main: &quot;#49B1F5&quot;</span><br><span class="line">#   paginator: &quot;#00c4b6&quot;</span><br><span class="line">#   button_hover: &quot;#FF7242&quot;</span><br><span class="line">#   text_selection: &quot;#00c4b6&quot;</span><br><span class="line">#   link_color: &quot;#99a9bf&quot;</span><br><span class="line">#   meta_color: &quot;#858585&quot;</span><br><span class="line">#   hr_color: &quot;#A4D8FA&quot;</span><br><span class="line">#   code_foreground: &quot;#F47466&quot;</span><br><span class="line">#   code_background: &quot;rgba(27, 31, 35, .05)&quot;</span><br><span class="line">#   toc_color: &quot;#00c4b6&quot;</span><br><span class="line">#   blockquote_padding_color: &quot;#49b1f5&quot;</span><br><span class="line">#   blockquote_background_color: &quot;#49b1f5&quot;</span><br></pre></td></tr></table></figure><p>4、网站背景<br>默认显示白色，可设置图片或者颜色<br>修改主题配置文件_config.butterfly.yml：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">background:</span><br></pre></td></tr></table></figure><p>5、footer 背景<br>footer 的背景，当设置 false 时，将与主题色一致。<br>修改主题配置文件_config.butterfly.yml：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># footer是否显示图片背景(与top_img一致)</span><br><span class="line">footer_bg: true</span><br></pre></td></tr></table></figure><p>6、背景特效<br>有三种，自己选择开启，将enable设置为true就可以<br>修改主题配置文件_config.butterfly.yml：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Background effects (背景特效)</span><br><span class="line"># --------------------------------------</span><br><span class="line"></span><br><span class="line"># canvas_ribbon (静止彩带)</span><br><span class="line"># See: https://github.com/hustcc/ribbon.js</span><br><span class="line">canvas_ribbon:</span><br><span class="line">  enable: false</span><br><span class="line">  size: 150</span><br><span class="line">  alpha: 0.6</span><br><span class="line">  zIndex: -1</span><br><span class="line">  click_to_change: false</span><br><span class="line">  mobile: false</span><br><span class="line"></span><br><span class="line"># Fluttering Ribbon (动态彩带)</span><br><span class="line">canvas_fluttering_ribbon:</span><br><span class="line">  enable: false</span><br><span class="line">  mobile: false</span><br><span class="line"></span><br><span class="line">#星空特效</span><br><span class="line"># canvas_nest</span><br><span class="line"># https://github.com/hustcc/canvas-nest.js</span><br><span class="line">canvas_nest:</span><br><span class="line">  enable: true</span><br><span class="line">  color: &#x27;0,0,255&#x27; #color of lines, default: &#x27;0,0,0&#x27;; RGB values: (R,G,B).(note: use &#x27;,&#x27; to separate.)</span><br><span class="line">  opacity: 0.7 # the opacity of line (0~1), default: 0.5.</span><br><span class="line">  zIndex: -1 # z-index property of the background, default: -1.</span><br><span class="line">  count: 99 # the number of lines, default: 99.</span><br><span class="line">  mobile: false</span><br></pre></td></tr></table></figure><h1 id="十四、字数统计"><a href="#十四、字数统计" class="headerlink" title="十四、字数统计"></a>十四、字数统计</h1><p>1、项目目录右键打开 Git Bash Here,<br>2、npm install hexo-wordcount –save or yarn add hexo-wordcount<br>3、修改主题配置文件_config.butterfly.yml</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># wordcount (字數統計)</span><br><span class="line">wordcount:</span><br><span class="line">  enable: true</span><br><span class="line">  post_wordcount: true</span><br><span class="line">  min2read: true</span><br><span class="line">  total_wordcount: true</span><br></pre></td></tr></table></figure><h1 id="十五、文章分享功能"><a href="#十五、文章分享功能" class="headerlink" title="十五、文章分享功能"></a>十五、文章分享功能</h1><p>addThis、sharejs、addtoany三个选一个开启<br>addThis官网：<a href="https://www.addthis.com/">https://www.addthis.com/</a></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Share System (分享功能)</span><br><span class="line"># --------------------------------------</span><br><span class="line"></span><br><span class="line"># AddThis</span><br><span class="line"># https://www.addthis.com/</span><br><span class="line">addThis:</span><br><span class="line">  enable: false</span><br><span class="line">  pubid:  #访问 AddThis 官网, 找到你的 pub-id</span><br><span class="line"></span><br><span class="line"># Share.js</span><br><span class="line"># https://github.com/overtrue/share.js</span><br><span class="line">sharejs:</span><br><span class="line">  enable: true  #我开启的</span><br><span class="line">  sites: facebook,twitter,wechat,weibo,qq  #想要显示的内容</span><br><span class="line"></span><br><span class="line"># AddToAny</span><br><span class="line"># https://www.addtoany.com/</span><br><span class="line">addtoany:</span><br><span class="line">  enable: false</span><br><span class="line">  item: facebook,twitter,wechat,sina_weibo,facebook_messenger,email,copy_link</span><br></pre></td></tr></table></figure><p>至此，简单的美化配置就已经完成啦，详细的可以看官网作者<a href="https://butterfly.js.org/posts/21cfbf15/%E7%9A%84%E6%96%87%E7%AB%A0.%E5%A6%82%E6%9E%9C%E6%83%B3%E8%BF%9B%E9%98%B6%E6%80%A7%E7%BE%8E%E5%8C%96%E7%9A%84%E8%AF%9D%E5%B0%B1%E7%BB%A7%E7%BB%AD%E8%AF%BB%E4%B8%8B%E5%8E%BB%EF%BC%88%E4%B8%89%EF%BC%89%E7%B3%BB%E5%88%97%E5%90%A7">https://butterfly.js.org/posts/21cfbf15/的文章.如果想进阶性美化的话就继续读下去（三）系列吧</a><br><img src="https://img-blog.csdnimg.cn/20190617135913421.jpeg" alt="avatar"></p>]]></content>
      
      
      
        <tags>
            
            <tag> -hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo个人博客搭建教程（一）</title>
      <link href="/post/10b7cf25.html"/>
      <url>/post/10b7cf25.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>本系列将记录自己搭建个人博客的全过程，将过程与自己搭建时候的心得记录下来，为以后反看自己成长留下的痕迹，也希望如果可以的话，帮助到其他想要尝试搭建自己博客的朋友。<br><img src="https://img-blog.csdnimg.cn/dc5e263948ce4e7a8a3a6e4f9e4328e6.jpeg" alt="avatar"></p><p>本文基于Windows10系统，且默认已有Github的账号以及下载安装好git。  </p><h1 id="1-绑定Github"><a href="#1-绑定Github" class="headerlink" title="1. 绑定Github"></a>1. 绑定Github</h1><h2 id="1-1-绑定Github"><a href="#1-1-绑定Github" class="headerlink" title="1.1 绑定Github"></a>1.1 绑定Github</h2><p>利用git上传文件到Github首先得利用SSH登录远程主机，有两种登陆方式：口令登录和公匙登录。口令登录每次登陆时需要密码，较为麻烦，公匙登录省去了每次输入密码的步骤，更为方便些。所以我选择了公匙登陆的方式。</p><p>首先，我们需要在Github上添加SSH key配置，安装好了Git Bash之后会自带SSH，可以通过在Git Bash中输入SSH命令来检查一下本机是否安装<br><img src="https://img-blog.csdnimg.cn/d3028d0bb1204a81b943ba9afb0ad0b8.png" alt="avatar"><br>即已经成功安装了SSH。</p><p>接着输入命令ssh-keygen -t rsa（注意空格），表示指定RSA算法生成秘匙id_rsa和公匙id_rsa.pub。表示指定RSA算法生成秘匙，然后按四次回车键，将会生成秘匙id_rsa和公匙id_rsa.pub。根据Git Bash上显示的目录找到所述文件。（注意：git中的复制黏贴不是Ctrl+C和Ctrl+V,而是Ctrl+insert和Shift+insert.）</p><p>或者觉得代码会看花眼，可以选择进入我的电脑&gt;C盘&gt;用户&gt;“你的电脑名字”&gt;.ssh中找到id_rsa.pub文件，打开复制文件内的所有代码，即为你的公匙。</p><p>也可以在Git Bash中输入如下指令。  </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ cd ~/.ssh </span><br><span class="line">$ ls</span><br><span class="line">$ cat id_rsa.pub</span><br></pre></td></tr></table></figure><p>接下来，我们要把公匙里的内容添加到Github。复制公匙里的内容，进到自己的Github主页，点击右上角的头像，找到setting点击。<br><img src="https://img-blog.csdnimg.cn/726142cf8acd428d9db032e2254d3a71.png" alt="avatar"><br>如下图，先点击 SSH and GPG keys，再点击 <em><strong>New SSH key。</strong></em><br><img src="https://img-blog.csdnimg.cn/9ac9cb6165f645f884e353d847505843.png" alt="avatar"><br>将刚才复制的公钥填写到下面 Key 的大框里，上面是标题，可填可不填，最后点击下面的 Add SSH key 。<br><img src="https://img-blog.csdnimg.cn/fd97e25028c9411a88772cfa4962820d.png" alt="avatar"><br>然后我们可以通过Git Bash 中输入 ssh -T <a href="mailto:&#x67;&#105;&#116;&#x40;&#103;&#105;&#116;&#104;&#117;&#98;&#46;&#99;&#x6f;&#109;">&#x67;&#105;&#116;&#x40;&#103;&#105;&#116;&#104;&#117;&#98;&#46;&#99;&#x6f;&#109;</a> 进行检验是否验证成功，第一次会询问，填写 yes，回车就好。<br><img src="https://img-blog.csdnimg.cn/a3399096f66c43afa3dbba9f9bd8dac2.png" alt="avatar">  </p><h2 id="1-2-提交文件"><a href="#1-2-提交文件" class="headerlink" title="1.2 提交文件"></a>1.2 提交文件</h2><p>提交文件有两种方法：<br>①本地没有 git 仓库</p><pre><code>    直接将远程仓库 clone 到本地；    将文件添加并 commit 到本地仓库；    将本地仓库的内容push到远程仓库。</code></pre><p>② 本地有 Git 仓库，并且已经进行了多次 commit 操作</p><pre><code>    建立一个本地仓库进入，init 初始化；    关联远程仓库；    同步远程仓库和本地仓库；    将文件添加提交到本地仓库；    将本地仓库的内容 push 到远程仓库。</code></pre><p>下面展开详细讲解一下两种方法如何使用：<br>① 本地没有 git 仓库：<br>首先进入自己的 GitHub 主页，创建一个新项目，我这里将新项目命名为《First-Demo》，点击进入。点击 Code，再点击SSH，点击网址后面的复制图标将路径复制。<br><img src="https://img-blog.csdnimg.cn/094a5b2c10b844c6a1cf52030217691f.png" alt="avatar"><br>然后打开 Git Bash 进入到自己准备存储 Git 仓库的目录<br><img src="https://img-blog.csdnimg.cn/8981bb8afdf9418386e4e3ae727ac371.png" alt="avatar"><br>这样就进入到仓库所在目录，然后运行命令git clone <a href="mailto:&#x67;&#105;&#x74;&#64;&#x67;&#x69;&#x74;&#104;&#x75;&#98;&#46;&#x63;&#111;&#109;">&#x67;&#105;&#x74;&#64;&#x67;&#x69;&#x74;&#104;&#x75;&#98;&#46;&#x63;&#111;&#109;</a>:7948…（把刚刚复制的地址粘贴过来），将远程仓库 clone 到本地。上图就证明 clone 成功了，我们可以打开仓库所在的目录，检验一下是否 clone 正确，可以看到本地内容与 GitHub 上的内容完全一致。</p><p>接下来我们在本地的仓库新建一个文件，然后从此目录进入 Git Bash，输入 git status 命令查看仓库状态：<br><img src="https://img-blog.csdnimg.cn/9a3155270fac4a44a03286f2bab4a34f.png" alt="avatar"><br>可以看到我们刚刚新建的文件并没有被追踪，现在用 git add 命令将文件添加到「临时缓冲区」，再用 git commit -m “提交信息” 将其提交到本地仓库，然后就可以输入git push origin master 命令，将本地仓库提交到远程仓库，origin是远程主机的名字,如下图：<br><img src="https://img-blog.csdnimg.cn/cc3a20d26cd54db1a420d098291a2054.png" alt="avatar"><br><img src="https://img-blog.csdnimg.cn/d5a8e80f61554163bfa23c976413eca0.png" alt="avatar"><br>此时再看 GitHub主页上仓库里已有我们更新提交的文件了<br>② 本地有 Git 仓库，并且已经进行了多次 commit 操作<br>首先，我们建立一个本地仓库 secondDemo，使用 git init 命令初始化这个仓库<br>输入 git remote add origin <a href="mailto:&#103;&#x69;&#116;&#x40;&#103;&#105;&#x74;&#104;&#x75;&#x62;&#x2e;&#x63;&#111;&#x6d;">&#103;&#x69;&#116;&#x40;&#103;&#105;&#x74;&#104;&#x75;&#x62;&#x2e;&#x63;&#111;&#x6d;</a>:7948…命令，关联远程仓库，接着输入 git pull origin master 命令，同步远程仓库和本地仓库git。然后我们打开本地仓库就可以看到，和远程仓库中的内容一致了，接下来的操作与第①种方法大同小异，add、push、commit等操作。</p><h1 id="2-购买域名"><a href="#2-购买域名" class="headerlink" title="2. 购买域名"></a>2. 购买域名</h1><p>作为一个程序员搭建自己的个人博客网站，大概率会想要一个专属于自己的域名。可以在阿里云的万网购买，首年还有优惠。建议在第三步进行，因为备案需要一段时间。首先先查询一下自己心仪的域名能否使用，接下来在阿里云注册账号，在域名备案成功之后进行解析域名，就可以拥有一个自己喜欢的域名啦。因为博客对于我来说只是起到一个记录作用，域名对我来说比较无所谓，内容才是王道，所以我就没进行这部。感兴趣的同学自行研究。  </p><h1 id="3-安装node-js"><a href="#3-安装node-js" class="headerlink" title="3. 安装node.js"></a>3. 安装node.js</h1><h2 id="3-1-安装node-js"><a href="#3-1-安装node-js" class="headerlink" title="3.1 安装node.js"></a>3.1 安装node.js</h2><p>先下载 nvm (node.js version management)，顾名思义是一个 nodejs 的版本管理工具。通过它可以安装和切换不同版本的 nodejs。方便之后的操作，可以<a href="https://github.com/coreybutler/nvm-windows/releases">点击此处</a>下载 nvm，下载Windows 版本</p><pre><code>nvm-noinstall.zip：绿色免安装版，但使用时需进行配置。nvm-setup.zip：安装版，推荐使用  </code></pre><p><img src="https://img-blog.csdnimg.cn/447edaa6cd3243bc9385195e69e06b4a.png" alt="avatar"><br>点击nvm-setup-zip<br>下载好之后一路 next 安装完成，打开 CMD，输入命令 nvm -v，若出现以下界面则证明安装成功：<br><img src="https://img-blog.csdnimg.cn/0a4c4f1445fb4d3b8c146f69b936530b.png" alt="avatar"><br>接下来输入命令 nvm install 14.17.5 即可安装14.17.5版本的 node.js 和 npm。之后输入命令 nvm use 14.17.5，即可使用该版本的 node 与 npm。<br>注：<br>1.下载到非c盘内的时候一定要要修改全局变量，位置在我的电脑&gt;属性&gt;环境变量设置&gt;path里加上路径。<br>2.一定要下载nvm，不然的话npm与node会出现版本不适配的情况，会报错。<br>3.node尽量不要下载最新版，作者不维护了，新版本很容易出问题，建议用稳定版本。</p><h2 id="3-2-常用nvm命令"><a href="#3-2-常用nvm命令" class="headerlink" title="3.2 常用nvm命令"></a>3.2 常用nvm命令</h2><p>以下附上常用的 nvm 命令：</p><pre><code>nvm arch ：显示node是运行在32位还是64位。nvm install &lt;version&gt; [arch] ：安装node， version是特定版本也可以是最新稳定版本latest。可选参数arch指定安装32位还是64位版本，默认是系统位数。可以添加–insecure绕过远程服务器的SSL。nvm list [available] ：显示已安装的列表。可选参数available，显示可安装的所有版本。list可简化为ls。nvm on ：开启node.js版本管理。nvm off ：关闭node.js版本管理。nvm proxy [url] ：设置下载代理。不加可选参数url，显示当前代理。将url设置为none则移除代理。nvm node_mirror [url] ：设置node镜像。默认是https://nodejs.org/dist/。如果不写url，则使用默认url。设置后可至安装目录settings.txt文件查看，也可直接在该文件操作。nvm npm_mirror [url]：设置npm镜像。https://github.com/npm/cli/archive/。如果不写url，则使用默认url。设置后可至安装目录settings.txt文件查看，也可直接在该文件操作。nvm uninstall &lt;version&gt;：卸载指定版本node。nvm use [version] [arch]：使用制定版本node。可指定32/64位。nvm root [path]：设置存储不同版本node的目录。如果未设置，默认使用当前目录。nvm version ：显示nvm版本。version可简化为v。  </code></pre><h1 id="4-安装Hexo"><a href="#4-安装Hexo" class="headerlink" title="4. 安装Hexo"></a>4. 安装Hexo</h1><p>Hexo就是我个人博客网站的框架，在安装前，先在 GitHub 新建一个仓库。仓库名称设置为“用户名+github.io”.<br><img src="https://img-blog.csdnimg.cn/daf4ce3c75d2435b86dfb4423172507c.png" alt="avatar"><br>然后就是安装 Hexo 了，首先在D盘新建文件夹“Blog”，打开命令行进入“D:\Blog” ，然后输入命令安装 Hexo：  </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/e976e326a82a4e9c9a7ca62ddd6b9217.png" alt="avatar"><br>安装完成后输入 hexo init 命令初始化博客：<br><img src="https://img-blog.csdnimg.cn/92c2823a2b6c420c84fb217350132045.png" alt="avatar"><br>然后输入 hexo g 静态部署：<br><img src="https://img-blog.csdnimg.cn/318292b5b2d843d48c6c532aaa6f8795.png" alt="avatar"><br>这时网页已经部署完成，输入 hexo s 命令可以查看：<br><img src="https://img-blog.csdnimg.cn/2119a91e54b14d08a169352e7032fd7a.png" alt="avatar"><br>在浏览器输入 <a href="http://localhost:4000/">http://localhost:4000</a> 就可以打开新部署的网页啦：<br><img src="https://img-blog.csdnimg.cn/59b7422c8da24ac5b91864315ba9eb73.png" alt="avatar"><br>最后记得要ctrl + c 停止运行服务器。  </p><h1 id="5-将-Hexo-部署到-GitHub"><a href="#5-将-Hexo-部署到-GitHub" class="headerlink" title="5. 将 Hexo 部署到 GitHub"></a>5. 将 Hexo 部署到 GitHub</h1><p>在根目录中找到_config.yml文件，用vscode打开（我用的是vsccode，也可以换成别的，yml文件首行缩进很严格，不推荐记事本打开），下滑到最底部，添加如下内容：  </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: https://github.com/FlowerXin/FlowerXin.github.io.git  #你的仓库地址</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure><p>仓库地址在这里：<br><img src="https://img-blog.csdnimg.cn/9b6ce284823d412994da9dacd9afedef.png" alt="avatar"><br>然后还是在命令行中进入D:&#x2F;Blog，安装Git部署插件，输入命令：  </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p>然后分别输入以下三条命令：  </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo clean   #清除缓存文件 db.json 和已生成的静态文件 public</span><br><span class="line">hexo g       #生成网站静态文件到默认设置的 public 文件夹(hexo generate 的缩写)</span><br><span class="line">hexo d       #自动生成网站静态文件，并部署到设定的仓库(hexo deploy 的缩写)</span><br></pre></td></tr></table></figure><p>完成以后，打开浏览器，输入 <a href="https://xxx.github.io/">https://xxx.github.io</a> 就可以打开你的网页了。至此，博客初步框架形成。  </p><h1 id="6-遇见的坑总结"><a href="#6-遇见的坑总结" class="headerlink" title="6. 遇见的坑总结"></a>6. 遇见的坑总结</h1><p>因为博客是一个月前搭建的，中间有事一直耽搁着没做。再记录的时候当时有很多的问题已经记不清了（哭唧唧）。下面将主要记录几个我印象深刻的坑。。。相信一些小问题大家这么聪明一定一看报错就能找到。<br> <img src="https://img-blog.csdnimg.cn/2019061713511395.jpeg" alt="avatar"></p><h2 id="6-1-npm遇见的问题"><a href="#6-1-npm遇见的问题" class="headerlink" title="6.1 npm遇见的问题"></a>6.1 npm遇见的问题</h2><p>1、用npm安装xx插件后，在使用时出现：<br>  错误1：无法加载文件 C:\Program Files\nodejs\npm.ps1，因为在此系统上禁止运行脚本，有关详细信息请参阅。。。<br>  错误2、xxx在使用时出现：无法将“xxx”项识别为 cmdlet、函数、脚本文件或可运行程序的名称。请检查名称的拼写，如果包括路径，请确保路径正确，然后再试一次。<br>2、问题原因：<br>（1）执行策略权限不足（执行不信任的脚本），执行策略默认是 Restricted<br>（2）没有配置全局npm的环境变量<br>3、解决办法<br>（1）执行策略权限<br>（2）用快捷键：Win + X，选择 Windows Power Shell(管理员)(A) ，以管理员身份打开Power Shell。<br>（3）在打开的打开Power Shell窗口中输入命令：set-executionpolicy remotesigned<br>（4）然后输入 Y，按回车键，问题解决。<br>总结：更改全局路径一般就能解决问题。这个Windows Power Shell界面我用着不习惯，不是长久之计（笑哭）</p><h2 id="6-2-报错bash-hexo-command-not-found"><a href="#6-2-报错bash-hexo-command-not-found" class="headerlink" title="6.2  报错bash: hexo: command not found"></a>6.2  报错bash: hexo: command not found</h2><p>1.问题：<br>安装hexo的时候一直报错bash: hexo: command not found，官网的安装命令如下：  </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1.npm install hexo-cli -g</span><br><span class="line">2.hexo init blog</span><br><span class="line">3.cd blog</span><br><span class="line">4.npm install</span><br><span class="line">5.hexo server</span><br></pre></td></tr></table></figure><p>2.解决办法：<br>总结网上解决这个错误的方法有以下几种：</p><p>（1）node的npm环境没有配好，重装node，默认路径安装解决<br>（2）利用git bash安装的时候没有安装git的，但我觉得你都已经在使用git bash了没有git怎么打开的？小纠结~<br>（3）然后还有node不是最新版本的这样<br>（4）Mac电脑报错命令前面要加suto这样。但是我的电脑是win10，所以也不行。<br><img src="https://img-blog.csdnimg.cn/20190617135231676.jpeg" alt="avatar"><br>3.附上我的解决方案：<br>前提： 重要！！！</p><pre><code>安装好node，配置好node的环境。选择一个磁盘，简历一个文件夹，取名为blog（取啥都行，自己知道以后干嘛的就好）打开建好的文件夹，按住shift同时鼠标右键，选择powershell选项打开命令提示符，前提准备就算完成啦。然后按照下面的指令一步步输入，然后就好啦~</code></pre><p>  在命令的前面加上npx。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npx hexo init blog</span><br><span class="line">cd blog</span><br><span class="line">npm install</span><br><span class="line">npx hexo server</span><br></pre></td></tr></table></figure><p>思考：<br>  最后解释一下，为啥要在前面加上npx。<br>  在大牛阮一峰的网络日志中，他是这么描述的：“npx 想要解决的主要问题，就是调用项目内部安装的模块”，所以可以理解为在命令行下调用，可以让项目内部安装的模块用起来更方便，npx运行的时候，会到node_modules&#x2F;.bin路径和环境变量$PATH里面，检查命令是否存在，所以系统命令也可以调用，即上面的命令安装不成功的时候加上npx的话也许就可以成功了哦~  </p><h2 id="6-3-npm镜像"><a href="#6-3-npm镜像" class="headerlink" title="6.3 npm镜像"></a>6.3 npm镜像</h2><p>用 npm 安装话经常出现卡住而导致无法正常安装，解决办法就是修改 npm 的安装源，这里选择淘宝 NPM 镜像，这是一个完整 npmjs.org 镜像，你可以用此代替官方版本(只读)，同步频率目前为 10分钟 一次以保证尽量与官方服务同步。。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm config set registry https://registry.npm.taobao.org</span><br></pre></td></tr></table></figure><h2 id="6-4-浏览器http-localhost-4000一直在加载中"><a href="#6-4-浏览器http-localhost-4000一直在加载中" class="headerlink" title="6.4 浏览器http://localhost:4000一直在加载中"></a>6.4 浏览器<a href="http://localhost:4000一直在加载中">http://localhost:4000一直在加载中</a></h2><p>用命令hexo server -p 3000来监听3000端口，页面成功加载出来。<br>这就说明，4000端口被占用了。然后看这里查看端口占用，一番复制粘贴之后，就能找到占用端口的程序。指不定是哪个老六占用了<br><img src="https://img-blog.csdnimg.cn/20200131224956516.jpg" alt="avatar">  </p><h2 id="6-5-hexo-d之后报错FATAL-err-Error-Spawn-failed"><a href="#6-5-hexo-d之后报错FATAL-err-Error-Spawn-failed" class="headerlink" title="6.5 hexo d之后报错FATAL err: Error: Spawn failed"></a>6.5 hexo d之后报错FATAL err: Error: Spawn failed</h2><p>这个也好坑，他丫的！不要怀疑，这不是你哪里错了，是服务器不稳定，报错要你检查几个文件。。。解决办法就是。。多试几次，如此的朴实无华。突然想起原来我解决电脑问题的两部曲：硬件问题-重启，软件问题-再来一遍。异曲同工之妙啊。。<br>重新试几次仍然无法解决后，尝试：<br>（1）删除.deploy_git文件，输入：git config –global core.autocrlf false<br>（2）然后再执行hexo clean、g、d三部曲</p><h2 id="6-6-cmd报错：’hexo’-不是内部或外部命令，也不是可运行的程序"><a href="#6-6-cmd报错：’hexo’-不是内部或外部命令，也不是可运行的程序" class="headerlink" title="6.6 cmd报错：’hexo’ 不是内部或外部命令，也不是可运行的程序"></a>6.6 cmd报错：’hexo’ 不是内部或外部命令，也不是可运行的程序</h2><p>这个问题快烦死啦！一般来说每步都正确的话，到这里都应该一帆风顺，程序正常执行的。可恶鸭！！<br><img src="https://img-blog.csdnimg.cn/2019061713555752.jpg" alt="avatar"><br>1.排查问题：<br>  （1）首先，检查各个软件版本是适配的。<br>  （2）用命令检查hexo安装成功<br>  （3）路径也已经加到全局路径中了<br>实在是想不到还有啥问题啊。。。在网上也没找到和我类似的问题。卡在这里两天。。。憋死了。。后来你说怎么着，研究了一下误打误撞的解决了，如果有知道这样的原理的读者可以给我留言讨论一下。<br>2.解决问题：<br>在安装完的node_modules文件中有.ban和hexo文件，以及一个执行完报错生成的hexo-deployer-git文件。其中的hexo文件内有个ban文件，ban文件里还有个hexo文件。但是与之前的目录相比，它少了个hexo.cmd文件。我配置路径的时候是配置的ban文件里的hexo文件的路径。但是其实是要配置带hexo.cmd的那个.ban文件到全局变量里，程序才能执行成功。。。  </p><p>但是还是不知道为啥。。但是谁在乎呢，程序和人有一个能跑的就行了，我不在乎。。<br><img src="https://img-blog.csdnimg.cn/20190617154137744.jpeg" alt="avatar">  </p>]]></content>
      
      
      
        <tags>
            
            <tag> -hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>YOLO v1-7系列总结、思考</title>
      <link href="/post/undefined.html"/>
      <url>/post/undefined.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><hr><p>本篇文章将详细介绍YOLO v1-7系列和自己在学习的时候一些思考与总结，由于文章内容过多，建议可以在目录栏导航进行阅读。后续若发布v8系列，本文将持续更新。感兴趣的话就读下去吧~<br><img src="https://img-blog.csdnimg.cn/img_convert/f8b21cb07c63093b44affd3e4d995477.png" alt="avatar"></p><h1 id="1-YOLO目标检测基本概念"><a href="#1-YOLO目标检测基本概念" class="headerlink" title="1. YOLO目标检测基本概念"></a>1. YOLO目标检测基本概念</h1><p>目标检测（Object Detection）是在图像对一类或多类感兴趣的目标进行查找和分类，以便于确定他们的类别和位置。</p><p>图像识别任务：<br>1.分类（classification）：判断照片里的目标类别 （指示出CAT）<br>2.定位（location）：给定目标位置（红框定位）<br>3.检测（detection）：定位出目标位置并知道目标是什么（红框加CAT）<br>4.分割（segmention):分为实例分割（instance-level）和场景分割（scene-level），每一个像素属于哪个目标物或场景问题 （边缘图像加红框加CAT）  </p><p>目标检需要解决的问题：<br>1.分类问题<br>2.定位问题<br>3.大小问题<br>4.形状问题  </p><p>知识点：<br>1.map指标:综合衡量检测效果<br>2.IOU：真实值与预测值交集&#x2F;真实值与预测值并集<br>3.精度（precision）：TP&#x2F;（TP+FP)<br>4.查全率（recall):TP&#x2F;(TP+FN)<br>5.TP:正例判定为正例<br>FP:错例判定为正例<br>FN:正例判定为错例<br>TN:错例判定为错例<br>6.如何计算map呢？需要把所有阈值都考虑进来，map就是类别的平均。<br>建立Py ,Rx坐标图，计算围起来的面积，结果就是map。需要注意的是，要取p的最大值，画成长方形，求面积。</p><h1 id="2-YOLO-v1"><a href="#2-YOLO-v1" class="headerlink" title="2. YOLO v1"></a>2. YOLO v1</h1><h2 id="2-1-基本思想"><a href="#2-1-基本思想" class="headerlink" title="2.1 基本思想"></a>2.1 基本思想</h2><p>YOLO（you only look once）是将整张图片作为网络的输入，直接在输出层Bounding Box的位置和所属类别进行回归。  </p><p>YOLO将图片划分为7*7（SxS）个网络，每个网格可以检测出2个边框，也就是98个bounding box。每个格子用来预测B个Bounding Box及其置信度，以及C个类别的概率。其中每个Bounding Box包含{x,y,w,h,c}5个参数。每个格子输出的向量长度为SxSx(5xB+C)    </p><p>（x,y）：中心点相对于网格的偏移量<br>（w,h）：高度h与宽度w是相对于整张图象预测的比例<br>置信度c(confidence)：预测框与实际边界框之间的IOU，置信度分数反映了该模型对框是否包含目标的可靠程度，以及预测框的准确程度。如果单元格中不存在目标，则置信度应该为0,置信度分数等于预测框与真实值之间联合的部分交集（IOU） </p><p>理解1:<br>每个格子输出的向量长度为SxSx(5xB+C)的理解：根据YOLO的设计，输入图像被划分为 7x7 的网格（grid），输出张量中的 7x7 就对应着输入图像的 7x7 网格。或者我们把 7x7x30 的张量看作 7x7&#x3D;49个30维的向量，也就是输入图像中的每个网格对应输出一个30维的向量。  </p><p>理解2:<br>30维向量的理解：30维的向量包含2个bbox的位置和置信度以及该网格属于20个类别的概率。20个对象分类指Yolo支持识别20种不同的对象（人、鸟、猫、汽车、椅子等），所以这里有20个值表示该网格位置存在任一种对象的概率。  </p><p>理解3:<br>20个对象分类的概率：对于输入图像中的每个对象，先找到其中心点。中心点在黄色圆点位置，中心点落在黄色网格内，所以这个黄色网格对应的30维向量中，该物体种类的概率是1，其它对象的概率是0。所有其它48个网格的30维向量中，该物体种类的概率就都是0。这就是所谓的”中心点所在的网格对预测该对象负责”。</p><p>损失函数：损失就是网络实际输出值与样品标签值之间的偏差，包括:位置误差，置信度误差（含有object的），置信度误差（不含有object),分类误差  </p><p>理解1：<br>位置误差：选择IOu最大的那个进行计算。公式加根号原因：物体大的就计算小一点，物体小的就计算精确点（敏感）。<br>理解2：<br>置信度误差（含有object的）：先预定一个置信度阈值如0.5，再找大于阈值的IOU，若多个大于择选最大的，在计算置信度（实际预测出来的结果）与真实值（1）之间差异<br>理解3：<br>置信度误差（不含有object)：先预定一个置信度阈值如0.5，再找大于阈值的IOU，若多个大于择选最大的，在计算置信度（实际预测出来的结果）与真实值（0）之间差异.前有个入说明：前景少，但是背景多，为了增加前景重要性。</p><p>例：<br>样品标签：00…1…0|1 0|目标边框坐标|坐标2<br>网络输出：0.03 0.1 …0.6…0.05|0.7 0.2|预测边框坐标1|预测边框坐标2  </p><h2 id="2-2-模型训练"><a href="#2-2-模型训练" class="headerlink" title="2.2 模型训练"></a>2.2 模型训练</h2><p>YOLO v1网络结构有24个卷积层，后面是2个全连接层。仅使用1×1卷积降维层，后面是3×3卷积层。Yolo先使用ImageNet数据集对前20层卷积网络进行预训练，然后使用完整的网络，在PASCAL VOC数据集上进行对象识别和定位的训练。Yolo的最后一层采用线性激活函数，其它层都是Leaky ReLU。训练中采用了drop out和数据增强（data augmentation）来防止过拟合。 </p><p>input image-&gt;卷积层-&gt;全连接层（第二层是1470&#x3D;7x7x30）-&gt;</p><h2 id="2-3-训练过程"><a href="#2-3-训练过程" class="headerlink" title="2.3 训练过程"></a>2.3 训练过程</h2><p>预训练：采用前20个卷积层、平均池化层、全连接层进行了大约一周的预训练；<br>输入：输入数据为224×224和448×448大小的图像。<br>采用相对坐标：通过图像宽度和高度来规范边界框的宽度和高度，使它们落在0和1之间，边界框 x x x和 y y y坐标参数化为特定网格单元位置的偏移量，边界也在0和1之间。<br>损失函数：由坐标预测、是否包含目标物体置信度、类别预测构成。如果目标存在于该网格单元中（前面讨论的条件类别概率），则损失函数仅惩罚分类错误。如果预测器“负责”实际边界框（即该网格单元中具有最高IOU的预测器），则它也仅惩罚边界框坐标错误。<br>学习率：第一个迭代周期，慢慢地将学习率从0.01提高到0.1，然后继续以0.1的学习率训练75个迭代周期，用0.01的学习率训练30个迭代周期，最后用0.001的学习率训练30个迭代周期。<br>避免过拟合策略：使用dropout和数据增强来避免过拟合。</p><h2 id="2-4-优缺点"><a href="#2-4-优缺点" class="headerlink" title="2.4 优缺点"></a>2.4 优缺点</h2><p>优点：<br>（1）YOLO检测物体速度较快。<br>（2）YOLO在训练和测试时都能看到一整张图的信息（而不像其它算法看到局部图片信息），因此YOLO在检测物体是能很好利用上下文信息，从而不容易在背景上预测出错误的物体信息。<br>（3）YOLO可以学到物体泛化特征<br>缺点：<br>（1）精度低于其它state-of-the-art的目标检测网络。<br>（2）对小物体检测效果不好，尤其是密集的小物体，因为一个栅格只能检测2个物体，限制检测效率。<br>（3）容易产生定位错误  </p><p>知识点：<br>1.NMS（非极大值抑制）：识别时很多框，选一个置信度高的，其他的舍弃</p><h1 id="3-YOLO-v2"><a href="#3-YOLO-v2" class="headerlink" title="3. YOLO v2"></a>3. YOLO v2</h1><h2 id="3-1-基本思想"><a href="#3-1-基本思想" class="headerlink" title="3.1 基本思想"></a>3.1 基本思想</h2><p>YOLO v2相对v1版本，在继续保持处理速度的基础上，从预测更准确（Better），速度更快（Faster），识别对象更多（Stronger）这三个方面进行了改进。在精度上利用一些列训练技巧，在速度上应用了新的网络模型DarkNet19（实际输入为416x416），在分类任务上采用联合训练方法，结合wordtree等方法，使YOLO v2的检测种类扩充到了上千种。舍弃了全连接层（FC),5次采样（13x13），卷积后全部加入批量归一化。1*1卷积节省了很多参数。</p><h2 id="3-2-改进部分"><a href="#3-2-改进部分" class="headerlink" title="3.2 改进部分"></a>3.2 改进部分</h2><h3 id="3-2-1-批量归一化（Batch-Normalization）"><a href="#3-2-1-批量归一化（Batch-Normalization）" class="headerlink" title="3.2.1 批量归一化（Batch Normalization）"></a>3.2.1 批量归一化（Batch Normalization）</h3><p>批量归一化有助于解决反向传播过程中的梯度消失和梯度爆炸问题，降低对一些超参数（比如学习率、网络参数的大小范围、激活函数的选择）的敏感性，并且每个batch分别进行归一化的时候，起到了一定的正则化效果（YOLO v2不再使用dropout），从而能够获得更好的收敛速度和收敛效果。(可以简单理解为将杂乱无章的x输入变量通过处理变成转化成按正太分布分布的数据x^,从而使得输入网络的数据分布较近，有利于网络的迭代优化，使得工作效率更快。)</p><p>通常，一次训练会输入一批样本（batch）进入神经网络。批量归一化在神经网络的每一层，在网络（线性变换）输出后和激活函数（非线性变换）之前增加一个批归一化层（BN）。  </p><p>具体推到如下<a href="https://blog.csdn.net/wjinjie/article/details/105028870">BN算法实现公式推导</a> </p><h3 id="3-2-2-高分辨率图像分类器（High-resolution-classifier）"><a href="#3-2-2-高分辨率图像分类器（High-resolution-classifier）" class="headerlink" title="3.2.2 高分辨率图像分类器（High resolution classifier）"></a>3.2.2 高分辨率图像分类器（High resolution classifier）</h3><p>YOLO v1在预训练的时候采用的是224×224的输入，然后在detection的时候采用448×448的输入，这会导致从分类模型切换到检测模型的时候，模型还要适应图像分辨率的改变。而YOLO v2则将预训练分成两步：先用224×224的输入从头开始训练网络，大概160个epoch（表示将所有训练数据循环跑160次），然后再将输入调整到448×448，再训练10个epoch。注意这两步都是在ImageNet数据集上操作。最后再在检测的数据集上微调（fine-tuning），也就是detection的时候用448×448的图像作为输入就可以顺利过渡了。  </p><h3 id="3-2-3-带Anchor-Boxes的卷积（Convolutional-With-Anchor-Boxes）"><a href="#3-2-3-带Anchor-Boxes的卷积（Convolutional-With-Anchor-Boxes）" class="headerlink" title="3.2.3 带Anchor Boxes的卷积（Convolutional With Anchor Boxes）"></a>3.2.3 带Anchor Boxes的卷积（Convolutional With Anchor Boxes）</h3><p>YOLO v1利用全连接层直接对边界框进行预测，导致丢失较多空间信息，定位不准。YOLO v2去掉了YOLO v1中的全连接层，使用Anchor Boxes预测边界框，同时为了得到更高分辨率的特征图，YOLO v2还去掉了一个池化层。由于图片中的物体都倾向于出现在图片的中心位置，特别是那种比较大的物体，若特征图恰好有一个中心位置，利用这个中心位置预测中心点落入该位置的物体，对这些物体的检测会更容易。所以总希望得到的特征图的宽高都为奇数。YOLO v2通过缩减网络，使用416×416的输入，模型下采样的总步长为32，最后得到13×13的特征图，然后对13×13的特征图的每个cell预测5个anchor boxes，对每个anchor box预测边界框的位置信息、置信度和一套分类概率值。  </p><p>理解1：<br>图片输入分辨率为416 * 416原因：目的是为了让后面产生的卷积特征图宽高都为奇数（下采样步长为32），最终得到13 * 13的卷积特征图（416&#x2F;32&#x3D;13），（32&#x3D;2^5)这样就可以只有一个center cell。因为，大物体通常占据了图像的中间位置， 就可以只用中心的一个cell来预测这些物体的位置，否则就要用中间的4个cell来进行预测，这个技巧可稍稍提升效率。  </p><p>理解2：<br>加入了anchor boxes后，召回率上升，准确率下降的原因：假设每个cell预测9个anchor boxs，那么总共会预测13 * 13 * 9 &#x3D; 1521个boxes，而之前的网络仅仅预测7 * 7 * 2 &#x3D; 98个boxes。  </p><p>理解3：<br>关于输出tensor：对于YOLOv1，每个cell都预测2个boxes，每个boxes包含5个值：( x , y , w , h , c )，前4个值是边界框位置与大小，最后一个值是置信度（confidence scores，包含两部分：含有物体的概率以及预测框与ground truth的IOU）。但是每个cell只预测一套分类概率值（class predictions，其实是置信度下的条件概率值）,供2个boxes共享。YOLOv2使用了anchor boxes之后，每个位置的各个anchor box都单独预测一套分类概率值。<br>例：<br>YOLO v1:  box1:4-x,y,w,h|box2:1-c|20-classes<br>YOLO v2:  box1:4-x,y,w,h|1-c|20-classes|box2:4-x,y,w,h|1-c|20-classes|…  </p><h3 id="3-2-4-维度聚类（Dimension-Clusters）"><a href="#3-2-4-维度聚类（Dimension-Clusters）" class="headerlink" title="3.2.4 维度聚类（Dimension Clusters）"></a>3.2.4 维度聚类（Dimension Clusters）</h3><p>在使用anchor的时候遇到了两个问题，第一个是anchor boxes的宽高维度往往是精选的先验框（hand-picked priors），虽说在训练过程中网络也会学习调整boxes的宽高维度，最终得到准确的bounding boxes。但是，如果一开始就选择了更好的、更有代表性的先验boxes维度，那么网络就更容易学到准确的预测位置。</p><p>使用K-means聚类方法类训练bounding boxes，可以自动找到更好的boxes宽高维度。传统的K-means聚类方法使用的是欧氏距离函数，也就意味着较大的boxes会比较小的boxes产生更多的error，聚类结果可能会偏离。并且，先验框的主要目的是为了使得预测框和ground truth的IOU更好，所以聚类分析时选用box与聚类中心box之间的IOU值作为距离标准，这样，error就和box的尺度无关了，最终的距离函数为：d(box,centroid)&#x3D;1−IOU(box,centroid)  </p><p>简单理解：很多框，按照大小分类（k&#x3D;5时最优），每类取出一个最体现这类特征的值作为预选框。</p><p>深入研究<a href="https://blog.csdn.net/xiaomifanhxx/article/details/81215051">K-means原理</a></p><h3 id="3-2-5-直接定位预测（Direct-location-Prediction）"><a href="#3-2-5-直接定位预测（Direct-location-Prediction）" class="headerlink" title="3.2.5 直接定位预测（Direct location Prediction）"></a>3.2.5 直接定位预测（Direct location Prediction）</h3><p>YOLO v2沿用YOLO v1的方法，根据所在网格单元的位置来预测坐标，则Ground Truth的值介于0到1之间。网络中将得到的网络预测结果再输入sigmoid函数中，让输出结果介于0到1之间。设一个网格相对于图片左上角的偏移量是 Cx,Cy,先验框的宽度和高度分别是Pw和Ph​，则预测的边界框相对于特征图的中心坐标(bx​,by​)和宽度bw​，bh​的计算公式如下图所示。其中，σ为sigmoid函数，tx​，ty​是预测的坐标偏移值（中心点坐标），tw​，th​是尺度缩放，分别经过sigmoid，输出0-1之间的偏移量，与cx​,cy​相加后得到bounding box中心点的位置。  </p><p>简单理解：<br>YOLO v1:<br>1.bbox:中心为（xp,yp);宽和高为（wp,hp),则x&#x3D;xp+wp<em>tx;y&#x3D;up+hp</em>ty。<br>2.当tx&#x3D;1时，将bbox在x轴向右移动wp;tx&#x3D;-1，则将其向左移动wp。<br>3.这样会导致收敛问题，模型不稳定，尤其是刚开始进行训练的时候<br>4.v2中没有使用偏移量，而是选择相对grid cell的偏移量  </p><p>YOLO v2：<br>1.公式：bx&#x3D;o(tx)+cx;by&#x3D;o(ty)+cy;bw&#x3D;pwe^tw;bh&#x3D;phe^th<br>2.先验框的pw,ph为实机照片大小除以32，表示的是在特征图里的位置。o(tx)与o（ty）表示的是预测的中心点相对于实际框（13x13中的一个）左上角点的偏移量。<br>3.例如：预测值（o(tx),o(ty),tw,th)&#x3D;(0.2,0.1,0.2,0.32),先验框为pw&#x3D;3.19275,ph&#x3D;4.00944<br>则<br>在特征图位置：bx&#x3D;0.2+1&#x3D;1.2,by&#x3D;0.1+1&#x3D;1.1,bw&#x3D;3.19275<em>e^0.2&#x3D;3.89963,bh&#x3D;4.00944</em>e^0.32&#x3D;5.5215<br>在原位置：bx&#x3D;1.2<em>32&#x3D;38.4,by&#x3D;1.1</em>32&#x3D;35.2,bw&#x3D;3.89963<em>32&#x3D;124.78,bh&#x3D;5.52151</em>32&#x3D;176.68  </p><h3 id="3-2-6-细粒度特征（Fine-Grained-Features）"><a href="#3-2-6-细粒度特征（Fine-Grained-Features）" class="headerlink" title="3.2.6 细粒度特征（Fine-Grained Features）"></a>3.2.6 细粒度特征（Fine-Grained Features）</h3><p>yolov2最终在13 * 13的特征图上进行预测，虽然这足以胜任大尺度物体的检测，但是用上细粒度特征的话，这可能对小尺度的物体检测有帮助。这里使用了一种不同的方法，简单添加了一个转移层（ passthrough layer），这一层要把浅层特征图（分辨率为26 * 26，是底层分辨率4倍）连接到深层特征图。</p><p>转移层（ passthrough layer）就是将深层特征与浅层特征进行连接，这里采用的是通道上的连接而非空间位置上的连接，即叠加特征到不同的通道。yolov2把26 * 26 * 512的特征图变换为13 * 13 * 2048的特征图，将这个特征图与原来的特征相连接。yolov2使用的就是经过连接的特征图，它可以拥有更好的细粒度特征，使得模型的性能获得了1%的提升。</p><p>关于特征融合的具体方法为：passthrough layer，具体来说就是特征重排（不涉及到参数学习），前面26 * 26 * 512的特征图使用按行和按列隔行采样的方法，就可以得到4个新的特征图，维度都是13 * 13 * 512，然后做concat操作，得到13 * 13 * 2048的特征图，将其拼接到后面的层，相当于做了一次特征融合，有利于检测小目标。  </p><p>简单理解：<br>1.概述来说就是特征图上的点能看到原始图像多大区域。越大的感受野越能体现图像特征。<br>2.最后一层时感受野太大了，小目标可能丢失了，需融合之前的特征。</p><p>思考：<br>1.如果堆叠3个3<em>3的卷积层，并且保持滑动窗口步长为1，其感受野就是7</em>7的了，这和使用7<em>7卷积核结果是一样的，那为什么要丢及3个小卷积呢？<br>假设输入大小都是h</em>w<em>c，并且都使用c个卷积核（得到C个特征图），可以计算一下各自需要的参数：<br>一个7</em>7卷积核所需参数：&#x3D;Cx(7x7xc)&#x3D;49c^2;一个3*3卷积核所需参数：&#x3D;3xc(3x3xc)&#x3D;27c^2<br>很明显，堆叠小的卷积核所需的参数更小一些，并且卷积过程越多，特征提取也会越细致，加入的非线性变换也随着增多，还不会增大权重参数个数，用小的卷积核来完成体积特征提取操作。</p><h3 id="3-2-7-多尺度训练（Multi-Scale-Training）"><a href="#3-2-7-多尺度训练（Multi-Scale-Training）" class="headerlink" title="3.2.7 多尺度训练（Multi-Scale Training）"></a>3.2.7 多尺度训练（Multi-Scale Training）</h3><p>yolov1网络使用固定的448 * 448的图片作为输入，在加入anchor boxes后，yolov2的输入变成了416 * 416。由于yolov2只用到了卷积层和池化层，那么就可以进行动态调整（意思是可检测任意大小图片）。</p><p>不同于固定输入网络的图片尺寸的方法，在几次迭代后就会微调网络。每经过10次训练（10 epoch），就会随机选择新的图片尺寸。YOLO网络使用的降采样参数为32，那么就使用32的倍数进行尺度池化{320,352，…，608}，最终最小的尺寸为320 * 320，最大的尺寸为608 * 608，接着按照输入尺寸调整网络进行训练。这种机制使得网络可以更好地预测不同尺寸的图片，意味着同一个网络可以进行不同分辨率的检测任务，在小尺寸图片上YOLOv2运行更快，在速度和精度上达到了平衡。  </p><h2 id="YOLO-V2反思"><a href="#YOLO-V2反思" class="headerlink" title="YOLO V2反思"></a>YOLO V2反思</h2><p>1.YOLO v1,v2中使用IOU为置信度标签有什么不好？<br>（1）很多预测框与中心点的IOU最高只有0.7（最好的学生只有70分，取乎其中的乎其下）<br>（2）coco中的小目标IOU对像素偏移很敏感无法有效学习。  </p><h1 id="4-YOLO-v3"><a href="#4-YOLO-v3" class="headerlink" title="4.YOLO v3"></a>4.YOLO v3</h1><h2 id="4-1-基础概念"><a href="#4-1-基础概念" class="headerlink" title="4.1 基础概念"></a>4.1 基础概念</h2><p>YOLO v3总结了在YOLO v2的基础上做了一些尝试性的改进。其中有两个值得一提的亮点，一个是使用残差模型，进一步加深了网络结构；另一个是使用FPN架构实现多尺度检测。特征做的更细致，融入多持续特征信息来预测不同规格的物体。先验框更丰富了，3种scale，每种3个规格，一共9种。  </p><h2 id="4-2-改进部分"><a href="#4-2-改进部分" class="headerlink" title="4.2 改进部分"></a>4.2 改进部分</h2><p>1.引入FPN（特征金字塔网络），实现多尺度预测 。<br>2.使用新网络结构darknet-53（类似于ResNet引入残差结构）。<br>3.分类器不在使用Softmax，分类损失采用二分类交叉损失熵（binary cross-entropy loss)<br>主要考虑因素有两个：<br>Softmax使得每个框分配一个类别（score最大的一个），而对于Open Images这种数据集，目标可能有重叠的类别标签，因此Softmax不适用于多标签分类。Softmax可被独立的多个logistic分类器替代，且准确率不会下降。  </p><h3 id="4-2-1-多尺度预测"><a href="#4-2-1-多尺度预测" class="headerlink" title="4.2.1 多尺度预测"></a>4.2.1 多尺度预测</h3><p>YOLO v3在基本特征提取器上添加几个卷积层，其中最后一个卷积层预测了一个三维张量——边界框，目标和类别预测。 在COCO实验中，为每个尺度预测3个框，4个边界框偏移量，1个目标预测和80个类别预测，张量的大小为 N × N × [ 3 × ( 4 + 1 + 80 ) ] N×N×[3 ×(4 + 1 + 80)] N×N×[3×(4+1+80)]。</p><p>YOLO v3还从主干网络层中获取特征图，并使用按元素相加的方式将其与上采样特征图进行合并。这种方法能够从上采样的特征图中获得更有意义的语义信息，同时可以从更前的层中获取更细粒度的信息。然后，再添加几个卷积层来处理这个组合的特征图，并最终预测出一个类似的张量。</p><p>每种尺度预测3个box， anchor的设计方式仍然使用聚类，得到9个聚类中心，将其按照大小均分给3个尺度。</p><p>尺度1: 在基础网络之后经过卷积输出13×13特征层信息（32倍下采样）13<em>13</em>3*（4+1+80）假如80个分类<br>尺度2: 从尺度1中倒数第二13×13卷积层上采样(x2)后与主干网络26x26大小的特征图相堆叠，再通过卷积后输出26×26特征层信息（16倍下采样）。<br>尺度3: 与尺度2类似卷积后输出52×52特征层信息（8倍下采样）。  </p><p>简单理解：<br>1.13为老年人，26为中年人，52为年轻人。一般是三个人分别提取特征值。YOLO v3方法则是将老年人与上一层的中年人特征值融合，中年人再与年轻人特征值融合。最后老年人的特征值就融合的很全面。<br>2.采用多个scale融合的方式做预测。原来的YOLO v2有一个层叫：passthrough layer，假设最后提取的feature map的size是13<em>13，那么这个层的作用就是将前面一层的26</em>26的feature map和本层的13*13的feature map进行连接。操作也是为了加强YOLO算法对小目标检测的精确度，在多个scale的feature map上做检测，对于小目标的检测效果提升还是比较明显的。     </p><p>思考：<br>在YOLO v3中每个grid cell预测3个bounding box，看起来比YOLO v2中每个grid cell预测5个bounding box要少呢？  </p><p>因为YOLO v3采用了多个scale的特征融合，所以boundign box的数量要比之前多很多，以输入图像为416<em>416为例：（13</em>13+26<em>26+52</em>52）<em>3和13</em>13*5相比哪个更多应该很清晰了  </p><h3 id="4-2-2-多标签分类"><a href="#4-2-2-多标签分类" class="headerlink" title="4.2.2 多标签分类"></a>4.2.2 多标签分类</h3><p>YOLOv3在类别预测方面将YOLOv2的单标签分类改进为多标签分类，在网络结构中将YOLOv2中用于分类的softmax层修改为逻辑分类器。在YOLOv2中，算法认定一个目标只从属于一个类别，根据网络输出类别的得分最大值，将其归为某一类。然而在一些复杂的场景中，单一目标可能从属于多个类别。</p><p>为实现多标签分类就需要用逻辑分类器来对每个类别都进行二分类。逻辑分类器主要用到了sigmoid函数，它可以把输出约束在0到1，如果某一特征图的输出经过该函数处理后的值大于设定阈值，那么就认定该目标框所对应的目标属于该类。（例如：大于阈值0.7，则小猫大猫都属于猫）</p><h3 id="4-2-3-初始尺寸"><a href="#4-2-3-初始尺寸" class="headerlink" title="4.2.3 初始尺寸"></a>4.2.3 初始尺寸</h3><p>3、关于bounding box的初始尺寸还是采用YOLO v2中的k-means聚类的方式来做，不过数量变了。这种先验知识对于bounding box的初始化帮助还是很大的，毕竟过多的bounding box虽然对于效果来说有保障，但是对于算法速度影响还是比较大的。作者在COCO数据集上得到的9种聚类结果：(10<em>13); (16</em>30); (33<em>23); (30</em>61); (62<em>45); (59</em>119); (116<em>90); (156</em>198); (373<em>326)，这应该是按照输入图像的尺寸是416</em>416计算得到的。</p><h2 id="4-3-YOLO-v3反思总结"><a href="#4-3-YOLO-v3反思总结" class="headerlink" title="4.3 YOLO v3反思总结"></a>4.3 YOLO v3反思总结</h2><p>1.结合ResNet骨干网络发现，三个分支输出特征图的大小分别为32倍下采样、16倍下采样和8倍下采样的大小，这样做有什么好处？ </p><p>在特征提取骨干网络中，下采样倍数越多，感受野越大。所以32倍下采样特征图中，每个点的感受野是最大的，映射到原图可以看到更大的区域，所以用于预测大目标，16倍和8倍下采样同理。</p><p>2.边框预测公式中tx ty为什么要sigmoid？  </p><p>预测的tx和ty可能会比较大，tx,ty&gt;1。用sigmoid将tx,ty压缩到(0,1)区间内，可以有效的确保目标中心处于执行预测的网格单元中，防止偏移过多。  </p><p>举个栗子，我们一直都说网络不会预测边界框中心的坐标而是预测与预测目标的grid cell左上角相关的便宜tx,ty。例如在图2中，某目标的中心点偏移值预测假设为(0.4,0.7)，Cx&#x3D;3，Cy&#x3D;3，该物体的在feature map中的中心实际坐标显然是（3.4，3.7），此时是正常的。但若预测出的tx和ty大于1，如(1.6,0.8)，则物体的在feature map中的中心实际坐标显然是（3.6，3.8），注意此时物体中心在这个所属grid cell外面了，但(3,3)这个grid cell却检测出这个单元格内含有目标的中心（YOLO系列是物体中心点落在哪个grid cell，那么就由哪个grid cell进行预测），这样就矛盾了，因为左上角为(3,3)的grid cell负责预测这个物体，这个物体中心必须出现在这个grid cell中，所以一旦tx和ty大于1就会出现矛盾，因此必须进行归一化，也就是通过sigmoid函数将tx和ty显示在（0,1）。  </p><p>3.边框预测公式中tw和th为什么指数？  </p><p>tw，th是log尺度缩放到对数空间了，需要指数回来。<br>网络预测出偏移值后，若在推理阶段，根据边框预测公式得到最终的坐标值即可。不同的是，在训练阶段，我们需要计算损失，然后反向传播更新网络参数，所以我们需要了解YOLOv3的损失函数。  </p><p>4.Ground Truth的计算  </p><p>既然网络预测的是偏移值，那么在计算损失时，也是按照偏移值计算损失。现在我们有预测的值，还需要真值Ground Truth的偏移值。Ground Truth的tw，th是log尺度缩放到对数空间了，所以在预测时需要指数回来。这就是答案。  </p><p>5.为什么在计算Ground Truth的tw，th时需要缩放到对数空间？  </p><p>tw和th是物体所在边框的长宽和anchor box长宽之间的比率。不直接回归bounding box的长宽，是为避免训练带来不稳定的梯度。将尺度缩放到对数空间，因为如果直接预测相对形变tw 和 th，那么要求tw,th&gt;0，因为框的宽高不可能是负数，这样的话是在做一个有不等式条件约束的优化问题，没法直接用SGD来做，所以先取一个对数变换，将其不等式约束去掉就可以了。  </p><p>6.正负样本的确定  </p><p>一个尺度的feature有三个anchors，那么对于某个ground truth框，究竟是哪个anchor负责匹配它呢？<br>与yolov1一样，对于训练图片中的ground truth，若其中心点落在某个cell内，那么该cell内的3个anchor box负责预测它，具体是哪个anchor box预测它，需要在训练中确定，即由那个与ground truth的IOU最大的anchor box预测它，而剩余的两个anchor box不予该GT框匹配。（YOLOv3需要假定每个cell至多含有一个ground truth，而在实际上基本不会出现多于1个的情况。）  </p><p>每个GT目标仅与一个anchor相关联，与GT匹配的anchor box计算坐标误差、置信度误差（此时target为1）以及分类误差，而其他anchor box只计算置信度误差（此时target为0）。对于重叠大于等于0.5的其他预测框，忽略，不算损失。  </p><p>总的来说，正样本是与GT的IOU最大的框。负样本是与GT的IOU&lt;0.5的框。忽略的样本是与GT的IOU&gt;0.5 但不是最大的框。  </p><h1 id="4-YOLO-v4"><a href="#4-YOLO-v4" class="headerlink" title="4. YOLO v4"></a>4. YOLO v4</h1><h2 id="4-1-基本概念"><a href="#4-1-基本概念" class="headerlink" title="4.1 基本概念"></a>4.1 基本概念</h2><p>说在开头 YOLO v4实在是太多tricks啦，我要吐血了。。引用了很多图来便于理解，不然看一会就变成天线宝宝了，阿巴阿巴  </p><p>YOLOv4对深度学习中一些常用Tricks进行了大量的测试，最终选择了这些有用的Tricks：WRC、CSP、CmBN、SAT、 Mish activation、Mosaic data augmentation、CmBN、DropBlock regularization 和 CIoU loss。</p><p>是一个高效而强大的目标检测网咯。它使我们每个人都可以使用 GTX 1080Ti 或 2080Ti 的GPU来训练一个超快速和精确的目标检测器。对当前先进的目标检测方法进行了改进，使之更有效，并且更适合在单GPU上训练；这些改进包括CBN、PAN、SAM等。</p><h2 id="4-2-网络框架"><a href="#4-2-网络框架" class="headerlink" title="4.2 网络框架"></a>4.2 网络框架</h2><p> YOLOv4 &#x3D; CSPDarknet53（主干） + SPP附加模块（颈） + PANet路径聚合（颈） + YOLOv3（头部）</p><h2 id="4-3-框架介绍"><a href="#4-3-框架介绍" class="headerlink" title="4.3 框架介绍"></a>4.3 框架介绍</h2><h3 id="4-3-1-CSPDarknet53"><a href="#4-3-1-CSPDarknet53" class="headerlink" title="4.3.1 CSPDarknet53"></a>4.3.1 CSPDarknet53</h3><p>CSPNet（Cross Stage Partial Network），用来解决以往网络结构需要大量推理计算的问题。作者将问题归结于网络优化中的重复梯度信息。CSPNet在ImageNet dataset和MS COCO数据集上有很好的测试效果，同时它易于实现，在ResNet、ResNeXt和DenseNet网络结构上都能通用。</p><p>CSPNet的主要目的是能够实现更丰富的梯度组合，同时减少计算量。这个目标是通过将基本层的特征图分成两部分，然后通过一个跨阶段的层次结构合并它们来实现的。</p><p>而在YOLOv4中，将原来的Darknet53结构换为了CSPDarknet53，这在原来的基础上主要进行了两项改变：</p><p>（1）将原来的Darknet53与CSPNet进行结合。在前面的YOLOv3中，我们了解了Darknet53的结构，它是由一系列残差结构组成。进行结合后，CSPnet的主要工作就是将原来的残差块的堆叠进行拆分，把它拆分成左右两部分：主干部分继续堆叠原来的残差块，支路部分则相当于一个残差边，经过少量处理直接连接到最后。</p><p>理解：v3拆分，v4将拆分后的再拆成两部分，一部分进行处理再与另外一部分进行连接。</p><p>（2）使用MIsh激活函数代替了原来的Leaky ReLU。在YOLOv3中，每个卷积层之后包含一个批量归一化层和一个Leaky ReLU。而在YOLOv4的主干网络CSPDarknet53中，使用Mish代替了原来的Leaky ReLU。</p><p>理解：数学公式是如此的神奇。</p><p>深入研究<a href="https://arxiv.org/pdf/1911.11929.pdf">CSPNet论文</a><br><a href="https://github.com/WongKinYiu/CrossStagePartialNetworks">CSPNet开源地址</a>  </p><h3 id="4-3-2-SPP"><a href="#4-3-2-SPP" class="headerlink" title="4.3.2 SPP"></a>4.3.2 SPP</h3><p>SPP最初的设计目的是用来使卷积神经网络不受固定输入尺寸的限制。在YOLOv4中，作者引入SPP，是因为它显著地增加了感受野，分离出了最重要的上下文特征，并且几乎不会降低的YOLOv4运行速度。SPP中经典的空间金字塔池化层。  </p><p>在YOLOv4中，具体的做法就是：分别利用四个不同尺度的最大池化对上层输出的feature map进行处理。最大池化的池化核大小分别为13x13、9x9、5x5、1x1，其中1x1就相当于不处理。<br><img src="https://img-blog.csdnimg.cn/img_convert/25a6c1825b22733dfab2fedcc8713d2d.png" alt="avatar"><br>注意：这里最大池化采用padding操作，移动的步长为1，比如13×13的输入特征图，使用5×5大小的池化核池化，padding&#x3D;2，因此池化后的特征图仍然是13×13大小。</p><p>深入研究<a href="https://link.csdn.net/?target=https://link.springer.com/content/pdf/10.1007/978-3-319-10578-9_23.pdf">SPP论文</a></p><h3 id="4-3-3-PANet"><a href="#4-3-3-PANet" class="headerlink" title="4.3.3 PANet"></a>4.3.3 PANet</h3><p>PANet整体上可以看做是在Mask R-CNN上做多处改进，充分利用了特征融合，比如引入Bottom-up path augmentation结构，充分利用网络浅特征进行分割；引入Adaptive feature pooling使得提取到的ROI特征更加丰富；引入Fully-conneFcted fusion，通过融合一个前背景二分类支路的输出得到更加精确的分割结果。</p><p>主要包含FPN、Bottom-up path augmentation、Adaptive feature pooling、Fully-connected fusion四个部分   </p><p><img src="https://img-blog.csdnimg.cn/20200730142333938.png" alt="avatar">  </p><p>FPN:主要是通过融合高低层特征提升目标检测的效果，尤其可以提高小尺寸目标的检测效果。<br>Bottom-up Path Augmentation:主要是考虑网络浅层特征信息对于实例分割非常重要，因为浅层特征一般是边缘形状等特征。<br>Adaptive Feature Pooling:用来特征融合。也就是用每个ROI提取不同层的特征来做融合，这对于提升模型效果显然是有利无害。<br>Fully-connected Fusion:针对原有的分割支路（FCN）引入一个前背景二分类的全连接支路，通过融合这两条支路的输出得到更加精确的分割结果。  </p><p>理解：<br>在YOLOv4中，作者使用PANet代替YOLOv3中的FPN作为参数聚合的方法，针对不同的检测器级别从不同的主干层进行参数聚合。并且对原PANet方法进行了修改, 使用张量连接(concat)代替了原来的捷径连接(shortcut connection)。  </p><p>思考：<br>YOLO v3中：<br><img src="https://img-blog.csdnimg.cn/img_convert/18dba1a64b98c8d019befc37598f2ea2.png" alt="avatar"><br>其中neck部分：我们将Neck部分用立体图画出来，更直观的看下两部分之间是如何通过FPN结构融合的。<br><img src="https://img-blog.csdnimg.cn/img_convert/70ac463cf2d23b2ec96283a0b78cf983.png" alt="avatar"><br>以及最后的Prediction中用于预测的三个特征图①19<em>19</em>255、②38<em>38</em>255、③76<em>76</em>255。[注：255表示80类别(1+4+80)×3&#x3D;255]<br>如图所示，FPN是自顶向下的，将高层的特征信息通过上采样的方式进行传递融合，得到进行预测的特征图。  </p><p>YOLO v4中：<br>而Yolov4中Neck这部分除了使用FPN外，还在此基础上使用了PAN结构：<br><img src="https://img-blog.csdnimg.cn/img_convert/273816f9e51dbcbd75e6a7ee9967ea4b.png" alt="avatar"><br>前面CSPDarknet53中讲到，每个CSP模块前面的卷积核都是3<em>3大小，步长为2，相当于下采样操作。因此可以看到三个紫色箭头处的特征图是76</em>76、38<em>38、19</em>19。以及最后Prediction中用于预测的三个特征图：①76<em>76</em>255，②38<em>38</em>255，③19<em>19</em>255。我们也看下Neck部分的立体图像，看下两部分是如何通过FPN+PAN结构进行融合的。<br><img src="https://img-blog.csdnimg.cn/img_convert/7a77751f54d9115247ff00705c013952.png" alt="avatar"><br>和Yolov3的FPN层不同，Yolov4在FPN层的后面还添加了一个自底向上的特征金字塔。<br>其中包含两个PAN结构。<br>这样结合操作，FPN层自顶向下传达强语义特征，而特征金字塔则自底向上传达强定位特征，两两联手，从不同的主干层对不同的检测层进行参数聚合,这样的操作确实很皮。  </p><p>注意：<br>注意一：Yolov3的FPN层输出的三个大小不一的特征图①②③直接进行预测。但Yolov4的FPN层，只使用最后的一个76x76特征图①，而经过两次PAN结构，输出预测的特征图②和③。这里的不同也体现在cfg文件中，这一点有很多同学之前不太明白，比如Yolov3.cfg最后的三个Yolo层，第一个Yolo层是最小的特征图19x19，mask&#x3D;6,7,8，对应最大的anchor box。第二个Yolo层是中等的特征图38x38，mask&#x3D;3,4,5，对应中等的anchor box第三个Yolo层是最大的特征图76x76，mask&#x3D;0,1,2，对应最小的anchor box。而Yolov4.cfg则恰恰相反第一个Yolo层是最大的特征图76x76，mask&#x3D;0,1,2，对应最小的anchor box。第二个Yolo层是中等的特征图38x38，mask&#x3D;3,4,5，对应中等的anchor box。第三个Yolo层是最小的特征图19*19，mask&#x3D;6,7,8，对应最大的anchor box。  </p><p>注意点二：原本的PANet网络的PAN结构中，两个特征图结合是采用shortcut操作，而Yolov4中则采用concat（route）操作，特征图融合后的尺寸发生了变化。<br><img src="https://img-blog.csdnimg.cn/img_convert/467b7d2a4e3bb426b8d03203e4ffaa53.png" alt="avatar"></p><h3 id="4-3-4-YOLOv3-Head"><a href="#4-3-4-YOLOv3-Head" class="headerlink" title="4.3.4 YOLOv3 Head"></a>4.3.4 YOLOv3 Head</h3><p>在YOLOv4中，继承了YOLOv3的Head进行多尺度预测，提高了对不同size目标的检测性能。YOLOv3的完整结构在上文已经详细介绍，下面我们截取了YOLOv3的Head进行分析：  </p><p>YOLOv4学习了YOLOv3的方式，采用三个不同层级的特征图进行融合，并且继承了YOLOv3的Head。在COCO数据集上训练时，YOLOv4的3个输出张量的shape分别是：（19，19，225）、（38，38，255）、（76，76，225）。这是因为COCO有80个类别，并且每一个网格对应3个Anchor boxes，而每个要预测的bounding box对应的5个值（tx、ty、tw、th、to),所以有:3 x (80+5)&#x3D;255  </p><h3 id="4-3-5-各种Tricks总结"><a href="#4-3-5-各种Tricks总结" class="headerlink" title="4.3.5 各种Tricks总结"></a>4.3.5 各种Tricks总结</h3><p>作者将所有的Tricks可以分为两类：</p><p>在不增加推理成本的前提下获得更好的精度，而只改变训练策略或只增加训练成本的方法，作着称之为 “免费包”（Bag of freebies）；<br>只增加少量推理成本但能显著提高目标检测精度的插件模块和后处理方法，称之为“特价包”（Bag of specials）</p><p>下面分别对这两类技巧进行介绍</p><p>（1）免费包<br>以数据增强方法为例，虽然增加了训练时间，但不增加推理时间，并且能让模型泛化性能和鲁棒性更好。像这种不增加推理成本，还能提升模型性能的方法，作者称之为”免费包”，非常形象。<br>下面总结了一些常用的数据增强方法：</p><p>随机缩放<br>翻转、旋转<br>图像扰动、加噪声、遮挡<br>改变亮度、对比对、饱和度、色调<br>随机裁剪（random crop）<br>随机擦除（random erase）<br>Cutout<br>MixUp<br>CutMix  </p><p>常见的正则化方法有：</p><p>DropOut<br>DropConnect<br>DropBlock  </p><p>传统的Dropout很简单，一句话就可以说的清：随机删除减少神经元的数量，使网络变得更简单。<br><img src="https://img-blog.csdnimg.cn/img_convert/671aad4ffe97e70163213479eec419e5.png" alt="avatar"><br>而Dropblock和Dropout相似，比如下图:<br><img src="https://img-blog.csdnimg.cn/img_convert/b25adf044a54c9259a8475982bc990fa.png" alt="avatar">   </p><p>中间Dropout的方式会随机的删减丢弃一些信息，但Dropblock的研究者认为，卷积层对于这种随机丢弃并不敏感，因为卷积层通常是三层连用：卷积+激活+池化层，池化层本身就是对相邻单元起作用。而且即使随机丢弃，卷积层仍然可以从相邻的激活单元学习到相同的信息。因此，在全连接层上效果很好的Dropout在卷积层上效果并不好。<br>所以右图Dropblock的研究者则干脆整个局部区域进行删减丢弃。<br>这种方式其实是借鉴2017年的cutout数据增强的方式，cutout是将输入图像的部分区域清零，而Dropblock则是将Cutout应用到每一个特征图。而且并不是用固定的归零比率，而是在训练时以一个小的比率开始，随着训练过程线性的增加这个比率。   </p><p>Dropblock的研究者与Cutout进行对比验证时，发现有几个特点：<br>优点一：Dropblock的效果优于Cutout<br>优点二：Cutout只能作用于输入层，而Dropblock则是将Cutout应用到网络中的每一个特征图上<br>优点三：Dropblock可以定制各种组合，在训练的不同阶段可以修改删减的概率，从空间层面和时间层面，和Cutout相比都有更精细的改进。<br>Yolov4中直接采用了更优的Dropblock，对网络的正则化过程进行了全面的升级改进。</p><p>平衡正负样本的方法有：</p><p>Focal loss<br>OHEM(在线难分样本挖掘)  </p><p>除此之外，还有回归 损失方面的改进：</p><p>GIOU<br>DIOU<br>CIoU  </p><p>（1）CIOU_loss<br>目标检测任务的损失函数一般由Classificition Loss（分类损失函数）和Bounding Box Regeression Loss（回归损失函数）两部分构成。<br>Bounding Box Regeression的Loss近些年的发展过程是：Smooth L1 Loss-&gt; IoU Loss（2016）-&gt; GIoU Loss（2019）-&gt; DIoU Loss（2020）-&gt;CIoU Loss（2020）<br>我们从最常用的IOU_Loss开始，进行对比拆解分析，看下Yolov4为啥要选择CIOU_Loss。<br>a.IOU_Loss<br><img src="https://img-blog.csdnimg.cn/img_convert/34096cdf5a9a844bc9049cc50da95858.png" alt="avatar"><br>可以看到IOU的loss其实很简单，主要是交集&#x2F;并集，但其实也存在两个问题。<br><img src="https://img-blog.csdnimg.cn/img_convert/dedcabfce67835e2b61d9ed9a79da46f.png" alt="avatar"><br>问题1：即状态1的情况，当预测框和目标框不相交时，IOU&#x3D;0，无法反应两个框距离的远近，此时损失函数不可导，IOU_Loss无法优化两个框不相交的情况。  </p><p>问题2：即状态2和状态3的情况，当两个预测框大小相同，两个IOU也相同，IOU_Loss无法区分两者相交情况的不同。</p><p>因此2019年出现了GIOU_Loss来进行改进。<br>b.GIOU_Loss<br><img src="https://img-blog.csdnimg.cn/img_convert/afa33fb2b025a1ba4832b72cda5495a4.png" alt="avatar"><br>可以看到右图GIOU_Loss中，增加了相交尺度的衡量方式，缓解了单纯IOU_Loss时的尴尬。<br>但为什么仅仅说缓解呢？<br>因为还存在一种不足：<br><img src="https://img-blog.csdnimg.cn/img_convert/dc250542414037a625b7a0332a9f34b3.png" alt="avatar"><br>问题：状态1、2、3都是预测框在目标框内部且预测框大小一致的情况，这时预测框和目标框的差集都是相同的，因此这三种状态的GIOU值也都是相同的，这时GIOU退化成了IOU，无法区分相对位置关系。<br>基于这个问题，2020年的AAAI又提出了DIOU_Loss。<br>c.DIOU_Loss<br>好的目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比。<br>针对IOU和GIOU存在的问题，作者从两个方面进行考虑<br>一：如何最小化预测框和目标框之间的归一化距离？<br>二：如何在预测框和目标框重叠时，回归的更准确？<br>针对第一个问题，提出了DIOU_Loss（Distance_IOU_Loss）<br><img src="https://img-blog.csdnimg.cn/img_convert/f114b3726da623a4bb6dfcc151fc3d61.png" alt="avatar"><br>DIOU_Loss考虑了重叠面积和中心点距离，当目标框包裹预测框的时候，直接度量2个框的距离，因此DIOU_Loss收敛的更快。</p><p>但就像前面好的目标框回归函数所说的，没有考虑到长宽比。<br><img src="https://img-blog.csdnimg.cn/img_convert/086fb149562f4574145a9334a6eb9d49.png" alt="avatar"><br>比如上面三种情况，目标框包裹预测框，本来DIOU_Loss可以起作用。<br>但预测框的中心点的位置都是一样的，因此按照DIOU_Loss的计算公式，三者的值都是相同的。<br>针对这个问题，又提出了CIOU_Loss，不对不说，科学总是在解决问题中，不断进步！！  </p><p>d.CIOU_Loss<br>CIOU_Loss和DIOU_Loss前面的公式都是一样的，不过在此基础上还增加了一个影响因子，将预测框和目标框的长宽比都考虑了进去。<br><img src="https://img-blog.csdnimg.cn/img_convert/aec09181721c0fc8f7efd1d53f065a2b.png" alt="avatar"><br>人类好强，这式子怎么推导出来的啊，我跪了<br>其中v是衡量长宽比一致性的参数。<br>这样CIOU_Loss就将目标框回归函数应该考虑三个重要几何因素：重叠面积、中心点距离，长宽比全都考虑进去了。<br>再来综合的看下各个Loss函数的不同点：<br>IOU_Loss：主要考虑检测框和目标框重叠面积。<br>GIOU_Loss：在IOU的基础上，解决边界框不重合时的问题。<br>DIOU_Loss：在IOU和GIOU的基础上，考虑边界框中心点距离的信息。<br>CIOU_Loss：在DIOU的基础上，考虑边界框宽高比的尺度信息。<br>Yolov4中采用了CIOU_Loss的回归方式，使得预测框回归的速度和精度更高一些。  </p><p>（2）DIOU_nms  </p><p>Nms主要用于预测框的筛选，常用的目标检测算法中，一般采用普通的nms的方式。   </p><p>问题思考：<br>1.这里为什么不用CIOU_nms，而用DIOU_nms?</p><p>答：因为前面讲到的CIOU_loss，是在DIOU_loss的基础上，添加的影响因子，包含groundtruth标注框的信息，在训练时用于回归。但在测试过程中，并没有groundtruth的信息，不用考虑影响因子，因此直接用DIOU_nms即可。<br>总体来说，YOLOv4的论文称的上良心之作，将近几年关于深度学习领域最新研究的tricks移植到Yolov4中做验证测试，将Yolov3的精度提高了不少。虽然没有全新的创新，但很多改进之处都值得借鉴，借用Yolov4作者的总结。<br>Yolov4 主要带来了 3 点新贡献：<br>（1）提出了一种高效而强大的目标检测模型，使用 1080Ti 或 2080Ti 就能训练出超快、准确的目标检测器。<br>（2）在检测器训练过程中，验证了最先进的一些研究成果对目标检测器的影响。<br>（3）改进了 SOTA 方法，使其更有效、更适合单 GPU 训练。<br>（2）特价包  </p><p>增大感受野技巧：SPP、ASPP、RFB  </p><p>SPP模块:<br>源于空间金字塔匹配（SPM），SPMs最初的方法是将特征映射分成若干个d×d相等的块，其中d可以是{1,2,3，…}，从而形成空间金字塔，然后提取bag-of-word的特征。SPP将SPM集成到CNN中，并使用最大池化操作代替bag-of-word操作。<br>在YOLOv3的设计中，Redmon和Farhadi将SPP模块改进为最大池输出与核大小k×k的串联，其中k&#x3D;{1,5,9,13}，步长等于1。在这种设计下，一个较大的k×k最大池化有效地增加了主干特征的感受野。在添加了SPP模块的改进版本后，YOLOv3-608在MS-COCO目标检测任务上将AP50 升级了2.7%，但增加了0.5%的计算量。<br>ASPP模块：<br>与改进的SPP模块在运算上的差异主要是源于原始的k×k核大小，在扩张卷积运算中，最大池化的步幅等于1到几个3×3核大小，扩张比等于k，步长等于1。<br>RFB模块：<br>是利用k×k核的几个展开卷积，展开比为k，步长为1，得到比ASPP更全面的空间覆盖。RFB仅需7%的额外推理时间，即可将COCO上的SSD的AP50提高5.7%。</p><p>注意力机制：Squeeze-and-Excitation (SE)、Spatial Attention Module (SAM)<br>（目标检测中常用的注意模块主要分为通道式注意和点式注意。这两种注意模型的代表分别是：挤压与激励（SE）和 空间注意模块（SAM））<br>SE模块：<br>在ImageNet图像分类任务中可以将ResNet50的能力提高1%的top1精度。但在GPU上通常会增加10%左右的推理时间，因此更适合用于移动设备。</p><p>SAM模块：<br>只需要额外消耗0.1%的计算量，就可以提高resnet50se在ImageNet图像分类任务中的0.5%的top1精度。最重要的是，它根本不影响GPU上的推理速度。  </p><p>特征融合集成：FPN、SFAM、ASFF、BiFPN （出自于大名鼎鼎的EfficientDet）  </p><p>（在特征集成方面，早期的方法是使用跳转连接（skip connection）或超列（hyper-column ）将低级物理特征集成到高级语义特征。 随着FPN等多尺度预测方法的广泛应用，人们提出了许多集成不同特征金字塔的轻量级模块。这类模块包括SFAM、ASFF和BiFPN。）  </p><p>SFAM：主要思想是利用SE模块对多尺度级联特征映射进行通道级加权。<br>ASFF：它使用softmax作为逐点加权，然后添加不同比例尺的特征地图。<br>BiFPN：提出了多输入加权残差连接进行尺度层次加权，然后加入不同尺度的特征映射  </p><p>更好的激活函数：ReLU、LReLU、PReLU、ReLU6、SELU、Swish、hard-Swish  </p><p>当梯度小于零的时候，首要的目的是解梯度小于零的问题。至于ReLU6和hard Swish，它们是专门为量化网络设计的。对于神经网络的自规范化，提出了SELU激活函数来满足这一目标。需要注意的是Swish和Mish都是连续可微的激活函数。基于深度学习的目标检测中常用的后处理方法是非极大抑制（NMS），它可以用来过滤那些对同一目标预测不好的Bounding box，只保留响应较高的候选Bounding box，NMS试图改进的方法与优化目标函数的方法是一致的。</p><p>后处理非极大值抑制算法：soft-NMS、DIoU NMS  </p><h3 id="4-3-6-改进方法"><a href="#4-3-6-改进方法" class="headerlink" title="4.3.6 改进方法"></a>4.3.6 改进方法</h3><p>（1）Mosaic：<br>这是作者提出的一种新的数据增强方法，该方法借鉴了CutMix数据增强方式的思想。CutMix数据增强方式利用两张图片进行拼接，但是Mosaic使利用四张图片进行拼接。Mosaic数据增强方法有一个优点：拥有丰富检测目标的背景，并且在BN计算的时候一次性会处理四张图片。<br>（2）SAT<br>SAT是一种自对抗训练数据增强方法，这一种新的对抗性训练方式。在第一阶段，神经网络改变原始图像而不改变网络权值。以这种方式，神经网络对自身进行对抗性攻击，改变原始图像，以制造图像上没有所需对象的欺骗。在第二阶段，用正常的方法训练神经网络去检测目标。<br>（3）CmBN<br>CmBN的全称是Cross mini-Batch Normalization，定义为跨小批量标准化（CmBN）。CmBN 是 CBN 的改进版本，它用来收集一个batch内多个mini-batch内的统计数据。<br>（４）修改过的SAM<br>作者在原SAM（Spatial Attention Module）方法上进行了修改，将SAM从空间注意修改为点注意。如下图所示，对于常规的SAM，最大值池化层和平均池化层分别作用于输入的feature map，得到两组shape相同的feature map，再将结果输入到一个卷积层，接着是一个 Sigmoid 函数来创建空间注意力。<br><img src="https://img-blog.csdnimg.cn/20200729172700153.png" alt="avatar"><br>将SAM（Spatial Attention Module）应用于输入特征，能够输出精细的特征图。<br><img src="https://img-blog.csdnimg.cn/20200729172909382.jpg" alt="avatar"><br>在YOLOv4中，对原来的SAM方法进行了修改。如下图所示，修改后的SAM直接使用一个卷积层作用于输入特征，得到输出特征，然后再使用一个Sigmoid 函数来创建注意力。作者认为，采用这种方式创建的是点注意力。<br><img src="https://img-blog.csdnimg.cn/20200729173158239.png" alt="avatar"><br>（５）修改过的PAN<br>作者对原PAN(Path Aggregation Network)方法进行了修改, 使用张量连接(concat)代替了原来的快捷连接(shortcut connection)。如下图所示：<br><img src="https://img-blog.csdnimg.cn/20200730003639808.png" alt="avatar">   </p><h1 id="5-YOLO-v5"><a href="#5-YOLO-v5" class="headerlink" title="5. YOLO v5"></a>5. YOLO v5</h1><h2 id="5-1-基本概念"><a href="#5-1-基本概念" class="headerlink" title="5.1 基本概念"></a>5.1 基本概念</h2><p>YOLOv5是一个在COCO数据集上预训练的物体检测架构和模型系列，它代表了Ultralytics对未来视觉AI方法的开源研究，其中包含了经过数千小时的研究和开发而形成的经验教训和最佳实践。<br>YOLOv5是YOLO系列的一个延申，也可以看作是基于YOLOv3、YOLOv4的改进作品。YOLOv5没有相应的论文说明，但是作者在Github上积极地开放源代码。  </p><h2 id="5-2-基本框架"><a href="#5-2-基本框架" class="headerlink" title="5.2 基本框架"></a>5.2 基本框架</h2><p><img src="https://img-blog.csdnimg.cn/img_convert/4af4b621a93a6811a6a69308d71284b6.png" alt="avatar"><br>由上图可知，YOLO v5主要由输入端、Backone、Neck以及Prediction四部分组成。其中：<br>(1) Backbone：在不同图像细粒度上聚合并形成图像特征的卷积神经网络。<br>(2) Neck：一系列混合和组合图像特征的网络层，并将图像特征传递到预测层。<br>(3) Head： 对图像特征进行预测，生成边界框和并预测类别。<br>下面介绍YOLO v5各部分网络包括的基础组件：<br>CBL：由Conv+BN+Leaky_relu激活函数组成<br>Res unit：借鉴ResNet网络中的残差结构，用来构建深层网络<br>CSP1_X：借鉴CSPNet网络结构，该模块由CBL模块、Res unint模块以及卷积层、Concate组成<br>CSP2_X：借鉴CSPNet网络结构，该模块由卷积层和X个Res unint模块Concate组成而成<br>Focus：首先将多个slice结果Concat起来，然后将其送入CBL模块中<br>SPP：采用1×1、5×5、9×9和13×13的最大池化方式，进行多尺度特征融合  </p><h2 id="5-3-框架详解"><a href="#5-3-框架详解" class="headerlink" title="5.3 框架详解"></a>5.3 框架详解</h2><h3 id="5-3-1-输入端详解"><a href="#5-3-1-输入端详解" class="headerlink" title="5.3.1 输入端详解"></a>5.3.1 输入端详解</h3><p>YOLO v5使用Mosaic数据增强操作提升模型的训练速度和网络的精度；并提出了一种自适应锚框计算与自适应图片缩放方法  </p><h3 id="5-3-1-1-Mosaic数据增强"><a href="#5-3-1-1-Mosaic数据增强" class="headerlink" title="5.3.1.1 Mosaic数据增强"></a>5.3.1.1 Mosaic数据增强</h3><p>Mosaic数据增强利用四张图片，并且按照随机缩放、随机裁剪和随机排布的方式对四张图片进行拼接，每一张图片都有其对应的框，将四张图片拼接之后就获得一张新的图片，同时也获得这张图片对应的框，然后我们将这样一张新的图片传入到神经网络当中去学习，相当于一下子传入四张图片进行学习了。该方法极大地丰富了检测物体的背景，且在标准化BN计算的时候一下子计算四张图片的数据，所以本身对batch size不是很依赖。（和v4一样，不要大惊小怪）</p><h2 id="5-3-1-2-自适应锚框计算"><a href="#5-3-1-2-自适应锚框计算" class="headerlink" title="5.3.1.2 自适应锚框计算"></a>5.3.1.2 自适应锚框计算</h2><p>在yolo系列算法中，针对不同的数据集，都需要设定特定长宽的锚点框。在网络训练阶段，模型在初始阶段，模型在初始锚点框的基础上输出对应的预测框，计算其与GT框之间的差距，并执行反向更新操作，从而更新整个网络的参数，因此设定初始锚点框是比较关键的一环。<br>在yolo V3和yolo V4中，训练不同的数据集，都是通过单独的程序运行来获得初始锚点框。<br>而在yoloV5中将此功能嵌入到代码中，每次训练，根据数据集的名称自适应的计算出最佳的锚点框，用户可以根据自己的需求将功能关闭或者打开，指令为：<br><img src="https://img-blog.csdnimg.cn/img_convert/8fbfc6822901565729df4ffc08bfaeca.png" alt="avatar">  </p><h2 id="5-3-1-3-自适应图片缩放"><a href="#5-3-1-3-自适应图片缩放" class="headerlink" title="5.3.1.3 自适应图片缩放"></a>5.3.1.3 自适应图片缩放</h2><p>在目标检测算法中，不同的图片长宽都不相同，因此常用的方式是将原始图片统一缩放到一个标准尺寸，再送入检测网络中。而原始的缩放方法存在着一些问题，由于在实际的使用中的很多图片的长宽比不同，因此缩放填充之后，两端的黑边大小都不相同，然而如果填充的过多，则会存在大量的信息冗余，从而影响整个算法的推理速度。为了进一步提升YOLO v5的推理速度，该算法提出一种方法能够自适应的添加最少的黑边到缩放之后的图片中。具体的实现步骤如下所述：</p><p>(1) 根据原始图片大小以及输入到网络的图片大小计算缩放比例<br>如：800x600照片缩放成416x416<br>a.416&#x2F;800&#x3D;0.52 416&#x2F;600&#x3D;0.69<br>b.根据原始图片大小与缩放比例计算缩放后的图片大小<br>800x0.52&#x3D;416 600x0.52&#x3D;312<br>c.计算黑边填充数值<br>416-312&#x3D;104 np.mod(104,32)&#x3D;8 8&#x2F;2&#x3D;4<br>其中，416表示YOLO v5网络所要求的图片宽度，312表示缩放后图片的宽度。首先执行相减操作来获得需要填充的黑边长度104；然后对该数值执行取余操作，即104取余32&#x3D;8，使用32是因为整个YOLOv5网络执行了5次下采样操作。最后对该数值除以2，即将填充的区域分散到两边。这样将416<em>416大小的图片缩小到416</em>320大小，因而极大的提升了算法的推理速度。  </p><h3 id="5-3-2-Backone网络"><a href="#5-3-2-Backone网络" class="headerlink" title="5.3.2 Backone网络"></a>5.3.2 Backone网络</h3><h4 id="5-3-2-1-Focus结构"><a href="#5-3-2-1-Focus结构" class="headerlink" title="5.3.2.1 Focus结构"></a>5.3.2.1 Focus结构</h4><p>Focus对图片进行切片操作，具体操作是在一张图片中每隔一个像素拿到一个值，类似于邻近下采样，这样就拿到了四张图片，四张图片互补，长的差不多，但是没有信息丢失，这样一来，将W、H信息就集中到了通道空间，输入通道扩充了4倍，即拼接起来的图片相对于原先的RGB三通道模式变成了12个通道，最后将得到的新图片再经过卷积操作，最终得到了没有信息丢失情况下的二倍下采样特征图。如下图所示，原始输入图片大小为608<em>608</em>3，经过Slice与Concat操作之后输出一个304<em>304</em>12的特征映射；接着经过一个通道个数为32的Conv层，输出一个304<em>304</em>32大小的特征映射。(图二解释：Focus重要的是切片操作，4x4x3的图像切片后变成2x2x12的特征图,，原始608x608x3的图像输入Focus结构，采用切片操作，先变成304x304x12的特征图，再经过一次32个卷积核的卷积操作，最终变成304x304x32的特征图。)<br><img src="https://img-blog.csdnimg.cn/20210212091942352.jpg" alt="avatar"><br><img src="https://img-blog.csdnimg.cn/7fda44666b5c474ba9287e856a11ab74.png#pic_center" alt="avatar"></p><h3 id="5-3-2-2-CSP结构"><a href="#5-3-2-2-CSP结构" class="headerlink" title="5.3.2.2 CSP结构"></a>5.3.2.2 CSP结构</h3><p>CSPNet主要是将feature map拆成两个部分，一部分进行卷积操作，另一部分和上一部分卷积操作的结果进行concate。在分类问题中，使用CSPNet可以降低计算量，但是准确率提升很小；<br>在目标检测问题中，使用CSPNet作为Backbone带来的提升比较大，可以有效增强CNN的学习能力，同时也降低了计算量。yolo V5设计了两种CSP结构，CSP1_X结构应用于Backbone网络中，CSP2_X结构应用于Neck网络中。  </p><h3 id="5-3-2-3-Neck网络"><a href="#5-3-2-3-Neck网络" class="headerlink" title="5.3.2.3 Neck网络"></a>5.3.2.3 Neck网络</h3><p>在YOLO v4中开始使用FPN-PAN。FPN层自顶向下传达强语义特征，而PAN塔自底向上传达定位特征.<br>yoloV5的Neck仍采用了FPN+PAN结构，但是在它的基础上做了一些改进操作，yoloV4的Neck结构中，采用的都是普通的卷积操作，而yoloV5的Neck中，采用CSPNet设计的CSP2结构，从而加强了网络特征融合能力。(和v4一样啊啊啊啊啊啊)  </p><h3 id="5-3-2-4-Head网络"><a href="#5-3-2-4-Head网络" class="headerlink" title="5.3.2.4 Head网络"></a>5.3.2.4 Head网络</h3><p>YOLO v5采用CIOU_LOSS 作为bounding box 的损失函数。V4中已经对IOU_Loss、GIOU_Loss、DIOU_Loss以及CIOU_Loss介绍完了，向上翻。  </p><h1 id="6-YOLO-v6"><a href="#6-YOLO-v6" class="headerlink" title="6. YOLO v6"></a>6. YOLO v6</h1><h2 id="6-1-基本概念"><a href="#6-1-基本概念" class="headerlink" title="6.1 基本概念"></a>6.1 基本概念</h2><p>YOLO v6框架支持模型训练、推理及多平台部署等全链条的工业应用需求，并在网络结构、训练策略等算法层面进行了多项改进和优化，在 COCO 数据集上，YOLOv6 在精度和速度方面均超越其他同体量算法。</p><h2 id="6-2-基本框架"><a href="#6-2-基本框架" class="headerlink" title="6.2 基本框架"></a>6.2 基本框架</h2><p>YOLOv6 主要在 Backbone、Neck、Head 以及训练策略等方面进行了诸多的改进：  </p><p>（1）统一设计了更高效的 Backbone 和 Neck ：受到硬件感知神经网络设计思想的启发，基于 RepVGG style设计了可重参数化、更高效的骨干网络 EfficientRep Backbone 和 Rep-PAN Neck。  </p><p>（2）优化设计了更简洁有效的 Efficient Decoupled Head，在维持精度的同时，进一步降低了一般解耦头带来的额外延时开销。  </p><p>（3）在训练策略上，我们采用Anchor-free 无锚范式，同时辅以 SimOTA 标签分配策略以及 SIoU边界框回归损失来进一步提高检测精度。  </p><h3 id="6-2-1-Hardware-friendly-的骨干网络设计"><a href="#6-2-1-Hardware-friendly-的骨干网络设计" class="headerlink" title="6.2.1 Hardware-friendly 的骨干网络设计"></a>6.2.1 Hardware-friendly 的骨干网络设计</h3><p>YOLOv5&#x2F;YOLOX 使用的 Backbone 和 Neck 都基于 CSPNet搭建，采用了多分支的方式和残差结构。对于 GPU 等硬件来说，这种结构会一定程度上增加延时，同时减小内存带宽利用率。  </p><p>基于硬件感知神经网络设计的思想，对 Backbone 和 Neck 进行了重新设计和优化。对上述重新设计的两个检测部件，我们在 YOLOv6 中分别称为 EfficientRep Backbone 和 Rep-PAN Neck，其主要贡献点在于：引入了 RepVGGstyle 结构。基于硬件感知思想重新设计了 Backbone 和 Neck。</p><p>RepVGG Style 结构是一种在训练时具有多分支拓扑，而在实际部署时可以等效融合为单个 3x3 卷积的一种可重参数化的结构。通过融合成的 3x3 卷积结构，可以有效利用计算密集型硬件计算能力（比如 GPU）。</p><p>通过上述策略，YOLOv6 减少了在硬件上的延时，并显著提升了算法的精度，让检测网络更快更强。以 nano 尺寸模型为例，对比 YOLOv5-nano 采用的网络结构，本方法在速度上提升，同时精度提升。<br><img src="https://img-blog.csdnimg.cn/img_convert/968c775928fdc160dce740aab3b9ce33.png" alt="avatar"><br>EfficientRep Backbone：在 Backbone 设计方面，基于以上 Rep 算子设计了一个高效的Backbone。相比于 YOLOv5 采用的 CSP-Backbone，该Backbone 能够高效利用硬件（如 GPU）算力的同时，还具有较强的表征能力。</p><p>下图 4 为 EfficientRep Backbone 具体设计结构图，我们将 Backbone 中 stride&#x3D;2 的普通 Conv 层替换成了 stride&#x3D;2 的 RepConv层。同时，将原始的 CSP-Block 都重新设计为 RepBlock，其中 RepBlock 的第一个 RepConv 会做 channel 维度的变换和对齐。另外，我们还将原始的 SPPF 优化设计为更加高效的 SimSPPF。<br><img src="https://img-blog.csdnimg.cn/img_convert/fd3b9eae4967a4e2c01a0edae4e0fe9d.png" alt="avatar"><br>Rep-PAN：在 Neck 设计方面，为了让其在硬件上推理更加高效，以达到更好的精度与速度的平衡，我们基于硬件感知神经网络设计思想，为 YOLOv6 设计了一个更有效的特征融合网络结构。</p><p>Rep-PAN 基于 PAN[6] 拓扑方式，用 RepBlock 替换了 YOLOv5 中使用的 CSP-Block，同时对整体 Neck 中的算子进行了调整，目的是在硬件上达到高效推理的同时，保持较好的多尺度特征融合能力（Rep-PAN 结构图如下图 5 所示）。<br><img src="https://img-blog.csdnimg.cn/img_convert/6c88fd43f84d634e2a86fe4774a9ce7f.png" alt="avatar">  </p><h3 id="6-2-2-更简洁高效的-Decoupled-Head"><a href="#6-2-2-更简洁高效的-Decoupled-Head" class="headerlink" title="6.2.2 更简洁高效的 Decoupled Head"></a>6.2.2 更简洁高效的 Decoupled Head</h3><p>在 YOLOv6 中，采用了解耦检测头（Decoupled Head）结构，并对其进行了精简设计。原始 YOLOv5 的检测头是通过分类和回归分支融合共享的方式来实现的。</p><p>因此，对解耦头进行了精简设计，同时综合考虑到相关算子表征能力和硬件上计算开销这两者的平衡，采用 Hybrid Channels 策略重新设计了一个更高效的解耦头结构，在维持精度的同时降低了延时，缓解了解耦头中 3x3 卷积带来的额外延时开销。通过在 nano 尺寸模型上进行消融实验，对比相同通道数的解耦头结构，精度提升的同时，速度提升。<br><img src="https://img-blog.csdnimg.cn/img_convert/52103b3dfc018e6877b189b8bf9da48e.png" alt="avatar">  </p><h3 id="6-2-3-更有效的训练策略"><a href="#6-2-3-更有效的训练策略" class="headerlink" title="6.2.3 更有效的训练策略"></a>6.2.3 更有效的训练策略</h3><p>为了进一步提升检测精度，我们吸收借鉴了学术界和业界其他检测框架的先进研究进展：Anchor-free 无锚范式 、SimOTA 标签分配策略以及 SIoU 边界框回归损失。<br>(1)Anchor-free 无锚范式  </p><p>YOLOv6 采用了更简洁的 Anchor-free 检测方法。由于 Anchor-based检测器需要在训练之前进行聚类分析以确定最佳 Anchor 集合，这会一定程度提高检测器的复杂度；同时，在一些边缘端的应用中，需要在硬件之间搬运大量检测结果的步骤，也会带来额外的延时。而 Anchor-free 无锚范式因其泛化能力强，解码逻辑更简单，在近几年中应用比较广泛。经过对 Anchor-free 的实验调研，我们发现，相较于Anchor-based 检测器的复杂度而带来的额外延时，Anchor-free 检测器在速度上有51%的提升。  </p><p>(2)SimOTA 标签分配策略  </p><p>为了获得更多高质量的正样本，YOLOv6 引入了 SimOTA 算法动态分配正样本，进一步提高检测精度。YOLOv5 的标签分配策略是基于 Shape 匹配，并通过跨网格匹配策略增加正样本数量，从而使得网络快速收敛，但是该方法属于静态分配方法，并不会随着网络训练的过程而调整。  </p><p>(3)SIoU 边界框回归损失  </p><p>为了进一步提升回归精度，YOLOv6 采用了 SIoU边界框回归损失函数来监督网络的学习。目标检测网络的训练一般需要至少定义两个损失函数：分类损失和边界框回归损失，而损失函数的定义往往对检测精度以及训练速度产生较大的影响。  </p><p>近年来，常用的边界框回归损失包括IoU、GIoU、CIoU、DIoU loss等等，这些损失函数通过考虑预测框与目标框之前的重叠程度、中心点距离、纵横比等因素来衡量两者之间的差距，从而指导网络最小化损失以提升回归精度，但是这些方法都没有考虑到预测框与目标框之间方向的匹配性。SIoU 损失函数通过引入了所需回归之间的向量角度，重新定义了距离损失，有效降低了回归的自由度，加快网络收敛，进一步提升了回归精度。通过在 YOLOv6s 上采用 SIoU loss 进行实验，对比 CIoU loss，平均检测精度提升  </p><h1 id="7-YOLO-v7"><a href="#7-YOLO-v7" class="headerlink" title="7. YOLO v7"></a>7. YOLO v7</h1><h2 id="7-1-基本概念"><a href="#7-1-基本概念" class="headerlink" title="7.1 基本概念"></a>7.1 基本概念</h2><p>YOLOv4的团队刚出炉的最新工作- YOLOv7，就是一个字——新鲜，啊，是两个字。。。  </p><h2 id="7-2-框架"><a href="#7-2-框架" class="headerlink" title="7.2 框架"></a>7.2 框架</h2><p><img src="https://img-blog.csdnimg.cn/eecd9a030a7e488eaa2a9ec981f7e6e6.png#pic_center" alt="avatar"><br>我们先整体来看下 YOLOV7，首先对输入的图片 resize 为 640x640 大小，输入到 backbone 网络中，然后经 head 层网络输出三层不同 size 大小的 feature map，经过 Rep 和 conv输出预测结果，这里以 coco 为例子，输出为 80 个类别，然后每个输出(x ,y, w, h, o) 即坐标位置和前后背景，3 是指的 anchor 数量，因此每一层的输出为 (80+5)x3 &#x3D; 255再乘上 feature map 的大小就是最终的输出</p><h3 id="7-2-1-扩展的高效层聚合网络"><a href="#7-2-1-扩展的高效层聚合网络" class="headerlink" title="7.2.1 扩展的高效层聚合网络"></a>7.2.1 扩展的高效层聚合网络</h3><p>在大多数关于设计高效架构的文献中，主要考虑因素不超过参数的数量、计算量和计算密度。Ma 等人还从内存访问成本的特点出发，分析了输入&#x2F;输出通道比、架构的分支数量以及 element-wise 操作对网络推理速度的影响。多尔阿尔等人在执行模型缩放时还考虑了激活，即更多地考虑卷积层输出张量中的元素数量。<br><img src="https://img-blog.csdnimg.cn/627addfdb10148a99e78e6cf01f3b49d.png" alt="avatar"><br>图 2（b）中 CSPVoVNet 的设计是 VoVNet 的一种变体。CSPVoVNet 的架构除了考虑上述基本设计问题外，还分析了梯度路径，以使不同层的权重能够学习到更多样化的特征。上述梯度分析方法使推理更快、更准确。<br>图 2 (c) 中的 ELAN 考虑了以下设计策略——“如何设计一个高效的网络？”。他们得出了一个结论：通过控制最短最长的梯度路径，更深的网络可以有效地学习和收敛。  </p><p>在本文中，作者提出了基于 ELAN 的Extended-ELAN (E-ELAN)，其主要架构如图 2（d）所示。  </p><p>无论梯度路径长度和大规模 ELAN 中计算块的堆叠数量如何，它都达到了稳定状态。如果无限堆叠更多的计算块，可能会破坏这种稳定状态，参数利用率会降低。作者提出的E-ELAN使用expand、shuffle、merge cardinality来实现在不破坏原有梯度路径的情况下不断增强网络学习能力的能力。  </p><p>在架构方面，E-ELAN 只改变了计算块的架构，而过渡层的架构完全没有改变。策略是使用组卷积来扩展计算块的通道和基数。将对计算层的所有计算块应用相同的组参数和通道乘数。然后，每个计算块计算出的特征图会根据设置的组参数g被打乱成g个组，然后将它们连接在一起。此时，每组特征图的通道数将与原始架构中的通道数相同。最后，添加 g 组特征图来执行合并基数。E-ELAN除了保持原有的ELAN设计架构外，还可以引导不同组的计算块学习更多样化的特征。  </p><h3 id="7-2-2-基于concatenate模型的模型缩放"><a href="#7-2-2-基于concatenate模型的模型缩放" class="headerlink" title="7.2.2 基于concatenate模型的模型缩放"></a>7.2.2 基于concatenate模型的模型缩放</h3><p>模型缩放的主要目的是调整模型的一些属性，生成不同尺度的模型，以满足不同推理速度的需求。例如，EfficientNet的缩放模型考虑了宽度、深度和分辨率。对于Scale-yolov4，其缩放模型是调整阶段数。Doll‘ar等人分析了卷积和群卷积对参数量和计算量的影响，并据此设计了相应的模型缩放方法。<br><img src="https://img-blog.csdnimg.cn/046bbb8f1b8a49cc80878f7356d71c9d.png" alt="avatar"><br>上述方法主要用于诸如PlainNet或ResNet等架构中。当这些架构在执行放大或缩小过程时，每一层的in-degree和out-degree都不会发生变化，因此可以独立分析每个缩放因子对参数量和计算量的影响。然而，如果这些方法应用于基于concatenate的架构时会发现当扩大或缩小执行深度，基于concatenate的转换层计算块将减少或增加，如图3(a)和(b).所示</p><p>从上述现象可以推断，对于基于concatenate的模型不能单独分析不同的缩放因子，而必须一起考虑。以scaling-up depth为例，这样的动作会导致transition layer的输入通道和输出通道的比例发生变化，这可能会导致模型的硬件使用率下降。</p><p>因此，必须为基于concatenate的模型提出相应的复合模型缩放方法。当缩放一个计算块的深度因子时，还必须计算该块的输出通道的变化。然后，将对过渡层进行等量变化的宽度因子缩放，结果如图3（c）所示。本文提出的复合缩放方法可以保持模型在初始设计时的特性并保持最佳结构。  </p><h2 id="7-3-训练方法"><a href="#7-3-训练方法" class="headerlink" title="7.3 训练方法"></a>7.3 训练方法</h2><h3 id="7-3-1-Planned-re-parameterized-convolution"><a href="#7-3-1-Planned-re-parameterized-convolution" class="headerlink" title="7.3.1 Planned re-parameterized convolution"></a>7.3.1 Planned re-parameterized convolution</h3><p>尽管RepConv在VGG基础上取得了优异的性能，但当将它直接应用于ResNet、DenseNet和其他架构时，它的精度将显著降低。作者使用梯度流传播路径来分析重参数化的卷积应该如何与不同的网络相结合。作者还相应地设计了计划中的重参数化的卷积。  </p><p>RepConv实际上结合了3×3卷积，1×1卷积，和在一个卷积层中的id连接。通过分析RepConv与不同架构的组合及其性能，作者发现RepConv中的id连接破坏了ResNet中的残差和DenseNet中的连接，为不同的特征图提供了更多的梯度多样性。<br><img src="https://img-blog.csdnimg.cn/b5ab930a3e7641b7aef3b72fe42f4582.png" alt="avatar"><br>基于上述原因，作者使用没有id连接的RepConv(RepConvN)来设计计划中的重参数化卷积的体系结构。在作者的思维中，当具有残差或连接的卷积层被重新参数化的卷积所取代时，不应该存在id连接。图4显示了在PlainNet和ResNet中使用的“Planned re-parameterized convolution”的一个示例。对于基于残差的模型和基于concatenate的模型中Planned re-parameterized convolution实验，它将在消融研究环节中提出。  </p><h3 id="7-3-2-标签匹配"><a href="#7-3-2-标签匹配" class="headerlink" title="7.3.2 标签匹配"></a>7.3.2 标签匹配</h3><p>深度监督是一种常用于训练深度网络的技术。其主要概念是在网络的中间层增加额外的auxiliary Head，以及以auxiliary损失为导向的浅层网络权值。即使对于像ResNet和DenseNet这样通常收敛得很好的体系结构，深度监督仍然可以显著提高模型在许多任务上的性能。图5(a)和(b)分别显示了“没有”和“有”深度监督的目标检测器架构。在本文中，将负责最终输出的Head为lead Head，将用于辅助训练的Head称为auxiliary Head。  </p><p>过去，在深度网络的训练中，标签分配通常直接指GT，并根据给定的规则生成硬标签。然而，近年来，如果以目标检测为例，研究者经常利用网络预测输出的质量和分布，然后结合GT考虑，使用一些计算和优化方法来生成可靠的软标签。例如，YOLO使用边界框回归预测和GT的IoU作为客观性的软标签。在本文中，将网络预测结果与GT一起考虑，然后将软标签分配为“label assigner”的机制。<br><img src="https://img-blog.csdnimg.cn/e7097071ffaf4dc08f8c483e1b858222.png" alt="avatar"><br>这里应该算是本文除过E-ELAN第二个贡献很大的地方了。<br>作者使用深度监督思路，如图5的（a）-&gt; (b)， 在网络的中间层添加了额外的辅助头结构（auxiliary head），以此作为辅助损失（assistant loss）来给浅层网络权重提供指导。即网络有auxiliary head和最终的lead head两个头结构。<br>所以针对此模式还尚无专门的标签匹配方法，作者说常用的就是( c )这种给两个头分别安排标签匹配工作。</p><p>这里作者先介绍了之前的标签匹配有hard label和soft label两种，其中hard label就是根据固定规则设置正负样本（比如使用iou阈值）。而soft label的生成 就是动态标签匹配，即根据预测输出的质量和分布与ground truth ，使用一些计算和优化方法来生成soft label（比如TAL, OTA, ASSD, SimOTA，YOLO使用BBOX的预测和真值的IOU作为置信度label）。作者将soft label的生成称作“ label assigner”。  </p><p>文章中作者提出了图5（d）Lead head guided label assigner 和（e）Coarse-to-fine lead head guided label assigner 两种思路：  </p><p>Lead head guided label assigner ：使用lead-head的预测值和真值通过优化过程生成soft label, soft label 用于auxiliary head 和lead-head, 作者将这个学习过程成为一种广义残差学习，通过让更浅层的auxiliary head 直接学习lead-head已经学习到的信息，lead-head将能够更集中的学习还为学习过的residual 信息。   </p><p>Coarse-to-fine lead head guided label assigner ： 生成两种soft label， coarse label 和fine label. 其中fine label生成方式和(d）中相同，而coarse label 则通过扩充正样本（更多网格被视为positive target），从而放宽正样本匹配过程，得到更多的正样本。目的是为了让auxiliary head 更专注recall 的优化。由于有些粗标签和细标签的额外权重接近，会造成不好的最终预测结果，所以作者在decoder增加了限制，使coarse positive grid 不能很好的生成soft label。  </p><p>作者提的两种思路也仅仅是两种思路，具体的soft label assignment 使用的是什么优化策略，以及loss等都没有介绍，文章说都在appendix中，可惜附录文件也没看到。代码里应该有。  </p><h3 id="7-3-3-其他Tricks"><a href="#7-3-3-其他Tricks" class="headerlink" title="7.3.3 其他Tricks"></a>7.3.3 其他Tricks</h3><p>这些免费的训练细节将在附录中详细说明，包括：<br>(1) conv-bn-activation topology中的Batch normalization：这部分主要将batch normalization layer直接连接到卷积层。这样做的目的是在推理阶段将批归一化的均值和方差整合到卷积层的偏差和权重中。  </p><p>简单理解：其实就是在推理阶段将BN和BN前的Conv融合，得到一个Conv。</p><p>(2) 隐性知识在YOLOR中结合卷积特征图的加法和乘法方式：YOLOR中的隐式知识可以在推理阶段通过预计算简化为向量。该向量可以与前一个或后一个卷积层的偏差和权重相结合。  </p><p>(3) EMA 模型（Exponential Moving Average）指数移动平均值：EMA 是一种在 mean teacher 中使用的技术，在系统中使用 EMA 模型纯粹作为最终的推理模型。  </p><h2 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h2><p>这里补充一下关于soft label的详细选取方法：采用yolo v5正负样本匹配法（hard label）+simOTA(soft label， 源自YOLOX)的组合方式<br>流程如下：<br>①使用yolov5正负样本分配策略分配正样本。（YOLO v5）<br>②计算每个样本对每个GT的Reg+Cla loss（Loss aware）（simOTA）<br>③使用每个GT的预测样本确定它需要分配到的正样本数（Dynamic k） （simOTA）<br>3.1 先获取和GT的IOU前10的样本，<br>3.2 然后对10个IOU求和取整，作为当前GT的Dynamic k，至少为1<br>3.3 10这个数字不敏感，5~15都可以取<br>④为每个GT取loss最小的前dynamic k个样本作为正样本 （simOTA）<br>⑤人工去掉同一个样本被分配到多个GT的正样本的情况（全局信息） （simOTA）  </p><p>YOLO v7的缺点应该是参数量比较大，不适合CPU等和一些低算力NPU，不过作者也说了他面向的对象是各种GPU。我感觉对于一些算力好的NPU应该效果也会很好，毕竟36.9也不是很大。  </p><h1 id="8-参考文献与文章"><a href="#8-参考文献与文章" class="headerlink" title="8. 参考文献与文章"></a>8. 参考文献与文章</h1><p>1.<a href="https://arxiv.org/pdf/1506.02640.pdf">YOLO v1论文</a><br>2.<a href="https://github.com/AlexeyAB/darknet">YOLO v2论文</a><br>3.<a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf">YOLO v3论文</a><br>4.<a href="https://arxiv.org/pdf/2004.10934.pdf">YOLO v4论文</a><br>5.<a href="https://tech.meituan.com/2022/06/23/yolov6-a-fast-and-accurate-target-detection-framework-is-opening-source.html">YOLO v6论文</a><br>6.<a href="https://arxiv.org/pdf/2207.02696.pdf">YOLO v7论文</a><br>7.<a href="https://github.com/pjreddie/darknet">YOLO v1234代码</a><br>8.<a href="https://github.com/ultralytics/yolov5">YOLO v5代码</a><br>9.<a href="https://github.com/meituan/YOLOv6">YOLO v6代码</a><br>10.<a href="https://github.com/WongKinYiu/yolov7">YOLO v7代码</a><br>11.<a href="https://ai-wx.blog.csdn.net/article/details/107509243?spm=1001.2014.3001.5506">YOLO综述</a></p>]]></content>
      
      
      <categories>
          
          <category> -YOLO </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -YOLO </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
